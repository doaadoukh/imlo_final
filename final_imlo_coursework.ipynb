{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doaadoukh/imlo_final/blob/main/final_imlo_coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KkJApIfKNoo",
        "outputId": "dea98ff6-ab3e-4f8e-be8e-b46befbabf87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000, Batch Size: 64, Loss: 0.3352, Train Acc: 1.23%, Val Acc: 3.43%\n",
            "Epoch 51/2000, Batch Size: 64, Loss: 0.0684, Train Acc: 2.21%, Val Acc: 1.96%\n",
            "Epoch 101/2000, Batch Size: 64, Loss: 0.0679, Train Acc: 2.82%, Val Acc: 1.96%\n",
            "Epoch 151/2000, Batch Size: 64, Loss: 0.0657, Train Acc: 3.68%, Val Acc: 1.47%\n",
            "Epoch 201/2000, Batch Size: 64, Loss: 0.0595, Train Acc: 12.01%, Val Acc: 6.86%\n",
            "Epoch 251/2000, Batch Size: 64, Loss: 0.0519, Train Acc: 16.67%, Val Acc: 13.24%\n",
            "Epoch 301/2000, Batch Size: 64, Loss: 0.0465, Train Acc: 25.86%, Val Acc: 13.24%\n",
            "Epoch 351/2000, Batch Size: 64, Loss: 0.0487, Train Acc: 22.43%, Val Acc: 11.27%\n",
            "Epoch 401/2000, Batch Size: 64, Loss: 0.0438, Train Acc: 29.53%, Val Acc: 20.10%\n",
            "Epoch 451/2000, Batch Size: 64, Loss: 0.0383, Train Acc: 37.25%, Val Acc: 21.57%\n",
            "Epoch 501/2000, Batch Size: 64, Loss: 0.0340, Train Acc: 41.18%, Val Acc: 27.94%\n",
            "Epoch 551/2000, Batch Size: 64, Loss: 0.0298, Train Acc: 49.88%, Val Acc: 34.31%\n",
            "Epoch 601/2000, Batch Size: 64, Loss: 0.0277, Train Acc: 55.51%, Val Acc: 37.25%\n",
            "Epoch 651/2000, Batch Size: 64, Loss: 0.0356, Train Acc: 42.16%, Val Acc: 18.14%\n",
            "Epoch 701/2000, Batch Size: 64, Loss: 0.0326, Train Acc: 46.69%, Val Acc: 35.29%\n",
            "Epoch 751/2000, Batch Size: 64, Loss: 0.0296, Train Acc: 49.39%, Val Acc: 30.88%\n",
            "Epoch 801/2000, Batch Size: 64, Loss: 0.0268, Train Acc: 55.39%, Val Acc: 32.84%\n",
            "Epoch 851/2000, Batch Size: 64, Loss: 0.0266, Train Acc: 56.62%, Val Acc: 34.80%\n",
            "Epoch 901/2000, Batch Size: 64, Loss: 0.0223, Train Acc: 62.75%, Val Acc: 40.20%\n",
            "Epoch 951/2000, Batch Size: 64, Loss: 0.0215, Train Acc: 64.46%, Val Acc: 44.12%\n",
            "Epoch 1001/2000, Batch Size: 64, Loss: 0.0196, Train Acc: 69.00%, Val Acc: 40.20%\n",
            "Epoch 1051/2000, Batch Size: 64, Loss: 0.0178, Train Acc: 70.96%, Val Acc: 44.61%\n",
            "Epoch 1101/2000, Batch Size: 64, Loss: 0.0155, Train Acc: 76.35%, Val Acc: 42.16%\n",
            "Epoch 1151/2000, Batch Size: 64, Loss: 0.0149, Train Acc: 78.68%, Val Acc: 49.02%\n",
            "Epoch 1201/2000, Batch Size: 64, Loss: 0.0141, Train Acc: 79.66%, Val Acc: 50.98%\n",
            "Epoch 1251/2000, Batch Size: 64, Loss: 0.0136, Train Acc: 79.90%, Val Acc: 49.51%\n",
            "Epoch 1301/2000, Batch Size: 64, Loss: 0.0237, Train Acc: 58.95%, Val Acc: 35.78%\n",
            "Epoch 1351/2000, Batch Size: 64, Loss: 0.0197, Train Acc: 67.65%, Val Acc: 37.25%\n",
            "Epoch 1401/2000, Batch Size: 64, Loss: 0.0211, Train Acc: 64.46%, Val Acc: 42.65%\n",
            "Epoch 1451/2000, Batch Size: 64, Loss: 0.0208, Train Acc: 64.09%, Val Acc: 33.33%\n",
            "Epoch 1501/2000, Batch Size: 64, Loss: 0.0189, Train Acc: 68.38%, Val Acc: 37.75%\n",
            "Epoch 1551/2000, Batch Size: 64, Loss: 0.0192, Train Acc: 67.28%, Val Acc: 37.75%\n",
            "Epoch 1601/2000, Batch Size: 64, Loss: 0.0192, Train Acc: 67.65%, Val Acc: 41.18%\n",
            "Epoch 1651/2000, Batch Size: 64, Loss: 0.0173, Train Acc: 71.08%, Val Acc: 46.57%\n",
            "Epoch 1701/2000, Batch Size: 64, Loss: 0.0177, Train Acc: 68.75%, Val Acc: 44.61%\n",
            "Epoch 1751/2000, Batch Size: 64, Loss: 0.0169, Train Acc: 70.34%, Val Acc: 44.61%\n",
            "Epoch 1801/2000, Batch Size: 64, Loss: 0.0165, Train Acc: 73.04%, Val Acc: 51.47%\n",
            "Epoch 1851/2000, Batch Size: 64, Loss: 0.0153, Train Acc: 74.63%, Val Acc: 48.53%\n",
            "Epoch 1901/2000, Batch Size: 64, Loss: 0.0152, Train Acc: 75.98%, Val Acc: 48.53%\n",
            "Epoch 1951/2000, Batch Size: 64, Loss: 0.0135, Train Acc: 77.57%, Val Acc: 46.57%\n",
            "Training is complete\n",
            "Test Accuracy:  62.63%\n"
          ]
        }
      ],
      "source": [
        "#import PyTorch packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "#Transformations for training and test sets\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "#Loading the datasets\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "#Splitting dataset into training, validation (80-20)\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "#Batch size\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "#Defining the CNN with dropout rate\n",
        "class Flower_Classifier_CNN(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(Flower_Classifier_CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(512 * 14 * 14,1024)\n",
        "        self.fc2 = nn.Linear(1024,102)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "      x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "      x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "      x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "      x = x.view(-1, 512 * 14 * 14)\n",
        "      x = self.dropout(F.relu(self.fc1(x)))\n",
        "      x = self.fc2(x)\n",
        "      return x\n",
        "\n",
        "#Setting the hyperparameters\n",
        "learning_rate = 0.0005\n",
        "weight_decay = 0.01\n",
        "dropout_rate = 0.5\n",
        "no_epochs = 2000\n",
        "\n",
        "#Defining the loss function and the optimiser\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Flower_Classifier_CNN(dropout_rate=dropout_rate).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "#Cosine annealing learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "#Functiona for calculating accuracy\n",
        "def calculate_acc(loader,model):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in loader:\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      _, preds = torch.max(outputs,1)\n",
        "      correct += torch.sum(preds == labels.data)\n",
        "      total += labels.size(0)\n",
        "  return(correct.double()/total)*100\n",
        "\n",
        "\n",
        "#Function to train the model\n",
        "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, no_epochs=2000):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    for epoch in range(no_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double()/total)*100\n",
        "        val_accuracy = calculate_acc(val_loader,model)\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "          print(f'Epoch {epoch+1}/{no_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training is now complete')\n",
        "    return model\n",
        "\n",
        "#Training the model\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, no_epochs)\n",
        "\n",
        "#Evaluating the model on the test set\n",
        "def eval_model(model, test_loader):\n",
        "  test_acc = calculate_acc(test_loader,model)\n",
        "  print(f'Test Accuracy: {test_acc: .2f}%')\n",
        "\n",
        "#Evaluating the training model\n",
        "eval_model(trained_model, test_loader)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "authorship_tag": "ABX9TyM65kY+Wypg8LviQmtOkbTr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}