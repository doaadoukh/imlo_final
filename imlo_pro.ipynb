{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOScOFYTiOgq6W5Kw/S5fw1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doaadoukh/imlo_final/blob/main/imlo_pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdkirVx3yImP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b311bb7c-0ce7-49ed-a25f-a396df5bf58d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May 17 01:58:40 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   42C    P8              12W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the CNN model\n",
        "class OptimizedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OptimizedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 25 * 25, 512)\n",
        "        self.dropout4 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.dropout3(x)\n",
        "        x = x.view(-1, 64 * 25 * 25)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout4(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "batch_size = 64\n",
        "num_epochs = 30\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(200),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "\n",
        "train_data = datasets.Flowers102(root='./data', split='train', transform=transform, download=True)\n",
        "val_data = datasets.Flowers102(root='./data', split='val', transform=transform, download=True)\n",
        "test_data = datasets.Flowers102(root='./data', split='test', transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "model = OptimizedCNN().to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "def compute_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "\n",
        "    train_accuracy = compute_accuracy(train_loader, model)\n",
        "    val_accuracy = compute_accuracy(val_loader, model)\n",
        "    test_accuracy = compute_accuracy(test_loader, model)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n",
        "    print(f'Training Accuracy: {train_accuracy:.2f}%')\n",
        "    print(f'Validation Accuracy: {val_accuracy:.2f}%')\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "id": "qYDdXJeDzfrQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "deb3ee8b-ed35-4071-cb3e-5a9386cea8c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 14.1140\n",
            "Training Accuracy: 3.14%\n",
            "Validation Accuracy: 2.16%\n",
            "Test Accuracy: 1.66%\n",
            "Epoch 2/30, Loss: 4.6140\n",
            "Training Accuracy: 1.47%\n",
            "Validation Accuracy: 1.08%\n",
            "Test Accuracy: 0.67%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f17ac558cd56>\u001b[0m in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/flowers102.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2190\u001b[0m                 )\n\u001b[1;32m   2191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(256 * 28 * 28, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 256 * 28 * 28)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "learning_rate = 0.00005\n",
        "weight_decay = 1e-2\n",
        "dropout_rate = 0.7\n",
        "\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, num_epochs=100):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "        test_accuracy = calculate_accuracy(test_loader, model)\n",
        "\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%, Test Acc: {test_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    return model\n",
        "\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, num_epochs=100)\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "evaluate_model(trained_model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "qfPEDsXR9VZt",
        "outputId": "5fd5a833-7b9f-431d-a5eb-25cf33cfba98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to data/flowers-102/102flowers.tgz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 344862509/344862509 [00:25<00:00, 13475120.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/flowers-102/102flowers.tgz to data/flowers-102\n",
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/imagelabels.mat to data/flowers-102/imagelabels.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 502/502 [00:00<00:00, 798763.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/setid.mat to data/flowers-102/setid.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14989/14989 [00:00<00:00, 21026228.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/99, Loss: 0.0757, Train Acc: 1.10%, Val Acc: 0.49%, Test Acc: 3.46%\n",
            "Epoch 1/99, Loss: 0.0701, Train Acc: 6.13%, Val Acc: 2.45%, Test Acc: 4.73%\n",
            "Epoch 2/99, Loss: 0.0644, Train Acc: 9.68%, Val Acc: 3.43%, Test Acc: 8.41%\n",
            "Epoch 3/99, Loss: 0.0582, Train Acc: 15.93%, Val Acc: 10.29%, Test Acc: 13.95%\n",
            "Epoch 4/99, Loss: 0.0539, Train Acc: 17.77%, Val Acc: 11.76%, Test Acc: 15.63%\n",
            "Epoch 5/99, Loss: 0.0513, Train Acc: 23.53%, Val Acc: 13.24%, Test Acc: 17.08%\n",
            "Epoch 6/99, Loss: 0.0493, Train Acc: 25.37%, Val Acc: 16.67%, Test Acc: 19.34%\n",
            "Epoch 7/99, Loss: 0.0479, Train Acc: 26.96%, Val Acc: 18.63%, Test Acc: 20.46%\n",
            "Epoch 8/99, Loss: 0.0469, Train Acc: 28.43%, Val Acc: 17.16%, Test Acc: 21.50%\n",
            "Epoch 9/99, Loss: 0.0462, Train Acc: 28.06%, Val Acc: 13.73%, Test Acc: 22.00%\n",
            "Epoch 10/99, Loss: 0.0447, Train Acc: 29.90%, Val Acc: 23.04%, Test Acc: 23.21%\n",
            "Epoch 11/99, Loss: 0.0437, Train Acc: 33.58%, Val Acc: 19.12%, Test Acc: 22.48%\n",
            "Epoch 12/99, Loss: 0.0424, Train Acc: 32.35%, Val Acc: 19.61%, Test Acc: 23.79%\n",
            "Epoch 13/99, Loss: 0.0420, Train Acc: 34.68%, Val Acc: 24.51%, Test Acc: 26.62%\n",
            "Epoch 14/99, Loss: 0.0417, Train Acc: 34.19%, Val Acc: 22.06%, Test Acc: 24.90%\n",
            "Epoch 15/99, Loss: 0.0400, Train Acc: 37.99%, Val Acc: 21.57%, Test Acc: 26.64%\n",
            "Epoch 16/99, Loss: 0.0394, Train Acc: 37.62%, Val Acc: 20.10%, Test Acc: 28.22%\n",
            "Epoch 17/99, Loss: 0.0385, Train Acc: 38.97%, Val Acc: 18.14%, Test Acc: 27.06%\n",
            "Epoch 18/99, Loss: 0.0374, Train Acc: 40.32%, Val Acc: 28.92%, Test Acc: 27.73%\n",
            "Epoch 19/99, Loss: 0.0373, Train Acc: 43.63%, Val Acc: 27.45%, Test Acc: 30.23%\n",
            "Epoch 20/99, Loss: 0.0358, Train Acc: 44.36%, Val Acc: 26.96%, Test Acc: 28.25%\n",
            "Epoch 21/99, Loss: 0.0350, Train Acc: 43.75%, Val Acc: 28.43%, Test Acc: 30.74%\n",
            "Epoch 22/99, Loss: 0.0355, Train Acc: 44.73%, Val Acc: 26.47%, Test Acc: 31.34%\n",
            "Epoch 23/99, Loss: 0.0350, Train Acc: 43.63%, Val Acc: 26.47%, Test Acc: 30.43%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d9c9ec8a0734>\u001b[0m in \u001b[0;36m<cell line: 117>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;31m# Evaluate the model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-d9c9ec8a0734>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, test_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(256 * 28 * 28, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 256 * 28 * 28)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "learning_rate = 0.001\n",
        "weight_decay = 1e-3\n",
        "dropout_rate = 0.5\n",
        "\n",
        "\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, num_epochs=100):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "        test_accuracy = calculate_accuracy(test_loader, model)\n",
        "\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%, Test Acc: {test_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    return model\n",
        "\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, num_epochs=100)\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "vr0iPp3AB3St",
        "outputId": "6b74e0e2-5f45-43cf-c6ae-b380ed0f2d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/99, Loss: 0.0923, Train Acc: 0.98%, Val Acc: 1.96%, Test Acc: 4.54%\n",
            "Epoch 1/99, Loss: 0.0679, Train Acc: 4.04%, Val Acc: 4.90%, Test Acc: 5.51%\n",
            "Epoch 2/99, Loss: 0.0620, Train Acc: 9.80%, Val Acc: 6.37%, Test Acc: 9.63%\n",
            "Epoch 3/99, Loss: 0.0581, Train Acc: 11.27%, Val Acc: 4.90%, Test Acc: 11.06%\n",
            "Epoch 4/99, Loss: 0.0554, Train Acc: 14.46%, Val Acc: 6.86%, Test Acc: 12.64%\n",
            "Epoch 5/99, Loss: 0.0537, Train Acc: 16.54%, Val Acc: 8.82%, Test Acc: 14.98%\n",
            "Epoch 6/99, Loss: 0.0525, Train Acc: 19.61%, Val Acc: 6.86%, Test Acc: 14.56%\n",
            "Epoch 7/99, Loss: 0.0511, Train Acc: 17.03%, Val Acc: 11.27%, Test Acc: 16.21%\n",
            "Epoch 8/99, Loss: 0.0498, Train Acc: 21.45%, Val Acc: 8.82%, Test Acc: 16.20%\n",
            "Epoch 9/99, Loss: 0.0500, Train Acc: 21.69%, Val Acc: 10.78%, Test Acc: 16.46%\n",
            "Epoch 10/99, Loss: 0.0487, Train Acc: 22.67%, Val Acc: 13.73%, Test Acc: 17.16%\n",
            "Epoch 11/99, Loss: 0.0471, Train Acc: 24.26%, Val Acc: 9.31%, Test Acc: 17.86%\n",
            "Epoch 12/99, Loss: 0.0474, Train Acc: 21.94%, Val Acc: 16.18%, Test Acc: 17.84%\n",
            "Epoch 13/99, Loss: 0.0471, Train Acc: 25.00%, Val Acc: 11.76%, Test Acc: 18.82%\n",
            "Epoch 14/99, Loss: 0.0453, Train Acc: 27.57%, Val Acc: 15.20%, Test Acc: 18.33%\n",
            "Epoch 15/99, Loss: 0.0450, Train Acc: 27.08%, Val Acc: 14.22%, Test Acc: 18.78%\n",
            "Epoch 16/99, Loss: 0.0446, Train Acc: 28.06%, Val Acc: 13.24%, Test Acc: 19.27%\n",
            "Epoch 17/99, Loss: 0.0446, Train Acc: 28.19%, Val Acc: 13.24%, Test Acc: 18.65%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-06f3b2c22dfb>\u001b[0m in \u001b[0;36m<cell line: 117>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;31m# Evaluate the model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-06f3b2c22dfb>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, test_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-06f3b2c22dfb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m28\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(192),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(192),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "s\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(128 * 24 * 24, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(-1, 128 * 24 * 24)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "learning_rate = 0.0005\n",
        "weight_decay = 0.01\n",
        "dropout_rate = 0.5\n",
        "\n",
        "\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "            labels = labels.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=300):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "            labels = labels.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "        test_accuracy = calculate_accuracy(test_loader, model)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%, Test Acc: {test_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    return model\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=300)\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW88qeHbSZ6Z",
        "outputId": "9e5aed35-cf8e-4fc1-dc69-bad113fcbbdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300, Loss: 0.1805, Train Acc: 1.96%, Val Acc: 5.88%, Test Acc: 5.94%\n",
            "Epoch 2/300, Loss: 0.1419, Train Acc: 7.72%, Val Acc: 6.37%, Test Acc: 7.56%\n",
            "Epoch 3/300, Loss: 0.1209, Train Acc: 8.82%, Val Acc: 5.88%, Test Acc: 8.72%\n",
            "Epoch 4/300, Loss: 0.0959, Train Acc: 10.91%, Val Acc: 5.39%, Test Acc: 9.14%\n",
            "Epoch 5/300, Loss: 0.0711, Train Acc: 14.34%, Val Acc: 8.33%, Test Acc: 9.01%\n",
            "Epoch 6/300, Loss: 0.0574, Train Acc: 14.95%, Val Acc: 12.75%, Test Acc: 13.60%\n",
            "Epoch 7/300, Loss: 0.0498, Train Acc: 16.91%, Val Acc: 6.37%, Test Acc: 12.00%\n",
            "Epoch 8/300, Loss: 0.0446, Train Acc: 18.14%, Val Acc: 11.76%, Test Acc: 15.87%\n",
            "Epoch 9/300, Loss: 0.0406, Train Acc: 23.16%, Val Acc: 11.27%, Test Acc: 13.95%\n",
            "Epoch 10/300, Loss: 0.0373, Train Acc: 25.25%, Val Acc: 14.71%, Test Acc: 16.93%\n",
            "Epoch 11/300, Loss: 0.0372, Train Acc: 24.75%, Val Acc: 10.78%, Test Acc: 15.79%\n",
            "Epoch 12/300, Loss: 0.0318, Train Acc: 29.90%, Val Acc: 16.18%, Test Acc: 18.20%\n",
            "Epoch 13/300, Loss: 0.0320, Train Acc: 28.92%, Val Acc: 16.18%, Test Acc: 21.92%\n",
            "Epoch 14/300, Loss: 0.0337, Train Acc: 29.29%, Val Acc: 18.14%, Test Acc: 21.22%\n",
            "Epoch 15/300, Loss: 0.0283, Train Acc: 32.84%, Val Acc: 12.25%, Test Acc: 21.30%\n",
            "Epoch 16/300, Loss: 0.0299, Train Acc: 32.11%, Val Acc: 13.73%, Test Acc: 20.64%\n",
            "Epoch 17/300, Loss: 0.0295, Train Acc: 35.17%, Val Acc: 19.12%, Test Acc: 23.81%\n",
            "Epoch 18/300, Loss: 0.0288, Train Acc: 35.91%, Val Acc: 18.14%, Test Acc: 22.26%\n",
            "Epoch 19/300, Loss: 0.0277, Train Acc: 38.48%, Val Acc: 21.57%, Test Acc: 20.80%\n",
            "Epoch 20/300, Loss: 0.0290, Train Acc: 36.52%, Val Acc: 15.69%, Test Acc: 20.69%\n",
            "Epoch 21/300, Loss: 0.0250, Train Acc: 41.42%, Val Acc: 20.10%, Test Acc: 23.60%\n",
            "Epoch 22/300, Loss: 0.0228, Train Acc: 43.01%, Val Acc: 19.61%, Test Acc: 25.47%\n",
            "Epoch 23/300, Loss: 0.0212, Train Acc: 42.77%, Val Acc: 24.02%, Test Acc: 26.70%\n",
            "Epoch 24/300, Loss: 0.0197, Train Acc: 43.87%, Val Acc: 25.98%, Test Acc: 28.18%\n",
            "Epoch 25/300, Loss: 0.0196, Train Acc: 48.90%, Val Acc: 25.00%, Test Acc: 29.09%\n",
            "Epoch 26/300, Loss: 0.0203, Train Acc: 48.28%, Val Acc: 21.08%, Test Acc: 29.40%\n",
            "Epoch 27/300, Loss: 0.0193, Train Acc: 46.20%, Val Acc: 22.06%, Test Acc: 26.35%\n",
            "Epoch 28/300, Loss: 0.0174, Train Acc: 52.33%, Val Acc: 25.98%, Test Acc: 27.44%\n",
            "Epoch 29/300, Loss: 0.0181, Train Acc: 51.59%, Val Acc: 23.53%, Test Acc: 28.39%\n",
            "Epoch 30/300, Loss: 0.0190, Train Acc: 48.16%, Val Acc: 23.53%, Test Acc: 26.98%\n",
            "Epoch 31/300, Loss: 0.0187, Train Acc: 47.30%, Val Acc: 22.55%, Test Acc: 29.27%\n",
            "Epoch 32/300, Loss: 0.0175, Train Acc: 51.35%, Val Acc: 21.57%, Test Acc: 28.59%\n",
            "Epoch 33/300, Loss: 0.0166, Train Acc: 53.92%, Val Acc: 22.55%, Test Acc: 26.59%\n",
            "Epoch 34/300, Loss: 0.0163, Train Acc: 53.43%, Val Acc: 23.04%, Test Acc: 30.07%\n",
            "Epoch 35/300, Loss: 0.0158, Train Acc: 54.66%, Val Acc: 27.94%, Test Acc: 30.69%\n",
            "Epoch 36/300, Loss: 0.0162, Train Acc: 54.78%, Val Acc: 24.51%, Test Acc: 29.60%\n",
            "Epoch 37/300, Loss: 0.0160, Train Acc: 56.13%, Val Acc: 27.94%, Test Acc: 30.12%\n",
            "Epoch 38/300, Loss: 0.0150, Train Acc: 55.88%, Val Acc: 28.43%, Test Acc: 28.28%\n",
            "Epoch 39/300, Loss: 0.0171, Train Acc: 51.35%, Val Acc: 21.57%, Test Acc: 30.48%\n",
            "Epoch 40/300, Loss: 0.0154, Train Acc: 58.95%, Val Acc: 24.02%, Test Acc: 29.60%\n",
            "Epoch 41/300, Loss: 0.0143, Train Acc: 59.56%, Val Acc: 23.53%, Test Acc: 30.57%\n",
            "Epoch 42/300, Loss: 0.0145, Train Acc: 62.13%, Val Acc: 22.06%, Test Acc: 31.70%\n",
            "Epoch 43/300, Loss: 0.0132, Train Acc: 59.93%, Val Acc: 25.49%, Test Acc: 32.41%\n",
            "Epoch 44/300, Loss: 0.0136, Train Acc: 62.75%, Val Acc: 24.51%, Test Acc: 31.03%\n",
            "Epoch 45/300, Loss: 0.0125, Train Acc: 61.27%, Val Acc: 27.45%, Test Acc: 31.60%\n",
            "Epoch 46/300, Loss: 0.0119, Train Acc: 64.09%, Val Acc: 26.96%, Test Acc: 33.34%\n",
            "Epoch 47/300, Loss: 0.0123, Train Acc: 63.11%, Val Acc: 26.47%, Test Acc: 32.87%\n",
            "Epoch 48/300, Loss: 0.0122, Train Acc: 63.11%, Val Acc: 28.43%, Test Acc: 33.14%\n",
            "Epoch 49/300, Loss: 0.0118, Train Acc: 62.99%, Val Acc: 23.53%, Test Acc: 33.36%\n",
            "Epoch 50/300, Loss: 0.0110, Train Acc: 64.83%, Val Acc: 26.47%, Test Acc: 33.52%\n",
            "Epoch 51/300, Loss: 0.0118, Train Acc: 65.32%, Val Acc: 25.98%, Test Acc: 32.92%\n",
            "Epoch 52/300, Loss: 0.0112, Train Acc: 65.56%, Val Acc: 27.45%, Test Acc: 32.14%\n",
            "Epoch 53/300, Loss: 0.0120, Train Acc: 65.07%, Val Acc: 29.90%, Test Acc: 34.09%\n",
            "Epoch 54/300, Loss: 0.0106, Train Acc: 67.40%, Val Acc: 29.41%, Test Acc: 33.49%\n",
            "Epoch 55/300, Loss: 0.0115, Train Acc: 66.91%, Val Acc: 26.96%, Test Acc: 32.17%\n",
            "Epoch 56/300, Loss: 0.0103, Train Acc: 69.36%, Val Acc: 28.43%, Test Acc: 33.58%\n",
            "Epoch 57/300, Loss: 0.0114, Train Acc: 65.44%, Val Acc: 30.88%, Test Acc: 33.63%\n",
            "Epoch 58/300, Loss: 0.0116, Train Acc: 64.34%, Val Acc: 30.88%, Test Acc: 32.54%\n",
            "Epoch 59/300, Loss: 0.0114, Train Acc: 65.07%, Val Acc: 30.39%, Test Acc: 33.86%\n",
            "Epoch 60/300, Loss: 0.0119, Train Acc: 65.69%, Val Acc: 28.92%, Test Acc: 33.13%\n",
            "Epoch 61/300, Loss: 0.0102, Train Acc: 69.12%, Val Acc: 29.41%, Test Acc: 34.20%\n",
            "Epoch 62/300, Loss: 0.0099, Train Acc: 70.96%, Val Acc: 26.96%, Test Acc: 34.30%\n",
            "Epoch 63/300, Loss: 0.0095, Train Acc: 70.10%, Val Acc: 29.41%, Test Acc: 34.54%\n",
            "Epoch 64/300, Loss: 0.0103, Train Acc: 68.63%, Val Acc: 29.90%, Test Acc: 34.27%\n",
            "Epoch 65/300, Loss: 0.0104, Train Acc: 69.61%, Val Acc: 28.43%, Test Acc: 34.22%\n",
            "Epoch 66/300, Loss: 0.0093, Train Acc: 70.71%, Val Acc: 28.43%, Test Acc: 34.57%\n",
            "Epoch 67/300, Loss: 0.0089, Train Acc: 71.81%, Val Acc: 25.49%, Test Acc: 35.10%\n",
            "Epoch 68/300, Loss: 0.0095, Train Acc: 71.57%, Val Acc: 26.47%, Test Acc: 35.21%\n",
            "Epoch 69/300, Loss: 0.0097, Train Acc: 70.83%, Val Acc: 25.98%, Test Acc: 34.92%\n",
            "Epoch 70/300, Loss: 0.0086, Train Acc: 73.65%, Val Acc: 28.43%, Test Acc: 34.82%\n",
            "Epoch 71/300, Loss: 0.0089, Train Acc: 71.81%, Val Acc: 25.00%, Test Acc: 35.00%\n",
            "Epoch 72/300, Loss: 0.0100, Train Acc: 69.98%, Val Acc: 27.94%, Test Acc: 35.00%\n",
            "Epoch 73/300, Loss: 0.0102, Train Acc: 69.49%, Val Acc: 29.90%, Test Acc: 35.37%\n",
            "Epoch 74/300, Loss: 0.0103, Train Acc: 68.38%, Val Acc: 30.88%, Test Acc: 33.11%\n",
            "Epoch 75/300, Loss: 0.0097, Train Acc: 70.22%, Val Acc: 30.39%, Test Acc: 34.07%\n",
            "Epoch 76/300, Loss: 0.0094, Train Acc: 72.43%, Val Acc: 31.86%, Test Acc: 35.91%\n",
            "Epoch 77/300, Loss: 0.0093, Train Acc: 73.65%, Val Acc: 30.39%, Test Acc: 35.50%\n",
            "Epoch 78/300, Loss: 0.0098, Train Acc: 69.98%, Val Acc: 29.41%, Test Acc: 35.08%\n",
            "Epoch 79/300, Loss: 0.0091, Train Acc: 71.69%, Val Acc: 33.82%, Test Acc: 35.03%\n",
            "Epoch 80/300, Loss: 0.0083, Train Acc: 75.61%, Val Acc: 30.88%, Test Acc: 35.31%\n",
            "Epoch 81/300, Loss: 0.0083, Train Acc: 73.53%, Val Acc: 30.88%, Test Acc: 35.57%\n",
            "Epoch 82/300, Loss: 0.0090, Train Acc: 74.63%, Val Acc: 29.41%, Test Acc: 36.04%\n",
            "Epoch 83/300, Loss: 0.0090, Train Acc: 71.94%, Val Acc: 31.37%, Test Acc: 36.30%\n",
            "Epoch 84/300, Loss: 0.0084, Train Acc: 73.53%, Val Acc: 28.92%, Test Acc: 36.14%\n",
            "Epoch 85/300, Loss: 0.0084, Train Acc: 76.47%, Val Acc: 31.86%, Test Acc: 35.79%\n",
            "Epoch 86/300, Loss: 0.0087, Train Acc: 75.49%, Val Acc: 31.86%, Test Acc: 35.62%\n",
            "Epoch 87/300, Loss: 0.0090, Train Acc: 72.67%, Val Acc: 27.94%, Test Acc: 35.76%\n",
            "Epoch 88/300, Loss: 0.0083, Train Acc: 76.84%, Val Acc: 33.33%, Test Acc: 36.04%\n",
            "Epoch 89/300, Loss: 0.0084, Train Acc: 73.77%, Val Acc: 29.41%, Test Acc: 36.23%\n",
            "Epoch 90/300, Loss: 0.0088, Train Acc: 73.77%, Val Acc: 31.86%, Test Acc: 36.30%\n",
            "Epoch 91/300, Loss: 0.0077, Train Acc: 75.74%, Val Acc: 28.92%, Test Acc: 36.56%\n",
            "Epoch 92/300, Loss: 0.0088, Train Acc: 73.41%, Val Acc: 30.88%, Test Acc: 36.22%\n",
            "Epoch 93/300, Loss: 0.0090, Train Acc: 74.51%, Val Acc: 32.84%, Test Acc: 35.55%\n",
            "Epoch 94/300, Loss: 0.0091, Train Acc: 72.55%, Val Acc: 31.37%, Test Acc: 36.04%\n",
            "Epoch 95/300, Loss: 0.0082, Train Acc: 75.86%, Val Acc: 35.29%, Test Acc: 36.43%\n",
            "Epoch 96/300, Loss: 0.0085, Train Acc: 74.02%, Val Acc: 32.35%, Test Acc: 35.79%\n",
            "Epoch 97/300, Loss: 0.0089, Train Acc: 74.02%, Val Acc: 30.39%, Test Acc: 36.12%\n",
            "Epoch 98/300, Loss: 0.0094, Train Acc: 71.57%, Val Acc: 31.37%, Test Acc: 35.86%\n",
            "Epoch 99/300, Loss: 0.0080, Train Acc: 74.75%, Val Acc: 32.84%, Test Acc: 35.73%\n",
            "Epoch 100/300, Loss: 0.0078, Train Acc: 74.75%, Val Acc: 33.33%, Test Acc: 35.27%\n",
            "Epoch 101/300, Loss: 0.0085, Train Acc: 74.39%, Val Acc: 32.35%, Test Acc: 35.58%\n",
            "Epoch 102/300, Loss: 0.0079, Train Acc: 75.00%, Val Acc: 30.39%, Test Acc: 35.76%\n",
            "Epoch 103/300, Loss: 0.0077, Train Acc: 77.33%, Val Acc: 32.84%, Test Acc: 35.75%\n",
            "Epoch 104/300, Loss: 0.0077, Train Acc: 76.72%, Val Acc: 27.94%, Test Acc: 36.12%\n",
            "Epoch 105/300, Loss: 0.0077, Train Acc: 75.86%, Val Acc: 31.86%, Test Acc: 36.80%\n",
            "Epoch 106/300, Loss: 0.0088, Train Acc: 75.49%, Val Acc: 34.80%, Test Acc: 36.95%\n",
            "Epoch 107/300, Loss: 0.0078, Train Acc: 78.31%, Val Acc: 35.78%, Test Acc: 36.84%\n",
            "Epoch 108/300, Loss: 0.0082, Train Acc: 76.35%, Val Acc: 35.29%, Test Acc: 36.97%\n",
            "Epoch 109/300, Loss: 0.0081, Train Acc: 74.14%, Val Acc: 33.33%, Test Acc: 36.90%\n",
            "Epoch 110/300, Loss: 0.0084, Train Acc: 75.49%, Val Acc: 31.37%, Test Acc: 36.77%\n",
            "Epoch 111/300, Loss: 0.0081, Train Acc: 75.00%, Val Acc: 29.41%, Test Acc: 36.85%\n",
            "Epoch 112/300, Loss: 0.0085, Train Acc: 74.26%, Val Acc: 32.84%, Test Acc: 36.64%\n",
            "Epoch 113/300, Loss: 0.0086, Train Acc: 75.00%, Val Acc: 30.39%, Test Acc: 36.22%\n",
            "Epoch 114/300, Loss: 0.0075, Train Acc: 76.10%, Val Acc: 28.92%, Test Acc: 35.83%\n",
            "Epoch 115/300, Loss: 0.0086, Train Acc: 75.49%, Val Acc: 33.82%, Test Acc: 36.25%\n",
            "Epoch 116/300, Loss: 0.0087, Train Acc: 74.14%, Val Acc: 29.90%, Test Acc: 36.25%\n",
            "Epoch 117/300, Loss: 0.0079, Train Acc: 75.25%, Val Acc: 32.84%, Test Acc: 36.44%\n",
            "Epoch 118/300, Loss: 0.0081, Train Acc: 75.12%, Val Acc: 31.37%, Test Acc: 36.85%\n",
            "Epoch 119/300, Loss: 0.0088, Train Acc: 74.51%, Val Acc: 33.33%, Test Acc: 36.84%\n",
            "Epoch 120/300, Loss: 0.0083, Train Acc: 74.63%, Val Acc: 34.80%, Test Acc: 37.18%\n",
            "Epoch 121/300, Loss: 0.0077, Train Acc: 75.00%, Val Acc: 30.88%, Test Acc: 37.11%\n",
            "Epoch 122/300, Loss: 0.0081, Train Acc: 74.88%, Val Acc: 31.37%, Test Acc: 37.14%\n",
            "Epoch 123/300, Loss: 0.0085, Train Acc: 75.98%, Val Acc: 29.41%, Test Acc: 37.23%\n",
            "Epoch 124/300, Loss: 0.0068, Train Acc: 79.41%, Val Acc: 28.43%, Test Acc: 37.13%\n",
            "Epoch 125/300, Loss: 0.0073, Train Acc: 77.82%, Val Acc: 25.98%, Test Acc: 36.92%\n",
            "Epoch 126/300, Loss: 0.0072, Train Acc: 79.17%, Val Acc: 30.39%, Test Acc: 36.77%\n",
            "Epoch 127/300, Loss: 0.0087, Train Acc: 72.43%, Val Acc: 31.37%, Test Acc: 36.66%\n",
            "Epoch 128/300, Loss: 0.0078, Train Acc: 76.35%, Val Acc: 30.39%, Test Acc: 36.88%\n",
            "Epoch 129/300, Loss: 0.0078, Train Acc: 76.72%, Val Acc: 30.39%, Test Acc: 37.16%\n",
            "Epoch 130/300, Loss: 0.0075, Train Acc: 75.98%, Val Acc: 32.35%, Test Acc: 37.23%\n",
            "Epoch 131/300, Loss: 0.0089, Train Acc: 74.51%, Val Acc: 34.31%, Test Acc: 37.32%\n",
            "Epoch 132/300, Loss: 0.0082, Train Acc: 75.49%, Val Acc: 32.84%, Test Acc: 37.21%\n",
            "Epoch 133/300, Loss: 0.0081, Train Acc: 77.08%, Val Acc: 28.92%, Test Acc: 36.97%\n",
            "Epoch 134/300, Loss: 0.0076, Train Acc: 74.14%, Val Acc: 28.92%, Test Acc: 37.08%\n",
            "Epoch 135/300, Loss: 0.0086, Train Acc: 73.41%, Val Acc: 27.45%, Test Acc: 37.03%\n",
            "Epoch 136/300, Loss: 0.0078, Train Acc: 76.84%, Val Acc: 32.84%, Test Acc: 37.31%\n",
            "Epoch 137/300, Loss: 0.0074, Train Acc: 77.33%, Val Acc: 35.78%, Test Acc: 37.27%\n",
            "Epoch 138/300, Loss: 0.0078, Train Acc: 76.47%, Val Acc: 33.82%, Test Acc: 36.88%\n",
            "Epoch 139/300, Loss: 0.0083, Train Acc: 75.12%, Val Acc: 33.82%, Test Acc: 36.74%\n",
            "Epoch 140/300, Loss: 0.0073, Train Acc: 79.17%, Val Acc: 31.86%, Test Acc: 36.87%\n",
            "Epoch 141/300, Loss: 0.0075, Train Acc: 76.23%, Val Acc: 32.84%, Test Acc: 37.06%\n",
            "Epoch 142/300, Loss: 0.0081, Train Acc: 75.00%, Val Acc: 30.39%, Test Acc: 37.13%\n",
            "Epoch 143/300, Loss: 0.0079, Train Acc: 75.61%, Val Acc: 28.92%, Test Acc: 36.95%\n",
            "Epoch 144/300, Loss: 0.0083, Train Acc: 76.96%, Val Acc: 33.33%, Test Acc: 36.98%\n",
            "Epoch 145/300, Loss: 0.0078, Train Acc: 76.59%, Val Acc: 31.37%, Test Acc: 36.92%\n",
            "Epoch 146/300, Loss: 0.0076, Train Acc: 76.96%, Val Acc: 32.35%, Test Acc: 36.92%\n",
            "Epoch 147/300, Loss: 0.0079, Train Acc: 78.06%, Val Acc: 30.39%, Test Acc: 37.16%\n",
            "Epoch 148/300, Loss: 0.0072, Train Acc: 77.94%, Val Acc: 34.80%, Test Acc: 37.14%\n",
            "Epoch 149/300, Loss: 0.0080, Train Acc: 74.63%, Val Acc: 32.84%, Test Acc: 37.14%\n",
            "Epoch 150/300, Loss: 0.0080, Train Acc: 75.98%, Val Acc: 30.39%, Test Acc: 36.98%\n",
            "Epoch 151/300, Loss: 0.0081, Train Acc: 78.06%, Val Acc: 29.90%, Test Acc: 36.97%\n",
            "Epoch 152/300, Loss: 0.0083, Train Acc: 76.10%, Val Acc: 30.88%, Test Acc: 37.03%\n",
            "Epoch 153/300, Loss: 0.0074, Train Acc: 78.06%, Val Acc: 31.86%, Test Acc: 36.84%\n",
            "Epoch 154/300, Loss: 0.0078, Train Acc: 75.37%, Val Acc: 28.92%, Test Acc: 36.85%\n",
            "Epoch 155/300, Loss: 0.0081, Train Acc: 76.72%, Val Acc: 31.37%, Test Acc: 36.88%\n",
            "Epoch 156/300, Loss: 0.0084, Train Acc: 76.35%, Val Acc: 32.35%, Test Acc: 36.85%\n",
            "Epoch 157/300, Loss: 0.0070, Train Acc: 78.55%, Val Acc: 30.88%, Test Acc: 37.03%\n",
            "Epoch 158/300, Loss: 0.0080, Train Acc: 75.37%, Val Acc: 30.39%, Test Acc: 36.90%\n",
            "Epoch 159/300, Loss: 0.0081, Train Acc: 76.72%, Val Acc: 31.86%, Test Acc: 36.72%\n",
            "Epoch 160/300, Loss: 0.0071, Train Acc: 80.15%, Val Acc: 30.39%, Test Acc: 36.82%\n",
            "Epoch 161/300, Loss: 0.0072, Train Acc: 78.68%, Val Acc: 31.86%, Test Acc: 36.67%\n",
            "Epoch 162/300, Loss: 0.0078, Train Acc: 76.10%, Val Acc: 27.45%, Test Acc: 36.75%\n",
            "Epoch 163/300, Loss: 0.0084, Train Acc: 75.98%, Val Acc: 31.86%, Test Acc: 36.74%\n",
            "Epoch 164/300, Loss: 0.0082, Train Acc: 76.35%, Val Acc: 31.37%, Test Acc: 36.71%\n",
            "Epoch 165/300, Loss: 0.0068, Train Acc: 77.57%, Val Acc: 27.45%, Test Acc: 36.74%\n",
            "Epoch 166/300, Loss: 0.0075, Train Acc: 76.96%, Val Acc: 30.88%, Test Acc: 36.82%\n",
            "Epoch 167/300, Loss: 0.0080, Train Acc: 75.74%, Val Acc: 26.96%, Test Acc: 36.69%\n",
            "Epoch 168/300, Loss: 0.0082, Train Acc: 75.12%, Val Acc: 30.39%, Test Acc: 36.98%\n",
            "Epoch 169/300, Loss: 0.0079, Train Acc: 77.33%, Val Acc: 31.37%, Test Acc: 37.03%\n",
            "Epoch 170/300, Loss: 0.0079, Train Acc: 76.96%, Val Acc: 32.84%, Test Acc: 37.11%\n",
            "Epoch 171/300, Loss: 0.0075, Train Acc: 76.47%, Val Acc: 27.94%, Test Acc: 36.95%\n",
            "Epoch 172/300, Loss: 0.0073, Train Acc: 76.96%, Val Acc: 31.86%, Test Acc: 37.05%\n",
            "Epoch 173/300, Loss: 0.0077, Train Acc: 76.59%, Val Acc: 30.39%, Test Acc: 36.93%\n",
            "Epoch 174/300, Loss: 0.0075, Train Acc: 79.17%, Val Acc: 29.90%, Test Acc: 37.14%\n",
            "Epoch 175/300, Loss: 0.0073, Train Acc: 78.43%, Val Acc: 34.31%, Test Acc: 37.10%\n",
            "Epoch 176/300, Loss: 0.0072, Train Acc: 78.68%, Val Acc: 31.37%, Test Acc: 37.21%\n",
            "Epoch 177/300, Loss: 0.0073, Train Acc: 76.84%, Val Acc: 33.33%, Test Acc: 37.13%\n",
            "Epoch 178/300, Loss: 0.0074, Train Acc: 76.59%, Val Acc: 30.88%, Test Acc: 37.23%\n",
            "Epoch 179/300, Loss: 0.0075, Train Acc: 78.06%, Val Acc: 31.86%, Test Acc: 37.16%\n",
            "Epoch 180/300, Loss: 0.0077, Train Acc: 76.23%, Val Acc: 27.45%, Test Acc: 36.95%\n",
            "Epoch 181/300, Loss: 0.0087, Train Acc: 73.77%, Val Acc: 30.88%, Test Acc: 37.21%\n",
            "Epoch 182/300, Loss: 0.0080, Train Acc: 74.63%, Val Acc: 27.94%, Test Acc: 37.26%\n",
            "Epoch 183/300, Loss: 0.0061, Train Acc: 80.64%, Val Acc: 31.86%, Test Acc: 37.23%\n",
            "Epoch 184/300, Loss: 0.0082, Train Acc: 77.82%, Val Acc: 32.84%, Test Acc: 37.14%\n",
            "Epoch 185/300, Loss: 0.0072, Train Acc: 77.08%, Val Acc: 31.86%, Test Acc: 36.95%\n",
            "Epoch 186/300, Loss: 0.0074, Train Acc: 79.29%, Val Acc: 27.45%, Test Acc: 37.05%\n",
            "Epoch 187/300, Loss: 0.0075, Train Acc: 76.72%, Val Acc: 28.43%, Test Acc: 37.16%\n",
            "Epoch 188/300, Loss: 0.0076, Train Acc: 77.94%, Val Acc: 29.90%, Test Acc: 37.08%\n",
            "Epoch 189/300, Loss: 0.0079, Train Acc: 76.59%, Val Acc: 30.39%, Test Acc: 37.01%\n",
            "Epoch 190/300, Loss: 0.0069, Train Acc: 77.82%, Val Acc: 32.35%, Test Acc: 37.24%\n",
            "Epoch 191/300, Loss: 0.0080, Train Acc: 75.61%, Val Acc: 30.88%, Test Acc: 37.29%\n",
            "Epoch 192/300, Loss: 0.0071, Train Acc: 77.57%, Val Acc: 33.33%, Test Acc: 37.27%\n",
            "Epoch 193/300, Loss: 0.0070, Train Acc: 78.19%, Val Acc: 34.80%, Test Acc: 37.24%\n",
            "Epoch 194/300, Loss: 0.0074, Train Acc: 77.82%, Val Acc: 30.39%, Test Acc: 37.13%\n",
            "Epoch 195/300, Loss: 0.0071, Train Acc: 78.68%, Val Acc: 32.35%, Test Acc: 37.24%\n",
            "Epoch 196/300, Loss: 0.0076, Train Acc: 76.59%, Val Acc: 31.37%, Test Acc: 37.14%\n",
            "Epoch 197/300, Loss: 0.0074, Train Acc: 78.55%, Val Acc: 32.35%, Test Acc: 36.97%\n",
            "Epoch 198/300, Loss: 0.0073, Train Acc: 78.55%, Val Acc: 35.29%, Test Acc: 36.80%\n",
            "Epoch 199/300, Loss: 0.0076, Train Acc: 76.59%, Val Acc: 29.90%, Test Acc: 37.00%\n",
            "Epoch 200/300, Loss: 0.0078, Train Acc: 76.72%, Val Acc: 32.84%, Test Acc: 37.10%\n",
            "Epoch 201/300, Loss: 0.0076, Train Acc: 76.35%, Val Acc: 31.86%, Test Acc: 36.95%\n",
            "Epoch 202/300, Loss: 0.0086, Train Acc: 75.74%, Val Acc: 29.90%, Test Acc: 36.77%\n",
            "Epoch 203/300, Loss: 0.0080, Train Acc: 75.25%, Val Acc: 32.35%, Test Acc: 36.92%\n",
            "Epoch 204/300, Loss: 0.0077, Train Acc: 76.96%, Val Acc: 32.35%, Test Acc: 36.82%\n",
            "Epoch 205/300, Loss: 0.0075, Train Acc: 77.21%, Val Acc: 33.82%, Test Acc: 37.19%\n",
            "Epoch 206/300, Loss: 0.0083, Train Acc: 74.63%, Val Acc: 32.84%, Test Acc: 37.24%\n",
            "Epoch 207/300, Loss: 0.0074, Train Acc: 78.43%, Val Acc: 30.88%, Test Acc: 36.92%\n",
            "Epoch 208/300, Loss: 0.0074, Train Acc: 76.72%, Val Acc: 33.82%, Test Acc: 36.87%\n",
            "Epoch 209/300, Loss: 0.0070, Train Acc: 78.43%, Val Acc: 32.84%, Test Acc: 36.93%\n",
            "Epoch 210/300, Loss: 0.0079, Train Acc: 76.96%, Val Acc: 34.31%, Test Acc: 37.05%\n",
            "Epoch 211/300, Loss: 0.0082, Train Acc: 76.72%, Val Acc: 34.31%, Test Acc: 37.10%\n",
            "Epoch 212/300, Loss: 0.0079, Train Acc: 74.75%, Val Acc: 33.33%, Test Acc: 36.77%\n",
            "Epoch 213/300, Loss: 0.0077, Train Acc: 75.98%, Val Acc: 30.39%, Test Acc: 37.11%\n",
            "Epoch 214/300, Loss: 0.0077, Train Acc: 76.84%, Val Acc: 31.37%, Test Acc: 36.95%\n",
            "Epoch 215/300, Loss: 0.0080, Train Acc: 74.88%, Val Acc: 31.86%, Test Acc: 37.13%\n",
            "Epoch 216/300, Loss: 0.0075, Train Acc: 77.82%, Val Acc: 33.82%, Test Acc: 37.37%\n",
            "Epoch 217/300, Loss: 0.0072, Train Acc: 77.82%, Val Acc: 29.90%, Test Acc: 37.10%\n",
            "Epoch 218/300, Loss: 0.0074, Train Acc: 78.19%, Val Acc: 30.39%, Test Acc: 37.18%\n",
            "Epoch 219/300, Loss: 0.0078, Train Acc: 76.84%, Val Acc: 30.39%, Test Acc: 37.16%\n",
            "Epoch 220/300, Loss: 0.0071, Train Acc: 77.33%, Val Acc: 34.80%, Test Acc: 37.24%\n",
            "Epoch 221/300, Loss: 0.0077, Train Acc: 77.33%, Val Acc: 34.31%, Test Acc: 37.27%\n",
            "Epoch 222/300, Loss: 0.0079, Train Acc: 76.35%, Val Acc: 33.33%, Test Acc: 37.23%\n",
            "Epoch 223/300, Loss: 0.0083, Train Acc: 75.25%, Val Acc: 30.39%, Test Acc: 37.21%\n",
            "Epoch 224/300, Loss: 0.0078, Train Acc: 76.96%, Val Acc: 28.92%, Test Acc: 36.95%\n",
            "Epoch 225/300, Loss: 0.0074, Train Acc: 77.57%, Val Acc: 31.37%, Test Acc: 37.11%\n",
            "Epoch 226/300, Loss: 0.0074, Train Acc: 77.33%, Val Acc: 30.88%, Test Acc: 37.08%\n",
            "Epoch 227/300, Loss: 0.0067, Train Acc: 79.53%, Val Acc: 32.35%, Test Acc: 37.19%\n",
            "Epoch 228/300, Loss: 0.0078, Train Acc: 75.12%, Val Acc: 32.35%, Test Acc: 36.98%\n",
            "Epoch 229/300, Loss: 0.0068, Train Acc: 78.55%, Val Acc: 31.37%, Test Acc: 37.05%\n",
            "Epoch 230/300, Loss: 0.0074, Train Acc: 77.70%, Val Acc: 32.84%, Test Acc: 37.05%\n",
            "Epoch 231/300, Loss: 0.0072, Train Acc: 76.59%, Val Acc: 32.84%, Test Acc: 37.21%\n",
            "Epoch 232/300, Loss: 0.0076, Train Acc: 76.96%, Val Acc: 32.84%, Test Acc: 37.26%\n",
            "Epoch 233/300, Loss: 0.0083, Train Acc: 75.98%, Val Acc: 32.84%, Test Acc: 37.16%\n",
            "Epoch 234/300, Loss: 0.0082, Train Acc: 75.25%, Val Acc: 31.37%, Test Acc: 37.26%\n",
            "Epoch 235/300, Loss: 0.0074, Train Acc: 75.74%, Val Acc: 33.82%, Test Acc: 37.21%\n",
            "Epoch 236/300, Loss: 0.0076, Train Acc: 77.70%, Val Acc: 32.84%, Test Acc: 37.19%\n",
            "Epoch 237/300, Loss: 0.0079, Train Acc: 76.35%, Val Acc: 31.86%, Test Acc: 37.24%\n",
            "Epoch 238/300, Loss: 0.0078, Train Acc: 76.84%, Val Acc: 34.31%, Test Acc: 37.27%\n",
            "Epoch 239/300, Loss: 0.0083, Train Acc: 74.63%, Val Acc: 31.86%, Test Acc: 37.23%\n",
            "Epoch 240/300, Loss: 0.0074, Train Acc: 78.19%, Val Acc: 28.92%, Test Acc: 37.06%\n",
            "Epoch 241/300, Loss: 0.0072, Train Acc: 79.90%, Val Acc: 33.33%, Test Acc: 37.06%\n",
            "Epoch 242/300, Loss: 0.0077, Train Acc: 77.82%, Val Acc: 33.82%, Test Acc: 37.11%\n",
            "Epoch 243/300, Loss: 0.0076, Train Acc: 77.82%, Val Acc: 29.90%, Test Acc: 37.13%\n",
            "Epoch 244/300, Loss: 0.0079, Train Acc: 73.77%, Val Acc: 31.86%, Test Acc: 37.42%\n",
            "Epoch 245/300, Loss: 0.0083, Train Acc: 74.88%, Val Acc: 30.88%, Test Acc: 37.39%\n",
            "Epoch 246/300, Loss: 0.0072, Train Acc: 77.70%, Val Acc: 28.43%, Test Acc: 37.37%\n",
            "Epoch 247/300, Loss: 0.0085, Train Acc: 76.23%, Val Acc: 32.35%, Test Acc: 37.36%\n",
            "Epoch 248/300, Loss: 0.0071, Train Acc: 75.61%, Val Acc: 33.33%, Test Acc: 37.29%\n",
            "Epoch 249/300, Loss: 0.0072, Train Acc: 77.57%, Val Acc: 35.29%, Test Acc: 37.26%\n",
            "Epoch 250/300, Loss: 0.0080, Train Acc: 73.90%, Val Acc: 31.86%, Test Acc: 37.14%\n",
            "Epoch 251/300, Loss: 0.0074, Train Acc: 80.15%, Val Acc: 28.92%, Test Acc: 37.21%\n",
            "Epoch 252/300, Loss: 0.0072, Train Acc: 78.55%, Val Acc: 30.88%, Test Acc: 37.18%\n",
            "Epoch 253/300, Loss: 0.0074, Train Acc: 77.57%, Val Acc: 30.39%, Test Acc: 37.14%\n",
            "Epoch 254/300, Loss: 0.0073, Train Acc: 77.57%, Val Acc: 29.90%, Test Acc: 37.24%\n",
            "Epoch 255/300, Loss: 0.0077, Train Acc: 76.23%, Val Acc: 33.33%, Test Acc: 37.26%\n",
            "Epoch 256/300, Loss: 0.0073, Train Acc: 77.57%, Val Acc: 30.39%, Test Acc: 37.19%\n",
            "Epoch 257/300, Loss: 0.0065, Train Acc: 78.92%, Val Acc: 30.39%, Test Acc: 37.29%\n",
            "Epoch 258/300, Loss: 0.0072, Train Acc: 78.06%, Val Acc: 35.78%, Test Acc: 37.26%\n",
            "Epoch 259/300, Loss: 0.0072, Train Acc: 77.57%, Val Acc: 29.41%, Test Acc: 37.31%\n",
            "Epoch 260/300, Loss: 0.0077, Train Acc: 77.21%, Val Acc: 34.80%, Test Acc: 37.49%\n",
            "Epoch 261/300, Loss: 0.0080, Train Acc: 77.82%, Val Acc: 31.37%, Test Acc: 37.45%\n",
            "Epoch 262/300, Loss: 0.0085, Train Acc: 75.12%, Val Acc: 29.41%, Test Acc: 37.24%\n",
            "Epoch 263/300, Loss: 0.0073, Train Acc: 76.96%, Val Acc: 34.31%, Test Acc: 37.26%\n",
            "Epoch 264/300, Loss: 0.0078, Train Acc: 76.35%, Val Acc: 30.88%, Test Acc: 37.16%\n",
            "Epoch 265/300, Loss: 0.0080, Train Acc: 78.92%, Val Acc: 31.37%, Test Acc: 37.36%\n",
            "Epoch 266/300, Loss: 0.0079, Train Acc: 77.70%, Val Acc: 29.90%, Test Acc: 37.49%\n",
            "Epoch 267/300, Loss: 0.0083, Train Acc: 76.23%, Val Acc: 29.90%, Test Acc: 37.31%\n",
            "Epoch 268/300, Loss: 0.0073, Train Acc: 78.43%, Val Acc: 35.78%, Test Acc: 37.03%\n",
            "Epoch 269/300, Loss: 0.0078, Train Acc: 77.08%, Val Acc: 31.86%, Test Acc: 37.11%\n",
            "Epoch 270/300, Loss: 0.0076, Train Acc: 76.59%, Val Acc: 34.80%, Test Acc: 37.01%\n",
            "Epoch 271/300, Loss: 0.0066, Train Acc: 79.53%, Val Acc: 32.35%, Test Acc: 37.01%\n",
            "Epoch 272/300, Loss: 0.0071, Train Acc: 78.80%, Val Acc: 31.37%, Test Acc: 37.10%\n",
            "Epoch 273/300, Loss: 0.0079, Train Acc: 75.86%, Val Acc: 29.90%, Test Acc: 37.24%\n",
            "Epoch 274/300, Loss: 0.0073, Train Acc: 80.02%, Val Acc: 36.27%, Test Acc: 37.19%\n",
            "Epoch 275/300, Loss: 0.0073, Train Acc: 77.94%, Val Acc: 33.82%, Test Acc: 37.19%\n",
            "Epoch 276/300, Loss: 0.0075, Train Acc: 77.21%, Val Acc: 29.90%, Test Acc: 37.16%\n",
            "Epoch 277/300, Loss: 0.0071, Train Acc: 79.17%, Val Acc: 34.31%, Test Acc: 37.21%\n",
            "Epoch 278/300, Loss: 0.0071, Train Acc: 78.31%, Val Acc: 29.41%, Test Acc: 37.24%\n",
            "Epoch 279/300, Loss: 0.0080, Train Acc: 74.02%, Val Acc: 29.90%, Test Acc: 37.11%\n",
            "Epoch 280/300, Loss: 0.0081, Train Acc: 76.35%, Val Acc: 31.37%, Test Acc: 36.92%\n",
            "Epoch 281/300, Loss: 0.0076, Train Acc: 77.94%, Val Acc: 29.41%, Test Acc: 37.21%\n",
            "Epoch 282/300, Loss: 0.0072, Train Acc: 78.68%, Val Acc: 27.45%, Test Acc: 37.29%\n",
            "Epoch 283/300, Loss: 0.0075, Train Acc: 78.31%, Val Acc: 27.45%, Test Acc: 37.18%\n",
            "Epoch 284/300, Loss: 0.0082, Train Acc: 75.25%, Val Acc: 32.84%, Test Acc: 37.27%\n",
            "Epoch 285/300, Loss: 0.0084, Train Acc: 75.74%, Val Acc: 34.80%, Test Acc: 37.26%\n",
            "Epoch 286/300, Loss: 0.0080, Train Acc: 76.84%, Val Acc: 32.84%, Test Acc: 37.16%\n",
            "Epoch 287/300, Loss: 0.0086, Train Acc: 75.74%, Val Acc: 29.90%, Test Acc: 37.11%\n",
            "Epoch 288/300, Loss: 0.0079, Train Acc: 77.57%, Val Acc: 32.35%, Test Acc: 37.26%\n",
            "Epoch 289/300, Loss: 0.0074, Train Acc: 76.23%, Val Acc: 32.35%, Test Acc: 37.18%\n",
            "Epoch 290/300, Loss: 0.0080, Train Acc: 76.10%, Val Acc: 32.84%, Test Acc: 37.27%\n",
            "Epoch 291/300, Loss: 0.0072, Train Acc: 77.57%, Val Acc: 31.86%, Test Acc: 37.06%\n",
            "Epoch 292/300, Loss: 0.0077, Train Acc: 76.35%, Val Acc: 29.90%, Test Acc: 36.90%\n",
            "Epoch 293/300, Loss: 0.0075, Train Acc: 76.47%, Val Acc: 31.86%, Test Acc: 37.31%\n",
            "Epoch 294/300, Loss: 0.0075, Train Acc: 75.98%, Val Acc: 26.96%, Test Acc: 37.14%\n",
            "Epoch 295/300, Loss: 0.0079, Train Acc: 76.47%, Val Acc: 28.92%, Test Acc: 37.10%\n",
            "Epoch 296/300, Loss: 0.0081, Train Acc: 75.25%, Val Acc: 35.78%, Test Acc: 37.14%\n",
            "Epoch 297/300, Loss: 0.0077, Train Acc: 78.55%, Val Acc: 30.88%, Test Acc: 37.14%\n",
            "Epoch 298/300, Loss: 0.0073, Train Acc: 77.45%, Val Acc: 29.41%, Test Acc: 37.00%\n",
            "Epoch 299/300, Loss: 0.0080, Train Acc: 76.96%, Val Acc: 34.31%, Test Acc: 37.01%\n",
            "Epoch 300/300, Loss: 0.0078, Train Acc: 77.08%, Val Acc: 30.88%, Test Acc: 36.98%\n",
            "Training complete\n",
            "Test Accuracy: 36.98%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(192),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(192),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(512 * 12 * 12, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 512 * 12 * 12)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "learning_rate = 0.0005\n",
        "weight_decay = 0.01\n",
        "dropout_rate = 0.5\n",
        "\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "            labels = labels.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "\n",
        "def train_model(model, train_dataset, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=300):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        batch_size = 64 if epoch < 100 else 32 if epoch < 200 else 16\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "        test_accuracy = calculate_accuracy(test_loader, model)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%, Test Acc: {test_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    return model\n",
        "\n",
        "\n",
        "trained_model = train_model(model, train_dataset, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=300)\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SqB50Ek2X8Bz",
        "outputId": "ddb77aa2-453f-429c-b0ad-6f6681c7d6ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300, Batch Size: 64, Loss: 0.3513, Train Acc: 1.47%, Val Acc: 0.98%, Test Acc: 2.81%\n",
            "Epoch 2/300, Batch Size: 64, Loss: 0.2092, Train Acc: 2.70%, Val Acc: 1.47%, Test Acc: 3.84%\n",
            "Epoch 3/300, Batch Size: 64, Loss: 0.1382, Train Acc: 2.45%, Val Acc: 2.45%, Test Acc: 5.61%\n",
            "Epoch 4/300, Batch Size: 64, Loss: 0.1132, Train Acc: 4.90%, Val Acc: 2.94%, Test Acc: 3.82%\n",
            "Epoch 5/300, Batch Size: 64, Loss: 0.1014, Train Acc: 5.02%, Val Acc: 4.90%, Test Acc: 5.98%\n",
            "Epoch 6/300, Batch Size: 64, Loss: 0.0925, Train Acc: 8.09%, Val Acc: 3.92%, Test Acc: 5.77%\n",
            "Epoch 7/300, Batch Size: 64, Loss: 0.0875, Train Acc: 7.48%, Val Acc: 5.88%, Test Acc: 7.27%\n",
            "Epoch 8/300, Batch Size: 64, Loss: 0.0845, Train Acc: 7.60%, Val Acc: 4.90%, Test Acc: 10.90%\n",
            "Epoch 9/300, Batch Size: 64, Loss: 0.0859, Train Acc: 9.19%, Val Acc: 3.92%, Test Acc: 9.82%\n",
            "Epoch 10/300, Batch Size: 64, Loss: 0.0812, Train Acc: 7.11%, Val Acc: 3.43%, Test Acc: 7.68%\n",
            "Epoch 11/300, Batch Size: 64, Loss: 0.0794, Train Acc: 8.95%, Val Acc: 4.41%, Test Acc: 8.39%\n",
            "Epoch 12/300, Batch Size: 64, Loss: 0.0746, Train Acc: 9.31%, Val Acc: 7.84%, Test Acc: 11.25%\n",
            "Epoch 13/300, Batch Size: 64, Loss: 0.0736, Train Acc: 10.29%, Val Acc: 5.39%, Test Acc: 10.29%\n",
            "Epoch 14/300, Batch Size: 64, Loss: 0.0769, Train Acc: 10.17%, Val Acc: 6.86%, Test Acc: 11.79%\n",
            "Epoch 15/300, Batch Size: 64, Loss: 0.0700, Train Acc: 12.50%, Val Acc: 5.88%, Test Acc: 12.02%\n",
            "Epoch 16/300, Batch Size: 64, Loss: 0.0673, Train Acc: 11.89%, Val Acc: 9.80%, Test Acc: 13.82%\n",
            "Epoch 17/300, Batch Size: 64, Loss: 0.0656, Train Acc: 12.50%, Val Acc: 10.29%, Test Acc: 13.87%\n",
            "Epoch 18/300, Batch Size: 64, Loss: 0.0676, Train Acc: 14.22%, Val Acc: 6.86%, Test Acc: 14.47%\n",
            "Epoch 19/300, Batch Size: 64, Loss: 0.0654, Train Acc: 12.87%, Val Acc: 7.84%, Test Acc: 13.40%\n",
            "Epoch 20/300, Batch Size: 64, Loss: 0.0653, Train Acc: 12.99%, Val Acc: 13.73%, Test Acc: 15.22%\n",
            "Epoch 21/300, Batch Size: 64, Loss: 0.0578, Train Acc: 17.16%, Val Acc: 16.18%, Test Acc: 20.08%\n",
            "Epoch 22/300, Batch Size: 64, Loss: 0.0526, Train Acc: 23.53%, Val Acc: 12.25%, Test Acc: 21.69%\n",
            "Epoch 23/300, Batch Size: 64, Loss: 0.0487, Train Acc: 24.88%, Val Acc: 22.55%, Test Acc: 21.78%\n",
            "Epoch 24/300, Batch Size: 64, Loss: 0.0494, Train Acc: 22.43%, Val Acc: 14.71%, Test Acc: 22.28%\n",
            "Epoch 25/300, Batch Size: 64, Loss: 0.0495, Train Acc: 25.25%, Val Acc: 16.18%, Test Acc: 20.75%\n",
            "Epoch 26/300, Batch Size: 64, Loss: 0.0508, Train Acc: 23.16%, Val Acc: 17.16%, Test Acc: 24.31%\n",
            "Epoch 27/300, Batch Size: 64, Loss: 0.0494, Train Acc: 24.02%, Val Acc: 22.06%, Test Acc: 23.63%\n",
            "Epoch 28/300, Batch Size: 64, Loss: 0.0488, Train Acc: 25.49%, Val Acc: 15.69%, Test Acc: 24.23%\n",
            "Epoch 29/300, Batch Size: 64, Loss: 0.0468, Train Acc: 26.47%, Val Acc: 16.67%, Test Acc: 25.30%\n",
            "Epoch 30/300, Batch Size: 64, Loss: 0.0485, Train Acc: 25.74%, Val Acc: 16.67%, Test Acc: 24.85%\n",
            "Epoch 31/300, Batch Size: 64, Loss: 0.0466, Train Acc: 27.21%, Val Acc: 19.61%, Test Acc: 24.95%\n",
            "Epoch 32/300, Batch Size: 64, Loss: 0.0466, Train Acc: 25.74%, Val Acc: 16.18%, Test Acc: 26.78%\n",
            "Epoch 33/300, Batch Size: 64, Loss: 0.0455, Train Acc: 30.51%, Val Acc: 15.20%, Test Acc: 24.51%\n",
            "Epoch 34/300, Batch Size: 64, Loss: 0.0482, Train Acc: 27.21%, Val Acc: 17.16%, Test Acc: 25.65%\n",
            "Epoch 35/300, Batch Size: 64, Loss: 0.0460, Train Acc: 27.82%, Val Acc: 19.61%, Test Acc: 27.94%\n",
            "Epoch 36/300, Batch Size: 64, Loss: 0.0469, Train Acc: 29.66%, Val Acc: 17.16%, Test Acc: 26.70%\n",
            "Epoch 37/300, Batch Size: 64, Loss: 0.0443, Train Acc: 31.25%, Val Acc: 18.63%, Test Acc: 25.96%\n",
            "Epoch 38/300, Batch Size: 64, Loss: 0.0456, Train Acc: 29.53%, Val Acc: 18.63%, Test Acc: 26.31%\n",
            "Epoch 39/300, Batch Size: 64, Loss: 0.0464, Train Acc: 28.80%, Val Acc: 19.12%, Test Acc: 26.09%\n",
            "Epoch 40/300, Batch Size: 64, Loss: 0.0449, Train Acc: 29.78%, Val Acc: 17.65%, Test Acc: 26.38%\n",
            "Epoch 41/300, Batch Size: 64, Loss: 0.0419, Train Acc: 33.21%, Val Acc: 18.14%, Test Acc: 28.98%\n",
            "Epoch 42/300, Batch Size: 64, Loss: 0.0395, Train Acc: 34.93%, Val Acc: 23.53%, Test Acc: 29.58%\n",
            "Epoch 43/300, Batch Size: 64, Loss: 0.0380, Train Acc: 39.22%, Val Acc: 17.65%, Test Acc: 31.14%\n",
            "Epoch 44/300, Batch Size: 64, Loss: 0.0374, Train Acc: 39.46%, Val Acc: 17.65%, Test Acc: 28.44%\n",
            "Epoch 45/300, Batch Size: 64, Loss: 0.0378, Train Acc: 39.22%, Val Acc: 17.65%, Test Acc: 29.84%\n",
            "Epoch 46/300, Batch Size: 64, Loss: 0.0370, Train Acc: 38.11%, Val Acc: 21.08%, Test Acc: 31.40%\n",
            "Epoch 47/300, Batch Size: 64, Loss: 0.0397, Train Acc: 36.76%, Val Acc: 20.10%, Test Acc: 30.43%\n",
            "Epoch 48/300, Batch Size: 64, Loss: 0.0376, Train Acc: 39.83%, Val Acc: 20.10%, Test Acc: 32.30%\n",
            "Epoch 49/300, Batch Size: 64, Loss: 0.0384, Train Acc: 38.60%, Val Acc: 21.08%, Test Acc: 31.84%\n",
            "Epoch 50/300, Batch Size: 64, Loss: 0.0364, Train Acc: 39.22%, Val Acc: 21.08%, Test Acc: 33.42%\n",
            "Epoch 51/300, Batch Size: 64, Loss: 0.0386, Train Acc: 38.85%, Val Acc: 22.55%, Test Acc: 32.92%\n",
            "Epoch 52/300, Batch Size: 64, Loss: 0.0379, Train Acc: 40.56%, Val Acc: 23.04%, Test Acc: 33.34%\n",
            "Epoch 53/300, Batch Size: 64, Loss: 0.0366, Train Acc: 41.42%, Val Acc: 25.00%, Test Acc: 32.98%\n",
            "Epoch 54/300, Batch Size: 64, Loss: 0.0353, Train Acc: 41.54%, Val Acc: 24.51%, Test Acc: 34.35%\n",
            "Epoch 55/300, Batch Size: 64, Loss: 0.0379, Train Acc: 37.87%, Val Acc: 24.51%, Test Acc: 33.40%\n",
            "Epoch 56/300, Batch Size: 64, Loss: 0.0368, Train Acc: 38.73%, Val Acc: 26.96%, Test Acc: 33.42%\n",
            "Epoch 57/300, Batch Size: 64, Loss: 0.0372, Train Acc: 39.83%, Val Acc: 25.49%, Test Acc: 33.27%\n",
            "Epoch 58/300, Batch Size: 64, Loss: 0.0359, Train Acc: 40.81%, Val Acc: 24.51%, Test Acc: 31.97%\n",
            "Epoch 59/300, Batch Size: 64, Loss: 0.0345, Train Acc: 44.61%, Val Acc: 20.59%, Test Acc: 34.92%\n",
            "Epoch 60/300, Batch Size: 64, Loss: 0.0362, Train Acc: 42.03%, Val Acc: 22.55%, Test Acc: 33.68%\n",
            "Epoch 61/300, Batch Size: 64, Loss: 0.0338, Train Acc: 46.81%, Val Acc: 23.53%, Test Acc: 34.44%\n",
            "Epoch 62/300, Batch Size: 64, Loss: 0.0328, Train Acc: 45.47%, Val Acc: 27.45%, Test Acc: 34.98%\n",
            "Epoch 63/300, Batch Size: 64, Loss: 0.0319, Train Acc: 49.02%, Val Acc: 27.94%, Test Acc: 36.61%\n",
            "Epoch 64/300, Batch Size: 64, Loss: 0.0312, Train Acc: 49.75%, Val Acc: 23.04%, Test Acc: 34.87%\n",
            "Epoch 65/300, Batch Size: 64, Loss: 0.0308, Train Acc: 48.65%, Val Acc: 22.06%, Test Acc: 35.81%\n",
            "Epoch 66/300, Batch Size: 64, Loss: 0.0322, Train Acc: 47.18%, Val Acc: 27.94%, Test Acc: 37.00%\n",
            "Epoch 67/300, Batch Size: 64, Loss: 0.0325, Train Acc: 44.98%, Val Acc: 23.53%, Test Acc: 35.55%\n",
            "Epoch 68/300, Batch Size: 64, Loss: 0.0301, Train Acc: 51.35%, Val Acc: 27.94%, Test Acc: 35.10%\n",
            "Epoch 69/300, Batch Size: 64, Loss: 0.0310, Train Acc: 49.63%, Val Acc: 23.53%, Test Acc: 36.44%\n",
            "Epoch 70/300, Batch Size: 64, Loss: 0.0328, Train Acc: 46.69%, Val Acc: 24.02%, Test Acc: 36.41%\n",
            "Epoch 71/300, Batch Size: 64, Loss: 0.0325, Train Acc: 45.96%, Val Acc: 25.00%, Test Acc: 36.58%\n",
            "Epoch 72/300, Batch Size: 64, Loss: 0.0326, Train Acc: 48.28%, Val Acc: 21.08%, Test Acc: 35.39%\n",
            "Epoch 73/300, Batch Size: 64, Loss: 0.0278, Train Acc: 54.04%, Val Acc: 25.98%, Test Acc: 36.18%\n",
            "Epoch 74/300, Batch Size: 64, Loss: 0.0311, Train Acc: 50.49%, Val Acc: 26.47%, Test Acc: 35.34%\n",
            "Epoch 75/300, Batch Size: 64, Loss: 0.0305, Train Acc: 47.55%, Val Acc: 25.49%, Test Acc: 36.87%\n",
            "Epoch 76/300, Batch Size: 64, Loss: 0.0311, Train Acc: 48.16%, Val Acc: 25.49%, Test Acc: 37.18%\n",
            "Epoch 77/300, Batch Size: 64, Loss: 0.0314, Train Acc: 49.88%, Val Acc: 29.41%, Test Acc: 37.05%\n",
            "Epoch 78/300, Batch Size: 64, Loss: 0.0294, Train Acc: 52.08%, Val Acc: 24.02%, Test Acc: 36.75%\n",
            "Epoch 79/300, Batch Size: 64, Loss: 0.0307, Train Acc: 48.53%, Val Acc: 23.53%, Test Acc: 36.92%\n",
            "Epoch 80/300, Batch Size: 64, Loss: 0.0310, Train Acc: 48.77%, Val Acc: 29.90%, Test Acc: 36.31%\n",
            "Epoch 81/300, Batch Size: 64, Loss: 0.0273, Train Acc: 53.55%, Val Acc: 25.98%, Test Acc: 36.67%\n",
            "Epoch 82/300, Batch Size: 64, Loss: 0.0300, Train Acc: 52.82%, Val Acc: 28.43%, Test Acc: 37.26%\n",
            "Epoch 83/300, Batch Size: 64, Loss: 0.0275, Train Acc: 54.41%, Val Acc: 25.98%, Test Acc: 37.45%\n",
            "Epoch 84/300, Batch Size: 64, Loss: 0.0278, Train Acc: 52.70%, Val Acc: 26.96%, Test Acc: 37.53%\n",
            "Epoch 85/300, Batch Size: 64, Loss: 0.0284, Train Acc: 53.31%, Val Acc: 26.47%, Test Acc: 38.09%\n",
            "Epoch 86/300, Batch Size: 64, Loss: 0.0276, Train Acc: 54.17%, Val Acc: 30.39%, Test Acc: 37.99%\n",
            "Epoch 87/300, Batch Size: 64, Loss: 0.0292, Train Acc: 53.06%, Val Acc: 25.98%, Test Acc: 38.41%\n",
            "Epoch 88/300, Batch Size: 64, Loss: 0.0287, Train Acc: 54.66%, Val Acc: 29.41%, Test Acc: 38.75%\n",
            "Epoch 89/300, Batch Size: 64, Loss: 0.0277, Train Acc: 55.15%, Val Acc: 27.94%, Test Acc: 38.27%\n",
            "Epoch 90/300, Batch Size: 64, Loss: 0.0281, Train Acc: 53.06%, Val Acc: 23.53%, Test Acc: 38.17%\n",
            "Epoch 91/300, Batch Size: 64, Loss: 0.0271, Train Acc: 52.57%, Val Acc: 25.98%, Test Acc: 37.27%\n",
            "Epoch 92/300, Batch Size: 64, Loss: 0.0270, Train Acc: 54.17%, Val Acc: 25.49%, Test Acc: 38.49%\n",
            "Epoch 93/300, Batch Size: 64, Loss: 0.0263, Train Acc: 55.39%, Val Acc: 27.94%, Test Acc: 37.63%\n",
            "Epoch 94/300, Batch Size: 64, Loss: 0.0286, Train Acc: 53.92%, Val Acc: 26.96%, Test Acc: 37.84%\n",
            "Epoch 95/300, Batch Size: 64, Loss: 0.0260, Train Acc: 57.23%, Val Acc: 29.90%, Test Acc: 38.28%\n",
            "Epoch 96/300, Batch Size: 64, Loss: 0.0281, Train Acc: 54.53%, Val Acc: 27.94%, Test Acc: 38.53%\n",
            "Epoch 97/300, Batch Size: 64, Loss: 0.0282, Train Acc: 51.59%, Val Acc: 25.49%, Test Acc: 38.49%\n",
            "Epoch 98/300, Batch Size: 64, Loss: 0.0275, Train Acc: 54.41%, Val Acc: 28.43%, Test Acc: 37.96%\n",
            "Epoch 99/300, Batch Size: 64, Loss: 0.0283, Train Acc: 53.68%, Val Acc: 27.45%, Test Acc: 39.32%\n",
            "Epoch 100/300, Batch Size: 64, Loss: 0.0275, Train Acc: 54.41%, Val Acc: 23.04%, Test Acc: 39.10%\n",
            "Epoch 101/300, Batch Size: 32, Loss: 0.0586, Train Acc: 52.21%, Val Acc: 28.43%, Test Acc: 38.66%\n",
            "Epoch 102/300, Batch Size: 32, Loss: 0.0593, Train Acc: 50.98%, Val Acc: 27.94%, Test Acc: 38.25%\n",
            "Epoch 103/300, Batch Size: 32, Loss: 0.0553, Train Acc: 55.02%, Val Acc: 25.49%, Test Acc: 39.05%\n",
            "Epoch 104/300, Batch Size: 32, Loss: 0.0603, Train Acc: 52.82%, Val Acc: 33.33%, Test Acc: 39.66%\n",
            "Epoch 105/300, Batch Size: 32, Loss: 0.0578, Train Acc: 52.45%, Val Acc: 24.51%, Test Acc: 37.92%\n",
            "Epoch 106/300, Batch Size: 32, Loss: 0.0553, Train Acc: 53.43%, Val Acc: 28.92%, Test Acc: 39.01%\n",
            "Epoch 107/300, Batch Size: 32, Loss: 0.0578, Train Acc: 50.49%, Val Acc: 28.92%, Test Acc: 38.33%\n",
            "Epoch 108/300, Batch Size: 32, Loss: 0.0590, Train Acc: 50.49%, Val Acc: 25.49%, Test Acc: 39.18%\n",
            "Epoch 109/300, Batch Size: 32, Loss: 0.0604, Train Acc: 50.12%, Val Acc: 25.49%, Test Acc: 37.96%\n",
            "Epoch 110/300, Batch Size: 32, Loss: 0.0570, Train Acc: 50.74%, Val Acc: 28.43%, Test Acc: 38.46%\n",
            "Epoch 111/300, Batch Size: 32, Loss: 0.0565, Train Acc: 53.80%, Val Acc: 28.43%, Test Acc: 39.73%\n",
            "Epoch 112/300, Batch Size: 32, Loss: 0.0550, Train Acc: 55.88%, Val Acc: 26.96%, Test Acc: 38.84%\n",
            "Epoch 113/300, Batch Size: 32, Loss: 0.0581, Train Acc: 53.43%, Val Acc: 26.47%, Test Acc: 39.37%\n",
            "Epoch 114/300, Batch Size: 32, Loss: 0.0575, Train Acc: 52.94%, Val Acc: 30.39%, Test Acc: 39.57%\n",
            "Epoch 115/300, Batch Size: 32, Loss: 0.0541, Train Acc: 54.53%, Val Acc: 26.47%, Test Acc: 39.16%\n",
            "Epoch 116/300, Batch Size: 32, Loss: 0.0570, Train Acc: 53.06%, Val Acc: 23.04%, Test Acc: 38.59%\n",
            "Epoch 117/300, Batch Size: 32, Loss: 0.0517, Train Acc: 56.00%, Val Acc: 29.41%, Test Acc: 38.46%\n",
            "Epoch 118/300, Batch Size: 32, Loss: 0.0566, Train Acc: 52.82%, Val Acc: 27.45%, Test Acc: 39.47%\n",
            "Epoch 119/300, Batch Size: 32, Loss: 0.0507, Train Acc: 58.33%, Val Acc: 31.37%, Test Acc: 39.57%\n",
            "Epoch 120/300, Batch Size: 32, Loss: 0.0555, Train Acc: 54.78%, Val Acc: 23.04%, Test Acc: 39.05%\n",
            "Epoch 121/300, Batch Size: 32, Loss: 0.0523, Train Acc: 55.51%, Val Acc: 25.98%, Test Acc: 39.21%\n",
            "Epoch 122/300, Batch Size: 32, Loss: 0.0540, Train Acc: 56.50%, Val Acc: 26.96%, Test Acc: 40.23%\n",
            "Epoch 123/300, Batch Size: 32, Loss: 0.0537, Train Acc: 54.90%, Val Acc: 29.90%, Test Acc: 40.04%\n",
            "Epoch 124/300, Batch Size: 32, Loss: 0.0554, Train Acc: 53.19%, Val Acc: 27.94%, Test Acc: 39.75%\n",
            "Epoch 125/300, Batch Size: 32, Loss: 0.0489, Train Acc: 59.31%, Val Acc: 24.51%, Test Acc: 39.49%\n",
            "Epoch 126/300, Batch Size: 32, Loss: 0.0509, Train Acc: 56.86%, Val Acc: 31.37%, Test Acc: 40.32%\n",
            "Epoch 127/300, Batch Size: 32, Loss: 0.0561, Train Acc: 52.94%, Val Acc: 37.75%, Test Acc: 40.35%\n",
            "Epoch 128/300, Batch Size: 32, Loss: 0.0504, Train Acc: 55.88%, Val Acc: 31.86%, Test Acc: 39.70%\n",
            "Epoch 129/300, Batch Size: 32, Loss: 0.0499, Train Acc: 58.33%, Val Acc: 28.43%, Test Acc: 39.94%\n",
            "Epoch 130/300, Batch Size: 32, Loss: 0.0526, Train Acc: 56.86%, Val Acc: 31.86%, Test Acc: 40.56%\n",
            "Epoch 131/300, Batch Size: 32, Loss: 0.0513, Train Acc: 57.48%, Val Acc: 23.04%, Test Acc: 40.22%\n",
            "Epoch 132/300, Batch Size: 32, Loss: 0.0549, Train Acc: 54.17%, Val Acc: 34.80%, Test Acc: 40.77%\n",
            "Epoch 133/300, Batch Size: 32, Loss: 0.0497, Train Acc: 56.25%, Val Acc: 30.39%, Test Acc: 39.68%\n",
            "Epoch 134/300, Batch Size: 32, Loss: 0.0504, Train Acc: 57.48%, Val Acc: 28.92%, Test Acc: 39.60%\n",
            "Epoch 135/300, Batch Size: 32, Loss: 0.0507, Train Acc: 57.97%, Val Acc: 25.00%, Test Acc: 39.52%\n",
            "Epoch 136/300, Batch Size: 32, Loss: 0.0525, Train Acc: 56.86%, Val Acc: 28.43%, Test Acc: 39.40%\n",
            "Epoch 137/300, Batch Size: 32, Loss: 0.0518, Train Acc: 58.82%, Val Acc: 29.41%, Test Acc: 40.67%\n",
            "Epoch 138/300, Batch Size: 32, Loss: 0.0504, Train Acc: 57.35%, Val Acc: 28.43%, Test Acc: 40.22%\n",
            "Epoch 139/300, Batch Size: 32, Loss: 0.0510, Train Acc: 58.21%, Val Acc: 29.41%, Test Acc: 39.29%\n",
            "Epoch 140/300, Batch Size: 32, Loss: 0.0546, Train Acc: 57.23%, Val Acc: 28.43%, Test Acc: 40.62%\n",
            "Epoch 141/300, Batch Size: 32, Loss: 0.0507, Train Acc: 60.66%, Val Acc: 27.45%, Test Acc: 40.45%\n",
            "Epoch 142/300, Batch Size: 32, Loss: 0.0481, Train Acc: 60.05%, Val Acc: 28.43%, Test Acc: 40.53%\n",
            "Epoch 143/300, Batch Size: 32, Loss: 0.0494, Train Acc: 60.29%, Val Acc: 30.88%, Test Acc: 40.32%\n",
            "Epoch 144/300, Batch Size: 32, Loss: 0.0515, Train Acc: 57.97%, Val Acc: 32.35%, Test Acc: 40.80%\n",
            "Epoch 145/300, Batch Size: 32, Loss: 0.0496, Train Acc: 58.70%, Val Acc: 32.35%, Test Acc: 40.88%\n",
            "Epoch 146/300, Batch Size: 32, Loss: 0.0477, Train Acc: 61.15%, Val Acc: 33.82%, Test Acc: 40.02%\n",
            "Epoch 147/300, Batch Size: 32, Loss: 0.0528, Train Acc: 58.82%, Val Acc: 30.88%, Test Acc: 40.32%\n",
            "Epoch 148/300, Batch Size: 32, Loss: 0.0536, Train Acc: 55.51%, Val Acc: 29.41%, Test Acc: 40.40%\n",
            "Epoch 149/300, Batch Size: 32, Loss: 0.0471, Train Acc: 60.66%, Val Acc: 30.88%, Test Acc: 40.58%\n",
            "Epoch 150/300, Batch Size: 32, Loss: 0.0520, Train Acc: 57.60%, Val Acc: 33.82%, Test Acc: 40.80%\n",
            "Epoch 151/300, Batch Size: 32, Loss: 0.0482, Train Acc: 60.42%, Val Acc: 27.94%, Test Acc: 40.72%\n",
            "Epoch 152/300, Batch Size: 32, Loss: 0.0507, Train Acc: 61.64%, Val Acc: 28.92%, Test Acc: 40.67%\n",
            "Epoch 153/300, Batch Size: 32, Loss: 0.0511, Train Acc: 56.13%, Val Acc: 27.94%, Test Acc: 40.51%\n",
            "Epoch 154/300, Batch Size: 32, Loss: 0.0511, Train Acc: 59.68%, Val Acc: 30.88%, Test Acc: 40.67%\n",
            "Epoch 155/300, Batch Size: 32, Loss: 0.0504, Train Acc: 58.70%, Val Acc: 30.39%, Test Acc: 40.30%\n",
            "Epoch 156/300, Batch Size: 32, Loss: 0.0507, Train Acc: 57.72%, Val Acc: 25.00%, Test Acc: 40.77%\n",
            "Epoch 157/300, Batch Size: 32, Loss: 0.0511, Train Acc: 58.33%, Val Acc: 26.47%, Test Acc: 40.25%\n",
            "Epoch 158/300, Batch Size: 32, Loss: 0.0505, Train Acc: 59.93%, Val Acc: 28.43%, Test Acc: 39.80%\n",
            "Epoch 159/300, Batch Size: 32, Loss: 0.0487, Train Acc: 59.19%, Val Acc: 26.47%, Test Acc: 40.48%\n",
            "Epoch 160/300, Batch Size: 32, Loss: 0.0497, Train Acc: 58.46%, Val Acc: 25.00%, Test Acc: 40.28%\n",
            "Epoch 161/300, Batch Size: 32, Loss: 0.0497, Train Acc: 58.21%, Val Acc: 29.41%, Test Acc: 40.56%\n",
            "Epoch 162/300, Batch Size: 32, Loss: 0.0525, Train Acc: 56.25%, Val Acc: 29.41%, Test Acc: 40.72%\n",
            "Epoch 163/300, Batch Size: 32, Loss: 0.0467, Train Acc: 61.64%, Val Acc: 27.94%, Test Acc: 40.15%\n",
            "Epoch 164/300, Batch Size: 32, Loss: 0.0500, Train Acc: 57.23%, Val Acc: 29.90%, Test Acc: 40.59%\n",
            "Epoch 165/300, Batch Size: 32, Loss: 0.0496, Train Acc: 57.84%, Val Acc: 31.86%, Test Acc: 40.66%\n",
            "Epoch 166/300, Batch Size: 32, Loss: 0.0497, Train Acc: 59.19%, Val Acc: 31.86%, Test Acc: 40.67%\n",
            "Epoch 167/300, Batch Size: 32, Loss: 0.0470, Train Acc: 60.42%, Val Acc: 30.88%, Test Acc: 40.79%\n",
            "Epoch 168/300, Batch Size: 32, Loss: 0.0479, Train Acc: 60.05%, Val Acc: 35.29%, Test Acc: 40.74%\n",
            "Epoch 169/300, Batch Size: 32, Loss: 0.0507, Train Acc: 56.25%, Val Acc: 33.33%, Test Acc: 40.74%\n",
            "Epoch 170/300, Batch Size: 32, Loss: 0.0507, Train Acc: 58.58%, Val Acc: 32.84%, Test Acc: 40.84%\n",
            "Epoch 171/300, Batch Size: 32, Loss: 0.0493, Train Acc: 58.82%, Val Acc: 30.88%, Test Acc: 40.62%\n",
            "Epoch 172/300, Batch Size: 32, Loss: 0.0484, Train Acc: 60.05%, Val Acc: 28.43%, Test Acc: 40.59%\n",
            "Epoch 173/300, Batch Size: 32, Loss: 0.0506, Train Acc: 58.33%, Val Acc: 28.92%, Test Acc: 40.33%\n",
            "Epoch 174/300, Batch Size: 32, Loss: 0.0479, Train Acc: 59.31%, Val Acc: 28.92%, Test Acc: 40.27%\n",
            "Epoch 175/300, Batch Size: 32, Loss: 0.0478, Train Acc: 59.93%, Val Acc: 24.51%, Test Acc: 40.07%\n",
            "Epoch 176/300, Batch Size: 32, Loss: 0.0471, Train Acc: 59.93%, Val Acc: 25.00%, Test Acc: 40.09%\n",
            "Epoch 177/300, Batch Size: 32, Loss: 0.0506, Train Acc: 57.84%, Val Acc: 30.39%, Test Acc: 40.28%\n",
            "Epoch 178/300, Batch Size: 32, Loss: 0.0468, Train Acc: 61.64%, Val Acc: 29.41%, Test Acc: 40.95%\n",
            "Epoch 179/300, Batch Size: 32, Loss: 0.0474, Train Acc: 61.15%, Val Acc: 33.82%, Test Acc: 40.66%\n",
            "Epoch 180/300, Batch Size: 32, Loss: 0.0521, Train Acc: 57.97%, Val Acc: 31.37%, Test Acc: 40.51%\n",
            "Epoch 181/300, Batch Size: 32, Loss: 0.0476, Train Acc: 60.91%, Val Acc: 28.92%, Test Acc: 40.85%\n",
            "Epoch 182/300, Batch Size: 32, Loss: 0.0492, Train Acc: 58.95%, Val Acc: 31.37%, Test Acc: 40.71%\n",
            "Epoch 183/300, Batch Size: 32, Loss: 0.0493, Train Acc: 59.56%, Val Acc: 31.37%, Test Acc: 41.05%\n",
            "Epoch 184/300, Batch Size: 32, Loss: 0.0513, Train Acc: 58.58%, Val Acc: 34.31%, Test Acc: 40.45%\n",
            "Epoch 185/300, Batch Size: 32, Loss: 0.0498, Train Acc: 57.84%, Val Acc: 28.92%, Test Acc: 40.75%\n",
            "Epoch 186/300, Batch Size: 32, Loss: 0.0500, Train Acc: 57.97%, Val Acc: 30.88%, Test Acc: 40.61%\n",
            "Epoch 187/300, Batch Size: 32, Loss: 0.0512, Train Acc: 56.74%, Val Acc: 30.88%, Test Acc: 40.71%\n",
            "Epoch 188/300, Batch Size: 32, Loss: 0.0483, Train Acc: 57.97%, Val Acc: 26.96%, Test Acc: 40.80%\n",
            "Epoch 189/300, Batch Size: 32, Loss: 0.0472, Train Acc: 58.95%, Val Acc: 31.37%, Test Acc: 41.19%\n",
            "Epoch 190/300, Batch Size: 32, Loss: 0.0482, Train Acc: 60.91%, Val Acc: 29.41%, Test Acc: 41.01%\n",
            "Epoch 191/300, Batch Size: 32, Loss: 0.0484, Train Acc: 60.17%, Val Acc: 26.96%, Test Acc: 40.75%\n",
            "Epoch 192/300, Batch Size: 32, Loss: 0.0445, Train Acc: 64.09%, Val Acc: 33.33%, Test Acc: 40.66%\n",
            "Epoch 193/300, Batch Size: 32, Loss: 0.0505, Train Acc: 57.97%, Val Acc: 26.47%, Test Acc: 40.92%\n",
            "Epoch 194/300, Batch Size: 32, Loss: 0.0499, Train Acc: 56.37%, Val Acc: 32.35%, Test Acc: 40.77%\n",
            "Epoch 195/300, Batch Size: 32, Loss: 0.0506, Train Acc: 57.72%, Val Acc: 29.90%, Test Acc: 40.23%\n",
            "Epoch 196/300, Batch Size: 32, Loss: 0.0468, Train Acc: 62.62%, Val Acc: 28.92%, Test Acc: 40.66%\n",
            "Epoch 197/300, Batch Size: 32, Loss: 0.0521, Train Acc: 59.19%, Val Acc: 31.37%, Test Acc: 40.84%\n",
            "Epoch 198/300, Batch Size: 32, Loss: 0.0482, Train Acc: 59.56%, Val Acc: 29.41%, Test Acc: 40.88%\n",
            "Epoch 199/300, Batch Size: 32, Loss: 0.0509, Train Acc: 59.68%, Val Acc: 29.90%, Test Acc: 40.59%\n",
            "Epoch 200/300, Batch Size: 32, Loss: 0.0516, Train Acc: 59.80%, Val Acc: 29.90%, Test Acc: 40.97%\n",
            "Epoch 201/300, Batch Size: 16, Loss: 0.1013, Train Acc: 58.33%, Val Acc: 28.92%, Test Acc: 39.86%\n",
            "Epoch 202/300, Batch Size: 16, Loss: 0.1082, Train Acc: 54.90%, Val Acc: 33.82%, Test Acc: 40.71%\n",
            "Epoch 203/300, Batch Size: 16, Loss: 0.1060, Train Acc: 56.74%, Val Acc: 30.88%, Test Acc: 40.04%\n",
            "Epoch 204/300, Batch Size: 16, Loss: 0.1019, Train Acc: 58.33%, Val Acc: 35.78%, Test Acc: 40.43%\n",
            "Epoch 205/300, Batch Size: 16, Loss: 0.1058, Train Acc: 55.76%, Val Acc: 27.94%, Test Acc: 40.33%\n",
            "Epoch 206/300, Batch Size: 16, Loss: 0.1053, Train Acc: 55.76%, Val Acc: 30.39%, Test Acc: 40.90%\n",
            "Epoch 207/300, Batch Size: 16, Loss: 0.0989, Train Acc: 59.31%, Val Acc: 32.35%, Test Acc: 40.85%\n",
            "Epoch 208/300, Batch Size: 16, Loss: 0.1112, Train Acc: 53.80%, Val Acc: 28.92%, Test Acc: 39.68%\n",
            "Epoch 209/300, Batch Size: 16, Loss: 0.1048, Train Acc: 56.00%, Val Acc: 28.43%, Test Acc: 40.64%\n",
            "Epoch 210/300, Batch Size: 16, Loss: 0.1020, Train Acc: 57.35%, Val Acc: 29.90%, Test Acc: 40.67%\n",
            "Epoch 211/300, Batch Size: 16, Loss: 0.1135, Train Acc: 53.43%, Val Acc: 29.41%, Test Acc: 40.74%\n",
            "Epoch 212/300, Batch Size: 16, Loss: 0.1067, Train Acc: 55.27%, Val Acc: 25.49%, Test Acc: 40.33%\n",
            "Epoch 213/300, Batch Size: 16, Loss: 0.1091, Train Acc: 53.68%, Val Acc: 32.84%, Test Acc: 39.75%\n",
            "Epoch 214/300, Batch Size: 16, Loss: 0.1112, Train Acc: 52.82%, Val Acc: 26.96%, Test Acc: 41.03%\n",
            "Epoch 215/300, Batch Size: 16, Loss: 0.1073, Train Acc: 57.23%, Val Acc: 25.00%, Test Acc: 40.88%\n",
            "Epoch 216/300, Batch Size: 16, Loss: 0.1082, Train Acc: 56.13%, Val Acc: 32.35%, Test Acc: 40.90%\n",
            "Epoch 217/300, Batch Size: 16, Loss: 0.1047, Train Acc: 54.78%, Val Acc: 27.45%, Test Acc: 39.80%\n",
            "Epoch 218/300, Batch Size: 16, Loss: 0.1053, Train Acc: 55.88%, Val Acc: 31.37%, Test Acc: 41.01%\n",
            "Epoch 219/300, Batch Size: 16, Loss: 0.1021, Train Acc: 56.74%, Val Acc: 29.41%, Test Acc: 41.01%\n",
            "Epoch 220/300, Batch Size: 16, Loss: 0.1094, Train Acc: 54.66%, Val Acc: 32.84%, Test Acc: 40.19%\n",
            "Epoch 221/300, Batch Size: 16, Loss: 0.1018, Train Acc: 56.37%, Val Acc: 30.39%, Test Acc: 40.51%\n",
            "Epoch 222/300, Batch Size: 16, Loss: 0.1020, Train Acc: 56.13%, Val Acc: 33.33%, Test Acc: 40.88%\n",
            "Epoch 223/300, Batch Size: 16, Loss: 0.1084, Train Acc: 56.37%, Val Acc: 28.92%, Test Acc: 40.80%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4c8bc259886a>\u001b[0m in \u001b[0;36m<cell line: 133>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;31m# Evaluate the model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-4c8bc259886a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataset, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Calculate test accuracy after each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/{num_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%, Test Acc: {test_accuracy:.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-4c8bc259886a>\u001b[0m in \u001b[0;36mcalculate_accuracy\u001b[0;34m(loader, model)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(192),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(192),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "def create_data_loaders(train_dataset, val_dataset, test_dataset, batch_size):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "train_loader, val_loader, test_loader = create_data_loaders(train_dataset, val_dataset, test_dataset, batch_size)\n",
        "\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(128 * 24 * 24, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(-1, 128 * 24 * 24)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "learning_rate = 0.0005\n",
        "weight_decay = 0.01\n",
        "dropout_rate = 0.5\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "\n",
        "def train_model(model, train_dataset, val_dataset, test_dataset, criterion, optimizer, scheduler, num_epochs=300):\n",
        "    batch_size = 64\n",
        "    train_loader, val_loader, test_loader = create_data_loaders(train_dataset, val_dataset, test_dataset, batch_size)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        if epoch == 100:\n",
        "            batch_size = 128\n",
        "            train_loader, val_loader, test_loader = create_data_loaders(train_dataset, val_dataset, test_dataset, batch_size)\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    return model\n",
        "\n",
        "\n",
        "trained_model = train_model(model, train_dataset, val_dataset, test_dataset, criterion, optimizer, scheduler, num_epochs=300)\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "final_train_loader, final_val_loader, final_test_loader = create_data_loaders(train_dataset, val_dataset, test_dataset, 128)\n",
        "evaluate_model(trained_model, final_test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl_Vtcsnns79",
        "outputId": "0b6254db-af9d-4169-dede-6a117e0f3d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _ConnectionBase.__del__ at 0x796b292f88b0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 132, in __del__\n",
            "    self._close()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
            "    _close(self._handle)\n",
            "OSError: [Errno 9] Bad file descriptor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300, Loss: 0.3231, Train Acc: 3.55%, Val Acc: 2.45%\n",
            "Epoch 2/300, Loss: 0.2129, Train Acc: 8.09%, Val Acc: 9.31%\n",
            "Epoch 3/300, Loss: 0.1441, Train Acc: 7.72%, Val Acc: 6.86%\n",
            "Epoch 4/300, Loss: 0.1196, Train Acc: 12.25%, Val Acc: 7.84%\n",
            "Epoch 5/300, Loss: 0.0919, Train Acc: 15.32%, Val Acc: 13.24%\n",
            "Epoch 6/300, Loss: 0.0949, Train Acc: 17.40%, Val Acc: 15.69%\n",
            "Epoch 7/300, Loss: 0.0843, Train Acc: 22.30%, Val Acc: 12.25%\n",
            "Epoch 8/300, Loss: 0.0848, Train Acc: 21.20%, Val Acc: 14.71%\n",
            "Epoch 9/300, Loss: 0.0825, Train Acc: 20.10%, Val Acc: 12.75%\n",
            "Epoch 10/300, Loss: 0.0814, Train Acc: 24.02%, Val Acc: 10.29%\n",
            "Epoch 11/300, Loss: 0.0725, Train Acc: 25.98%, Val Acc: 12.25%\n",
            "Epoch 12/300, Loss: 0.0693, Train Acc: 30.64%, Val Acc: 19.12%\n",
            "Epoch 13/300, Loss: 0.0734, Train Acc: 29.53%, Val Acc: 23.04%\n",
            "Epoch 14/300, Loss: 0.0667, Train Acc: 30.64%, Val Acc: 16.18%\n",
            "Epoch 15/300, Loss: 0.0658, Train Acc: 30.15%, Val Acc: 20.59%\n",
            "Epoch 16/300, Loss: 0.0669, Train Acc: 33.33%, Val Acc: 15.20%\n",
            "Epoch 17/300, Loss: 0.0583, Train Acc: 34.31%, Val Acc: 17.16%\n",
            "Epoch 18/300, Loss: 0.0584, Train Acc: 34.80%, Val Acc: 16.67%\n",
            "Epoch 19/300, Loss: 0.0558, Train Acc: 36.40%, Val Acc: 22.06%\n",
            "Epoch 20/300, Loss: 0.0575, Train Acc: 36.03%, Val Acc: 22.06%\n",
            "Epoch 21/300, Loss: 0.0502, Train Acc: 40.81%, Val Acc: 20.59%\n",
            "Epoch 22/300, Loss: 0.0421, Train Acc: 44.73%, Val Acc: 24.51%\n",
            "Epoch 23/300, Loss: 0.0372, Train Acc: 46.32%, Val Acc: 25.49%\n",
            "Epoch 24/300, Loss: 0.0347, Train Acc: 49.63%, Val Acc: 26.47%\n",
            "Epoch 25/300, Loss: 0.0352, Train Acc: 48.53%, Val Acc: 24.51%\n",
            "Epoch 26/300, Loss: 0.0327, Train Acc: 50.86%, Val Acc: 25.98%\n",
            "Epoch 27/300, Loss: 0.0352, Train Acc: 51.23%, Val Acc: 26.96%\n",
            "Epoch 28/300, Loss: 0.0339, Train Acc: 50.61%, Val Acc: 25.98%\n",
            "Epoch 29/300, Loss: 0.0316, Train Acc: 53.19%, Val Acc: 27.45%\n",
            "Epoch 30/300, Loss: 0.0315, Train Acc: 54.29%, Val Acc: 27.94%\n",
            "Epoch 31/300, Loss: 0.0303, Train Acc: 52.82%, Val Acc: 20.59%\n",
            "Epoch 32/300, Loss: 0.0321, Train Acc: 51.72%, Val Acc: 25.98%\n",
            "Epoch 33/300, Loss: 0.0289, Train Acc: 52.45%, Val Acc: 28.92%\n",
            "Epoch 34/300, Loss: 0.0286, Train Acc: 56.37%, Val Acc: 27.94%\n",
            "Epoch 35/300, Loss: 0.0302, Train Acc: 56.37%, Val Acc: 33.82%\n",
            "Epoch 36/300, Loss: 0.0301, Train Acc: 52.33%, Val Acc: 26.47%\n",
            "Epoch 37/300, Loss: 0.0303, Train Acc: 55.27%, Val Acc: 25.49%\n",
            "Epoch 38/300, Loss: 0.0289, Train Acc: 57.48%, Val Acc: 24.51%\n",
            "Epoch 39/300, Loss: 0.0284, Train Acc: 56.99%, Val Acc: 25.98%\n",
            "Epoch 40/300, Loss: 0.0292, Train Acc: 56.00%, Val Acc: 25.49%\n",
            "Epoch 41/300, Loss: 0.0257, Train Acc: 59.44%, Val Acc: 34.31%\n",
            "Epoch 42/300, Loss: 0.0240, Train Acc: 61.03%, Val Acc: 33.33%\n",
            "Epoch 43/300, Loss: 0.0245, Train Acc: 61.89%, Val Acc: 30.39%\n",
            "Epoch 44/300, Loss: 0.0209, Train Acc: 66.18%, Val Acc: 32.35%\n",
            "Epoch 45/300, Loss: 0.0209, Train Acc: 64.22%, Val Acc: 29.41%\n",
            "Epoch 46/300, Loss: 0.0209, Train Acc: 66.91%, Val Acc: 35.78%\n",
            "Epoch 47/300, Loss: 0.0221, Train Acc: 65.20%, Val Acc: 27.45%\n",
            "Epoch 48/300, Loss: 0.0227, Train Acc: 65.20%, Val Acc: 24.51%\n",
            "Epoch 49/300, Loss: 0.0205, Train Acc: 66.91%, Val Acc: 31.86%\n",
            "Epoch 50/300, Loss: 0.0227, Train Acc: 65.20%, Val Acc: 30.39%\n",
            "Epoch 51/300, Loss: 0.0214, Train Acc: 66.18%, Val Acc: 29.90%\n",
            "Epoch 52/300, Loss: 0.0207, Train Acc: 66.91%, Val Acc: 28.92%\n",
            "Epoch 53/300, Loss: 0.0211, Train Acc: 65.44%, Val Acc: 34.31%\n",
            "Epoch 54/300, Loss: 0.0217, Train Acc: 65.69%, Val Acc: 29.90%\n",
            "Epoch 55/300, Loss: 0.0211, Train Acc: 66.42%, Val Acc: 33.33%\n",
            "Epoch 56/300, Loss: 0.0192, Train Acc: 66.91%, Val Acc: 33.33%\n",
            "Epoch 57/300, Loss: 0.0211, Train Acc: 66.54%, Val Acc: 30.39%\n",
            "Epoch 58/300, Loss: 0.0211, Train Acc: 66.05%, Val Acc: 35.29%\n",
            "Epoch 59/300, Loss: 0.0217, Train Acc: 65.44%, Val Acc: 29.90%\n",
            "Epoch 60/300, Loss: 0.0201, Train Acc: 66.67%, Val Acc: 32.84%\n",
            "Epoch 61/300, Loss: 0.0205, Train Acc: 68.01%, Val Acc: 27.94%\n",
            "Epoch 62/300, Loss: 0.0167, Train Acc: 72.18%, Val Acc: 34.80%\n",
            "Epoch 63/300, Loss: 0.0189, Train Acc: 70.34%, Val Acc: 32.84%\n",
            "Epoch 64/300, Loss: 0.0182, Train Acc: 70.96%, Val Acc: 29.90%\n",
            "Epoch 65/300, Loss: 0.0175, Train Acc: 70.71%, Val Acc: 34.80%\n",
            "Epoch 66/300, Loss: 0.0184, Train Acc: 68.87%, Val Acc: 32.84%\n",
            "Epoch 67/300, Loss: 0.0182, Train Acc: 70.59%, Val Acc: 35.29%\n",
            "Epoch 68/300, Loss: 0.0176, Train Acc: 73.04%, Val Acc: 34.31%\n",
            "Epoch 69/300, Loss: 0.0165, Train Acc: 72.92%, Val Acc: 35.78%\n",
            "Epoch 70/300, Loss: 0.0180, Train Acc: 67.77%, Val Acc: 34.80%\n",
            "Epoch 71/300, Loss: 0.0173, Train Acc: 71.08%, Val Acc: 34.80%\n",
            "Epoch 72/300, Loss: 0.0165, Train Acc: 72.06%, Val Acc: 36.27%\n",
            "Epoch 73/300, Loss: 0.0163, Train Acc: 73.77%, Val Acc: 26.96%\n",
            "Epoch 74/300, Loss: 0.0154, Train Acc: 73.41%, Val Acc: 32.84%\n",
            "Epoch 75/300, Loss: 0.0173, Train Acc: 70.71%, Val Acc: 29.41%\n",
            "Epoch 76/300, Loss: 0.0172, Train Acc: 73.16%, Val Acc: 31.86%\n",
            "Epoch 77/300, Loss: 0.0148, Train Acc: 76.10%, Val Acc: 30.88%\n",
            "Epoch 78/300, Loss: 0.0169, Train Acc: 72.55%, Val Acc: 33.33%\n",
            "Epoch 79/300, Loss: 0.0184, Train Acc: 70.47%, Val Acc: 33.33%\n",
            "Epoch 80/300, Loss: 0.0156, Train Acc: 73.53%, Val Acc: 30.39%\n",
            "Epoch 81/300, Loss: 0.0165, Train Acc: 71.81%, Val Acc: 30.88%\n",
            "Epoch 82/300, Loss: 0.0152, Train Acc: 74.51%, Val Acc: 32.84%\n",
            "Epoch 83/300, Loss: 0.0152, Train Acc: 72.43%, Val Acc: 33.82%\n",
            "Epoch 84/300, Loss: 0.0142, Train Acc: 76.23%, Val Acc: 32.84%\n",
            "Epoch 85/300, Loss: 0.0150, Train Acc: 74.88%, Val Acc: 33.82%\n",
            "Epoch 86/300, Loss: 0.0163, Train Acc: 74.26%, Val Acc: 26.96%\n",
            "Epoch 87/300, Loss: 0.0136, Train Acc: 78.19%, Val Acc: 39.22%\n",
            "Epoch 88/300, Loss: 0.0153, Train Acc: 74.63%, Val Acc: 34.31%\n",
            "Epoch 89/300, Loss: 0.0152, Train Acc: 74.88%, Val Acc: 34.80%\n",
            "Epoch 90/300, Loss: 0.0152, Train Acc: 75.12%, Val Acc: 31.86%\n",
            "Epoch 91/300, Loss: 0.0154, Train Acc: 74.51%, Val Acc: 36.76%\n",
            "Epoch 92/300, Loss: 0.0141, Train Acc: 77.70%, Val Acc: 39.71%\n",
            "Epoch 93/300, Loss: 0.0140, Train Acc: 75.49%, Val Acc: 31.37%\n",
            "Epoch 94/300, Loss: 0.0144, Train Acc: 76.35%, Val Acc: 37.75%\n",
            "Epoch 95/300, Loss: 0.0150, Train Acc: 75.37%, Val Acc: 32.35%\n",
            "Epoch 96/300, Loss: 0.0136, Train Acc: 79.41%, Val Acc: 35.29%\n",
            "Epoch 97/300, Loss: 0.0150, Train Acc: 75.49%, Val Acc: 34.80%\n",
            "Epoch 98/300, Loss: 0.0150, Train Acc: 75.86%, Val Acc: 33.82%\n",
            "Epoch 99/300, Loss: 0.0157, Train Acc: 75.49%, Val Acc: 33.33%\n",
            "Epoch 100/300, Loss: 0.0153, Train Acc: 74.02%, Val Acc: 32.35%\n",
            "Epoch 101/300, Loss: 0.0085, Train Acc: 75.00%, Val Acc: 30.88%\n",
            "Epoch 102/300, Loss: 0.0078, Train Acc: 75.98%, Val Acc: 32.35%\n",
            "Epoch 103/300, Loss: 0.0074, Train Acc: 76.72%, Val Acc: 29.41%\n",
            "Epoch 104/300, Loss: 0.0082, Train Acc: 75.61%, Val Acc: 38.24%\n",
            "Epoch 105/300, Loss: 0.0070, Train Acc: 78.80%, Val Acc: 33.33%\n",
            "Epoch 106/300, Loss: 0.0070, Train Acc: 80.02%, Val Acc: 35.78%\n",
            "Epoch 107/300, Loss: 0.0081, Train Acc: 76.84%, Val Acc: 36.27%\n",
            "Epoch 108/300, Loss: 0.0074, Train Acc: 77.33%, Val Acc: 36.76%\n",
            "Epoch 109/300, Loss: 0.0072, Train Acc: 77.82%, Val Acc: 30.88%\n",
            "Epoch 110/300, Loss: 0.0076, Train Acc: 77.45%, Val Acc: 31.86%\n",
            "Epoch 111/300, Loss: 0.0072, Train Acc: 77.94%, Val Acc: 31.37%\n",
            "Epoch 112/300, Loss: 0.0074, Train Acc: 78.06%, Val Acc: 31.86%\n",
            "Epoch 113/300, Loss: 0.0064, Train Acc: 80.51%, Val Acc: 34.80%\n",
            "Epoch 114/300, Loss: 0.0072, Train Acc: 78.55%, Val Acc: 32.84%\n",
            "Epoch 115/300, Loss: 0.0075, Train Acc: 80.27%, Val Acc: 33.33%\n",
            "Epoch 116/300, Loss: 0.0072, Train Acc: 78.68%, Val Acc: 34.31%\n",
            "Epoch 117/300, Loss: 0.0069, Train Acc: 78.68%, Val Acc: 35.29%\n",
            "Epoch 118/300, Loss: 0.0073, Train Acc: 76.96%, Val Acc: 34.80%\n",
            "Epoch 119/300, Loss: 0.0071, Train Acc: 77.08%, Val Acc: 32.35%\n",
            "Epoch 120/300, Loss: 0.0083, Train Acc: 75.12%, Val Acc: 36.27%\n",
            "Epoch 121/300, Loss: 0.0066, Train Acc: 79.66%, Val Acc: 35.29%\n",
            "Epoch 122/300, Loss: 0.0074, Train Acc: 76.84%, Val Acc: 33.33%\n",
            "Epoch 123/300, Loss: 0.0066, Train Acc: 80.64%, Val Acc: 36.76%\n",
            "Epoch 124/300, Loss: 0.0078, Train Acc: 75.61%, Val Acc: 30.88%\n",
            "Epoch 125/300, Loss: 0.0082, Train Acc: 77.82%, Val Acc: 33.33%\n",
            "Epoch 126/300, Loss: 0.0070, Train Acc: 78.19%, Val Acc: 33.82%\n",
            "Epoch 127/300, Loss: 0.0076, Train Acc: 75.86%, Val Acc: 31.86%\n",
            "Epoch 128/300, Loss: 0.0071, Train Acc: 79.41%, Val Acc: 33.33%\n",
            "Epoch 129/300, Loss: 0.0065, Train Acc: 81.00%, Val Acc: 33.33%\n",
            "Epoch 130/300, Loss: 0.0071, Train Acc: 79.04%, Val Acc: 34.31%\n",
            "Epoch 131/300, Loss: 0.0072, Train Acc: 77.82%, Val Acc: 36.27%\n",
            "Epoch 132/300, Loss: 0.0068, Train Acc: 79.53%, Val Acc: 35.29%\n",
            "Epoch 133/300, Loss: 0.0081, Train Acc: 76.84%, Val Acc: 30.88%\n",
            "Epoch 134/300, Loss: 0.0074, Train Acc: 79.53%, Val Acc: 34.80%\n",
            "Epoch 135/300, Loss: 0.0072, Train Acc: 78.68%, Val Acc: 39.71%\n",
            "Epoch 136/300, Loss: 0.0076, Train Acc: 75.86%, Val Acc: 32.84%\n",
            "Epoch 137/300, Loss: 0.0064, Train Acc: 80.76%, Val Acc: 34.31%\n",
            "Epoch 138/300, Loss: 0.0074, Train Acc: 80.27%, Val Acc: 34.80%\n",
            "Epoch 139/300, Loss: 0.0068, Train Acc: 80.27%, Val Acc: 30.88%\n",
            "Epoch 140/300, Loss: 0.0074, Train Acc: 77.33%, Val Acc: 39.22%\n",
            "Epoch 141/300, Loss: 0.0075, Train Acc: 76.23%, Val Acc: 37.25%\n",
            "Epoch 142/300, Loss: 0.0067, Train Acc: 78.92%, Val Acc: 32.84%\n",
            "Epoch 143/300, Loss: 0.0071, Train Acc: 78.55%, Val Acc: 36.27%\n",
            "Epoch 144/300, Loss: 0.0079, Train Acc: 76.10%, Val Acc: 34.80%\n",
            "Epoch 145/300, Loss: 0.0073, Train Acc: 77.82%, Val Acc: 35.29%\n",
            "Epoch 146/300, Loss: 0.0077, Train Acc: 78.80%, Val Acc: 37.25%\n",
            "Epoch 147/300, Loss: 0.0073, Train Acc: 77.33%, Val Acc: 34.31%\n",
            "Epoch 148/300, Loss: 0.0071, Train Acc: 80.76%, Val Acc: 30.39%\n",
            "Epoch 149/300, Loss: 0.0075, Train Acc: 77.94%, Val Acc: 35.78%\n",
            "Epoch 150/300, Loss: 0.0075, Train Acc: 78.19%, Val Acc: 35.78%\n",
            "Epoch 151/300, Loss: 0.0078, Train Acc: 76.35%, Val Acc: 38.73%\n",
            "Epoch 152/300, Loss: 0.0075, Train Acc: 76.72%, Val Acc: 35.78%\n",
            "Epoch 153/300, Loss: 0.0071, Train Acc: 78.43%, Val Acc: 35.78%\n",
            "Epoch 154/300, Loss: 0.0071, Train Acc: 78.31%, Val Acc: 37.25%\n",
            "Epoch 155/300, Loss: 0.0067, Train Acc: 80.51%, Val Acc: 38.73%\n",
            "Epoch 156/300, Loss: 0.0067, Train Acc: 79.78%, Val Acc: 36.27%\n",
            "Epoch 157/300, Loss: 0.0067, Train Acc: 78.68%, Val Acc: 35.29%\n",
            "Epoch 158/300, Loss: 0.0073, Train Acc: 78.80%, Val Acc: 31.37%\n",
            "Epoch 159/300, Loss: 0.0074, Train Acc: 78.92%, Val Acc: 28.92%\n",
            "Epoch 160/300, Loss: 0.0066, Train Acc: 80.15%, Val Acc: 38.24%\n",
            "Epoch 161/300, Loss: 0.0071, Train Acc: 77.45%, Val Acc: 37.25%\n",
            "Epoch 162/300, Loss: 0.0067, Train Acc: 81.13%, Val Acc: 32.35%\n",
            "Epoch 163/300, Loss: 0.0077, Train Acc: 77.57%, Val Acc: 32.35%\n",
            "Epoch 164/300, Loss: 0.0073, Train Acc: 78.43%, Val Acc: 35.29%\n",
            "Epoch 165/300, Loss: 0.0071, Train Acc: 76.84%, Val Acc: 36.76%\n",
            "Epoch 166/300, Loss: 0.0066, Train Acc: 79.78%, Val Acc: 33.33%\n",
            "Epoch 167/300, Loss: 0.0069, Train Acc: 80.27%, Val Acc: 34.80%\n",
            "Epoch 168/300, Loss: 0.0077, Train Acc: 77.33%, Val Acc: 35.78%\n",
            "Epoch 169/300, Loss: 0.0077, Train Acc: 76.84%, Val Acc: 36.27%\n",
            "Epoch 170/300, Loss: 0.0072, Train Acc: 78.55%, Val Acc: 31.37%\n",
            "Epoch 171/300, Loss: 0.0064, Train Acc: 81.00%, Val Acc: 33.82%\n",
            "Epoch 172/300, Loss: 0.0070, Train Acc: 77.94%, Val Acc: 33.33%\n",
            "Epoch 173/300, Loss: 0.0075, Train Acc: 76.10%, Val Acc: 37.25%\n",
            "Epoch 174/300, Loss: 0.0062, Train Acc: 79.29%, Val Acc: 35.29%\n",
            "Epoch 175/300, Loss: 0.0064, Train Acc: 80.76%, Val Acc: 36.76%\n",
            "Epoch 176/300, Loss: 0.0072, Train Acc: 78.80%, Val Acc: 33.82%\n",
            "Epoch 177/300, Loss: 0.0065, Train Acc: 80.51%, Val Acc: 34.31%\n",
            "Epoch 178/300, Loss: 0.0064, Train Acc: 79.04%, Val Acc: 35.29%\n",
            "Epoch 179/300, Loss: 0.0068, Train Acc: 78.92%, Val Acc: 36.27%\n",
            "Epoch 180/300, Loss: 0.0071, Train Acc: 78.80%, Val Acc: 31.86%\n",
            "Epoch 181/300, Loss: 0.0071, Train Acc: 78.31%, Val Acc: 36.27%\n",
            "Epoch 182/300, Loss: 0.0067, Train Acc: 82.48%, Val Acc: 36.27%\n",
            "Epoch 183/300, Loss: 0.0069, Train Acc: 79.17%, Val Acc: 37.25%\n",
            "Epoch 184/300, Loss: 0.0068, Train Acc: 78.80%, Val Acc: 40.20%\n",
            "Epoch 185/300, Loss: 0.0073, Train Acc: 78.43%, Val Acc: 33.33%\n",
            "Epoch 186/300, Loss: 0.0066, Train Acc: 80.15%, Val Acc: 29.90%\n",
            "Epoch 187/300, Loss: 0.0076, Train Acc: 78.31%, Val Acc: 32.84%\n",
            "Epoch 188/300, Loss: 0.0071, Train Acc: 78.31%, Val Acc: 32.84%\n",
            "Epoch 189/300, Loss: 0.0074, Train Acc: 78.68%, Val Acc: 36.76%\n",
            "Epoch 190/300, Loss: 0.0072, Train Acc: 77.33%, Val Acc: 35.78%\n",
            "Epoch 191/300, Loss: 0.0069, Train Acc: 78.80%, Val Acc: 30.88%\n",
            "Epoch 192/300, Loss: 0.0071, Train Acc: 79.90%, Val Acc: 35.78%\n",
            "Epoch 193/300, Loss: 0.0077, Train Acc: 78.68%, Val Acc: 38.73%\n",
            "Epoch 194/300, Loss: 0.0078, Train Acc: 77.57%, Val Acc: 33.82%\n",
            "Epoch 195/300, Loss: 0.0069, Train Acc: 78.68%, Val Acc: 32.35%\n",
            "Epoch 196/300, Loss: 0.0077, Train Acc: 76.96%, Val Acc: 34.31%\n",
            "Epoch 197/300, Loss: 0.0072, Train Acc: 79.78%, Val Acc: 34.31%\n",
            "Epoch 198/300, Loss: 0.0070, Train Acc: 79.29%, Val Acc: 32.35%\n",
            "Epoch 199/300, Loss: 0.0072, Train Acc: 77.08%, Val Acc: 34.31%\n",
            "Epoch 200/300, Loss: 0.0062, Train Acc: 80.27%, Val Acc: 33.33%\n",
            "Epoch 201/300, Loss: 0.0071, Train Acc: 80.88%, Val Acc: 32.35%\n",
            "Epoch 202/300, Loss: 0.0079, Train Acc: 78.68%, Val Acc: 36.76%\n",
            "Epoch 203/300, Loss: 0.0075, Train Acc: 78.19%, Val Acc: 33.33%\n",
            "Epoch 204/300, Loss: 0.0067, Train Acc: 79.90%, Val Acc: 33.33%\n",
            "Epoch 205/300, Loss: 0.0069, Train Acc: 79.66%, Val Acc: 38.73%\n",
            "Epoch 206/300, Loss: 0.0071, Train Acc: 80.27%, Val Acc: 33.33%\n",
            "Epoch 207/300, Loss: 0.0065, Train Acc: 81.62%, Val Acc: 35.78%\n",
            "Epoch 208/300, Loss: 0.0082, Train Acc: 77.57%, Val Acc: 33.33%\n",
            "Epoch 209/300, Loss: 0.0069, Train Acc: 80.76%, Val Acc: 34.31%\n",
            "Epoch 210/300, Loss: 0.0070, Train Acc: 78.19%, Val Acc: 38.73%\n",
            "Epoch 211/300, Loss: 0.0070, Train Acc: 79.78%, Val Acc: 33.82%\n",
            "Epoch 212/300, Loss: 0.0067, Train Acc: 79.78%, Val Acc: 35.29%\n",
            "Epoch 213/300, Loss: 0.0066, Train Acc: 79.66%, Val Acc: 34.80%\n",
            "Epoch 214/300, Loss: 0.0071, Train Acc: 79.66%, Val Acc: 35.29%\n",
            "Epoch 215/300, Loss: 0.0069, Train Acc: 79.66%, Val Acc: 35.78%\n",
            "Epoch 216/300, Loss: 0.0070, Train Acc: 79.66%, Val Acc: 32.35%\n",
            "Epoch 217/300, Loss: 0.0073, Train Acc: 78.80%, Val Acc: 33.33%\n",
            "Epoch 218/300, Loss: 0.0066, Train Acc: 79.04%, Val Acc: 32.35%\n",
            "Epoch 219/300, Loss: 0.0072, Train Acc: 79.04%, Val Acc: 31.86%\n",
            "Epoch 220/300, Loss: 0.0064, Train Acc: 79.29%, Val Acc: 37.25%\n",
            "Epoch 221/300, Loss: 0.0075, Train Acc: 76.35%, Val Acc: 36.27%\n",
            "Epoch 222/300, Loss: 0.0077, Train Acc: 78.68%, Val Acc: 34.80%\n",
            "Epoch 223/300, Loss: 0.0077, Train Acc: 78.55%, Val Acc: 33.33%\n",
            "Epoch 224/300, Loss: 0.0070, Train Acc: 79.66%, Val Acc: 34.31%\n",
            "Epoch 225/300, Loss: 0.0067, Train Acc: 80.51%, Val Acc: 33.33%\n",
            "Epoch 226/300, Loss: 0.0065, Train Acc: 80.15%, Val Acc: 36.27%\n",
            "Epoch 227/300, Loss: 0.0072, Train Acc: 81.25%, Val Acc: 36.27%\n",
            "Epoch 228/300, Loss: 0.0075, Train Acc: 75.74%, Val Acc: 32.35%\n",
            "Epoch 229/300, Loss: 0.0063, Train Acc: 81.00%, Val Acc: 30.88%\n",
            "Epoch 230/300, Loss: 0.0066, Train Acc: 81.37%, Val Acc: 36.76%\n",
            "Epoch 231/300, Loss: 0.0071, Train Acc: 77.82%, Val Acc: 33.82%\n",
            "Epoch 232/300, Loss: 0.0069, Train Acc: 79.17%, Val Acc: 35.78%\n",
            "Epoch 233/300, Loss: 0.0075, Train Acc: 76.10%, Val Acc: 35.78%\n",
            "Epoch 234/300, Loss: 0.0075, Train Acc: 78.68%, Val Acc: 34.31%\n",
            "Epoch 235/300, Loss: 0.0074, Train Acc: 78.68%, Val Acc: 32.84%\n",
            "Epoch 236/300, Loss: 0.0073, Train Acc: 78.68%, Val Acc: 33.33%\n",
            "Epoch 237/300, Loss: 0.0071, Train Acc: 79.04%, Val Acc: 32.35%\n",
            "Epoch 238/300, Loss: 0.0072, Train Acc: 78.31%, Val Acc: 35.29%\n",
            "Epoch 239/300, Loss: 0.0068, Train Acc: 80.02%, Val Acc: 35.29%\n",
            "Epoch 240/300, Loss: 0.0068, Train Acc: 81.00%, Val Acc: 32.84%\n",
            "Epoch 241/300, Loss: 0.0073, Train Acc: 77.82%, Val Acc: 35.29%\n",
            "Epoch 242/300, Loss: 0.0057, Train Acc: 83.09%, Val Acc: 33.33%\n",
            "Epoch 243/300, Loss: 0.0073, Train Acc: 78.92%, Val Acc: 40.69%\n",
            "Epoch 244/300, Loss: 0.0080, Train Acc: 75.74%, Val Acc: 36.76%\n",
            "Epoch 245/300, Loss: 0.0085, Train Acc: 77.57%, Val Acc: 39.71%\n",
            "Epoch 246/300, Loss: 0.0065, Train Acc: 81.37%, Val Acc: 33.33%\n",
            "Epoch 247/300, Loss: 0.0061, Train Acc: 80.76%, Val Acc: 37.75%\n",
            "Epoch 248/300, Loss: 0.0066, Train Acc: 79.66%, Val Acc: 28.43%\n",
            "Epoch 249/300, Loss: 0.0072, Train Acc: 78.55%, Val Acc: 36.27%\n",
            "Epoch 250/300, Loss: 0.0074, Train Acc: 77.08%, Val Acc: 33.33%\n",
            "Epoch 251/300, Loss: 0.0070, Train Acc: 76.59%, Val Acc: 34.80%\n",
            "Epoch 252/300, Loss: 0.0073, Train Acc: 78.80%, Val Acc: 34.31%\n",
            "Epoch 253/300, Loss: 0.0079, Train Acc: 75.86%, Val Acc: 33.82%\n",
            "Epoch 254/300, Loss: 0.0068, Train Acc: 78.19%, Val Acc: 37.75%\n",
            "Epoch 255/300, Loss: 0.0070, Train Acc: 79.29%, Val Acc: 36.76%\n",
            "Epoch 256/300, Loss: 0.0075, Train Acc: 77.70%, Val Acc: 35.29%\n",
            "Epoch 257/300, Loss: 0.0062, Train Acc: 81.86%, Val Acc: 35.78%\n",
            "Epoch 258/300, Loss: 0.0077, Train Acc: 77.08%, Val Acc: 30.39%\n",
            "Epoch 259/300, Loss: 0.0067, Train Acc: 81.13%, Val Acc: 34.80%\n",
            "Epoch 260/300, Loss: 0.0078, Train Acc: 78.19%, Val Acc: 33.33%\n",
            "Epoch 261/300, Loss: 0.0079, Train Acc: 76.23%, Val Acc: 32.84%\n",
            "Epoch 262/300, Loss: 0.0066, Train Acc: 79.04%, Val Acc: 36.27%\n",
            "Epoch 263/300, Loss: 0.0072, Train Acc: 80.88%, Val Acc: 33.82%\n",
            "Epoch 264/300, Loss: 0.0071, Train Acc: 79.41%, Val Acc: 35.29%\n",
            "Epoch 265/300, Loss: 0.0077, Train Acc: 79.53%, Val Acc: 36.76%\n",
            "Epoch 266/300, Loss: 0.0075, Train Acc: 78.92%, Val Acc: 30.39%\n",
            "Epoch 267/300, Loss: 0.0072, Train Acc: 78.92%, Val Acc: 38.24%\n",
            "Epoch 268/300, Loss: 0.0065, Train Acc: 79.17%, Val Acc: 34.80%\n",
            "Epoch 269/300, Loss: 0.0075, Train Acc: 78.43%, Val Acc: 30.88%\n",
            "Epoch 270/300, Loss: 0.0076, Train Acc: 78.43%, Val Acc: 33.82%\n",
            "Epoch 271/300, Loss: 0.0071, Train Acc: 78.31%, Val Acc: 38.24%\n",
            "Epoch 272/300, Loss: 0.0068, Train Acc: 78.92%, Val Acc: 33.33%\n",
            "Epoch 273/300, Loss: 0.0071, Train Acc: 78.92%, Val Acc: 33.82%\n",
            "Epoch 274/300, Loss: 0.0072, Train Acc: 77.57%, Val Acc: 33.82%\n",
            "Epoch 275/300, Loss: 0.0072, Train Acc: 78.55%, Val Acc: 35.29%\n",
            "Epoch 276/300, Loss: 0.0079, Train Acc: 77.82%, Val Acc: 35.78%\n",
            "Epoch 277/300, Loss: 0.0071, Train Acc: 79.41%, Val Acc: 30.39%\n",
            "Epoch 278/300, Loss: 0.0065, Train Acc: 79.04%, Val Acc: 37.75%\n",
            "Epoch 279/300, Loss: 0.0074, Train Acc: 78.06%, Val Acc: 33.82%\n",
            "Epoch 280/300, Loss: 0.0079, Train Acc: 76.23%, Val Acc: 34.31%\n",
            "Epoch 281/300, Loss: 0.0072, Train Acc: 78.92%, Val Acc: 37.25%\n",
            "Epoch 282/300, Loss: 0.0064, Train Acc: 80.64%, Val Acc: 31.37%\n",
            "Epoch 283/300, Loss: 0.0075, Train Acc: 79.41%, Val Acc: 35.29%\n",
            "Epoch 284/300, Loss: 0.0069, Train Acc: 79.66%, Val Acc: 33.82%\n",
            "Epoch 285/300, Loss: 0.0077, Train Acc: 77.21%, Val Acc: 38.73%\n",
            "Epoch 286/300, Loss: 0.0058, Train Acc: 82.84%, Val Acc: 31.86%\n",
            "Epoch 287/300, Loss: 0.0070, Train Acc: 79.53%, Val Acc: 32.84%\n",
            "Epoch 288/300, Loss: 0.0071, Train Acc: 80.39%, Val Acc: 34.31%\n",
            "Epoch 289/300, Loss: 0.0071, Train Acc: 79.41%, Val Acc: 34.31%\n",
            "Epoch 290/300, Loss: 0.0074, Train Acc: 77.57%, Val Acc: 36.27%\n",
            "Epoch 291/300, Loss: 0.0071, Train Acc: 79.90%, Val Acc: 37.75%\n",
            "Epoch 292/300, Loss: 0.0069, Train Acc: 78.55%, Val Acc: 33.82%\n",
            "Epoch 293/300, Loss: 0.0078, Train Acc: 76.72%, Val Acc: 36.27%\n",
            "Epoch 294/300, Loss: 0.0068, Train Acc: 80.27%, Val Acc: 39.22%\n",
            "Epoch 295/300, Loss: 0.0065, Train Acc: 79.78%, Val Acc: 31.86%\n",
            "Epoch 296/300, Loss: 0.0073, Train Acc: 76.59%, Val Acc: 33.82%\n",
            "Epoch 297/300, Loss: 0.0069, Train Acc: 79.66%, Val Acc: 36.27%\n",
            "Epoch 298/300, Loss: 0.0070, Train Acc: 78.55%, Val Acc: 34.31%\n",
            "Epoch 299/300, Loss: 0.0061, Train Acc: 80.27%, Val Acc: 35.29%\n",
            "Epoch 300/300, Loss: 0.0064, Train Acc: 80.27%, Val Acc: 37.25%\n",
            "Training complete\n",
            "Test Accuracy: 38.87%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(192),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(192),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "initial_batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=initial_batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=initial_batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=initial_batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(512 * 12 * 12, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 512 * 12 * 12)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "learning_rate = 0.0005\n",
        "weight_decay = 0.01\n",
        "dropout_rate = 0.5\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "\n",
        "def train_model(model, train_dataset, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=500):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        if epoch < 100:\n",
        "            batch_size = 64\n",
        "        elif epoch < 200:\n",
        "            batch_size = 32\n",
        "        elif epoch < 300:\n",
        "            batch_size = 64\n",
        "        else:\n",
        "            batch_size = 128\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    return model\n",
        "\n",
        "\n",
        "trained_model = train_model(model, train_dataset, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=500)\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzOzl0rCG123",
        "outputId": "c65c69ea-ccad-4528-d5c0-daed73f588ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Batch Size: 64, Loss: 0.3639, Train Acc: 2.21%, Val Acc: 1.47%\n",
            "Epoch 2/500, Batch Size: 64, Loss: 0.1867, Train Acc: 2.82%, Val Acc: 2.94%\n",
            "Epoch 3/500, Batch Size: 64, Loss: 0.1438, Train Acc: 3.80%, Val Acc: 3.92%\n",
            "Epoch 4/500, Batch Size: 64, Loss: 0.1168, Train Acc: 3.19%, Val Acc: 5.88%\n",
            "Epoch 5/500, Batch Size: 64, Loss: 0.1027, Train Acc: 4.17%, Val Acc: 4.41%\n",
            "Epoch 6/500, Batch Size: 64, Loss: 0.0877, Train Acc: 4.78%, Val Acc: 6.37%\n",
            "Epoch 7/500, Batch Size: 64, Loss: 0.0858, Train Acc: 4.90%, Val Acc: 4.90%\n",
            "Epoch 8/500, Batch Size: 64, Loss: 0.0830, Train Acc: 4.66%, Val Acc: 4.41%\n",
            "Epoch 9/500, Batch Size: 64, Loss: 0.0798, Train Acc: 7.11%, Val Acc: 6.37%\n",
            "Epoch 10/500, Batch Size: 64, Loss: 0.0817, Train Acc: 6.74%, Val Acc: 3.92%\n",
            "Epoch 11/500, Batch Size: 64, Loss: 0.0746, Train Acc: 9.56%, Val Acc: 5.88%\n",
            "Epoch 12/500, Batch Size: 64, Loss: 0.0734, Train Acc: 9.07%, Val Acc: 5.88%\n",
            "Epoch 13/500, Batch Size: 64, Loss: 0.0740, Train Acc: 9.31%, Val Acc: 5.88%\n",
            "Epoch 14/500, Batch Size: 64, Loss: 0.0708, Train Acc: 11.64%, Val Acc: 9.31%\n",
            "Epoch 15/500, Batch Size: 64, Loss: 0.0707, Train Acc: 9.80%, Val Acc: 5.88%\n",
            "Epoch 16/500, Batch Size: 64, Loss: 0.0676, Train Acc: 10.42%, Val Acc: 7.35%\n",
            "Epoch 17/500, Batch Size: 64, Loss: 0.0684, Train Acc: 11.03%, Val Acc: 8.33%\n",
            "Epoch 18/500, Batch Size: 64, Loss: 0.0685, Train Acc: 13.60%, Val Acc: 8.33%\n",
            "Epoch 19/500, Batch Size: 64, Loss: 0.0638, Train Acc: 12.99%, Val Acc: 11.76%\n",
            "Epoch 20/500, Batch Size: 64, Loss: 0.0636, Train Acc: 15.07%, Val Acc: 6.86%\n",
            "Epoch 21/500, Batch Size: 64, Loss: 0.0583, Train Acc: 16.05%, Val Acc: 14.22%\n",
            "Epoch 22/500, Batch Size: 64, Loss: 0.0514, Train Acc: 21.20%, Val Acc: 12.75%\n",
            "Epoch 23/500, Batch Size: 64, Loss: 0.0515, Train Acc: 19.73%, Val Acc: 16.18%\n",
            "Epoch 24/500, Batch Size: 64, Loss: 0.0509, Train Acc: 22.55%, Val Acc: 19.12%\n",
            "Epoch 25/500, Batch Size: 64, Loss: 0.0501, Train Acc: 21.08%, Val Acc: 13.24%\n",
            "Epoch 26/500, Batch Size: 64, Loss: 0.0507, Train Acc: 22.06%, Val Acc: 13.24%\n",
            "Epoch 27/500, Batch Size: 64, Loss: 0.0483, Train Acc: 24.51%, Val Acc: 13.73%\n",
            "Epoch 28/500, Batch Size: 64, Loss: 0.0489, Train Acc: 24.88%, Val Acc: 17.16%\n",
            "Epoch 29/500, Batch Size: 64, Loss: 0.0486, Train Acc: 24.51%, Val Acc: 16.67%\n",
            "Epoch 30/500, Batch Size: 64, Loss: 0.0472, Train Acc: 26.23%, Val Acc: 14.22%\n",
            "Epoch 31/500, Batch Size: 64, Loss: 0.0472, Train Acc: 25.98%, Val Acc: 15.69%\n",
            "Epoch 32/500, Batch Size: 64, Loss: 0.0474, Train Acc: 26.84%, Val Acc: 19.61%\n",
            "Epoch 33/500, Batch Size: 64, Loss: 0.0490, Train Acc: 26.59%, Val Acc: 17.65%\n",
            "Epoch 34/500, Batch Size: 64, Loss: 0.0472, Train Acc: 27.33%, Val Acc: 22.55%\n",
            "Epoch 35/500, Batch Size: 64, Loss: 0.0476, Train Acc: 27.82%, Val Acc: 14.22%\n",
            "Epoch 36/500, Batch Size: 64, Loss: 0.0483, Train Acc: 23.16%, Val Acc: 17.65%\n",
            "Epoch 37/500, Batch Size: 64, Loss: 0.0483, Train Acc: 25.74%, Val Acc: 15.20%\n",
            "Epoch 38/500, Batch Size: 64, Loss: 0.0468, Train Acc: 27.57%, Val Acc: 22.55%\n",
            "Epoch 39/500, Batch Size: 64, Loss: 0.0440, Train Acc: 30.64%, Val Acc: 19.12%\n",
            "Epoch 40/500, Batch Size: 64, Loss: 0.0462, Train Acc: 26.96%, Val Acc: 16.67%\n",
            "Epoch 41/500, Batch Size: 64, Loss: 0.0444, Train Acc: 32.84%, Val Acc: 25.98%\n",
            "Epoch 42/500, Batch Size: 64, Loss: 0.0401, Train Acc: 34.44%, Val Acc: 21.08%\n",
            "Epoch 43/500, Batch Size: 64, Loss: 0.0409, Train Acc: 34.07%, Val Acc: 16.67%\n",
            "Epoch 44/500, Batch Size: 64, Loss: 0.0415, Train Acc: 32.97%, Val Acc: 19.12%\n",
            "Epoch 45/500, Batch Size: 64, Loss: 0.0398, Train Acc: 35.29%, Val Acc: 23.04%\n",
            "Epoch 46/500, Batch Size: 64, Loss: 0.0388, Train Acc: 37.01%, Val Acc: 23.04%\n",
            "Epoch 47/500, Batch Size: 64, Loss: 0.0388, Train Acc: 37.87%, Val Acc: 21.57%\n",
            "Epoch 48/500, Batch Size: 64, Loss: 0.0392, Train Acc: 35.78%, Val Acc: 23.04%\n",
            "Epoch 49/500, Batch Size: 64, Loss: 0.0398, Train Acc: 36.27%, Val Acc: 23.53%\n",
            "Epoch 50/500, Batch Size: 64, Loss: 0.0396, Train Acc: 35.54%, Val Acc: 22.55%\n",
            "Epoch 51/500, Batch Size: 64, Loss: 0.0391, Train Acc: 38.24%, Val Acc: 26.47%\n",
            "Epoch 52/500, Batch Size: 64, Loss: 0.0386, Train Acc: 38.73%, Val Acc: 23.53%\n",
            "Epoch 53/500, Batch Size: 64, Loss: 0.0397, Train Acc: 35.42%, Val Acc: 20.59%\n",
            "Epoch 54/500, Batch Size: 64, Loss: 0.0354, Train Acc: 42.16%, Val Acc: 23.53%\n",
            "Epoch 55/500, Batch Size: 64, Loss: 0.0384, Train Acc: 38.36%, Val Acc: 21.57%\n",
            "Epoch 56/500, Batch Size: 64, Loss: 0.0368, Train Acc: 39.22%, Val Acc: 20.59%\n",
            "Epoch 57/500, Batch Size: 64, Loss: 0.0376, Train Acc: 38.11%, Val Acc: 20.10%\n",
            "Epoch 58/500, Batch Size: 64, Loss: 0.0380, Train Acc: 37.01%, Val Acc: 25.98%\n",
            "Epoch 59/500, Batch Size: 64, Loss: 0.0375, Train Acc: 39.58%, Val Acc: 20.59%\n",
            "Epoch 60/500, Batch Size: 64, Loss: 0.0361, Train Acc: 37.62%, Val Acc: 23.04%\n",
            "Epoch 61/500, Batch Size: 64, Loss: 0.0349, Train Acc: 41.91%, Val Acc: 24.02%\n",
            "Epoch 62/500, Batch Size: 64, Loss: 0.0346, Train Acc: 43.87%, Val Acc: 26.96%\n",
            "Epoch 63/500, Batch Size: 64, Loss: 0.0321, Train Acc: 48.28%, Val Acc: 22.55%\n",
            "Epoch 64/500, Batch Size: 64, Loss: 0.0335, Train Acc: 46.08%, Val Acc: 25.98%\n",
            "Epoch 65/500, Batch Size: 64, Loss: 0.0329, Train Acc: 44.49%, Val Acc: 28.92%\n",
            "Epoch 66/500, Batch Size: 64, Loss: 0.0334, Train Acc: 44.12%, Val Acc: 23.53%\n",
            "Epoch 67/500, Batch Size: 64, Loss: 0.0331, Train Acc: 45.10%, Val Acc: 23.53%\n",
            "Epoch 68/500, Batch Size: 64, Loss: 0.0316, Train Acc: 47.67%, Val Acc: 24.51%\n",
            "Epoch 69/500, Batch Size: 64, Loss: 0.0341, Train Acc: 43.87%, Val Acc: 24.02%\n",
            "Epoch 70/500, Batch Size: 64, Loss: 0.0316, Train Acc: 50.49%, Val Acc: 21.57%\n",
            "Epoch 71/500, Batch Size: 64, Loss: 0.0333, Train Acc: 45.59%, Val Acc: 22.55%\n",
            "Epoch 72/500, Batch Size: 64, Loss: 0.0309, Train Acc: 48.28%, Val Acc: 28.43%\n",
            "Epoch 73/500, Batch Size: 64, Loss: 0.0319, Train Acc: 46.20%, Val Acc: 24.51%\n",
            "Epoch 74/500, Batch Size: 64, Loss: 0.0313, Train Acc: 48.16%, Val Acc: 25.98%\n",
            "Epoch 75/500, Batch Size: 64, Loss: 0.0315, Train Acc: 48.04%, Val Acc: 24.02%\n",
            "Epoch 76/500, Batch Size: 64, Loss: 0.0324, Train Acc: 45.71%, Val Acc: 26.47%\n",
            "Epoch 77/500, Batch Size: 64, Loss: 0.0322, Train Acc: 48.65%, Val Acc: 28.43%\n",
            "Epoch 78/500, Batch Size: 64, Loss: 0.0321, Train Acc: 48.53%, Val Acc: 25.00%\n",
            "Epoch 79/500, Batch Size: 64, Loss: 0.0306, Train Acc: 49.14%, Val Acc: 26.47%\n",
            "Epoch 80/500, Batch Size: 64, Loss: 0.0311, Train Acc: 50.00%, Val Acc: 27.94%\n",
            "Epoch 81/500, Batch Size: 64, Loss: 0.0305, Train Acc: 48.65%, Val Acc: 32.35%\n",
            "Epoch 82/500, Batch Size: 64, Loss: 0.0297, Train Acc: 50.49%, Val Acc: 30.88%\n",
            "Epoch 83/500, Batch Size: 64, Loss: 0.0297, Train Acc: 52.82%, Val Acc: 28.92%\n",
            "Epoch 84/500, Batch Size: 64, Loss: 0.0300, Train Acc: 49.88%, Val Acc: 25.49%\n",
            "Epoch 85/500, Batch Size: 64, Loss: 0.0304, Train Acc: 51.96%, Val Acc: 25.49%\n",
            "Epoch 86/500, Batch Size: 64, Loss: 0.0302, Train Acc: 51.47%, Val Acc: 29.90%\n",
            "Epoch 87/500, Batch Size: 64, Loss: 0.0282, Train Acc: 52.57%, Val Acc: 26.47%\n",
            "Epoch 88/500, Batch Size: 64, Loss: 0.0298, Train Acc: 50.86%, Val Acc: 25.49%\n",
            "Epoch 89/500, Batch Size: 64, Loss: 0.0299, Train Acc: 51.47%, Val Acc: 29.41%\n",
            "Epoch 90/500, Batch Size: 64, Loss: 0.0292, Train Acc: 50.98%, Val Acc: 26.47%\n",
            "Epoch 91/500, Batch Size: 64, Loss: 0.0294, Train Acc: 51.84%, Val Acc: 26.47%\n",
            "Epoch 92/500, Batch Size: 64, Loss: 0.0280, Train Acc: 53.31%, Val Acc: 27.94%\n",
            "Epoch 93/500, Batch Size: 64, Loss: 0.0291, Train Acc: 51.84%, Val Acc: 26.96%\n",
            "Epoch 94/500, Batch Size: 64, Loss: 0.0288, Train Acc: 51.35%, Val Acc: 27.45%\n",
            "Epoch 95/500, Batch Size: 64, Loss: 0.0288, Train Acc: 52.94%, Val Acc: 28.92%\n",
            "Epoch 96/500, Batch Size: 64, Loss: 0.0299, Train Acc: 49.51%, Val Acc: 29.90%\n",
            "Epoch 97/500, Batch Size: 64, Loss: 0.0284, Train Acc: 54.41%, Val Acc: 29.90%\n",
            "Epoch 98/500, Batch Size: 64, Loss: 0.0257, Train Acc: 56.37%, Val Acc: 31.37%\n",
            "Epoch 99/500, Batch Size: 64, Loss: 0.0277, Train Acc: 55.15%, Val Acc: 32.84%\n",
            "Epoch 100/500, Batch Size: 64, Loss: 0.0285, Train Acc: 52.45%, Val Acc: 34.31%\n",
            "Epoch 101/500, Batch Size: 32, Loss: 0.0613, Train Acc: 50.37%, Val Acc: 31.37%\n",
            "Epoch 102/500, Batch Size: 32, Loss: 0.0610, Train Acc: 49.14%, Val Acc: 27.45%\n",
            "Epoch 103/500, Batch Size: 32, Loss: 0.0588, Train Acc: 52.21%, Val Acc: 25.49%\n",
            "Epoch 104/500, Batch Size: 32, Loss: 0.0597, Train Acc: 50.98%, Val Acc: 25.49%\n",
            "Epoch 105/500, Batch Size: 32, Loss: 0.0573, Train Acc: 51.35%, Val Acc: 30.88%\n",
            "Epoch 106/500, Batch Size: 32, Loss: 0.0618, Train Acc: 51.23%, Val Acc: 30.39%\n",
            "Epoch 107/500, Batch Size: 32, Loss: 0.0586, Train Acc: 51.23%, Val Acc: 28.43%\n",
            "Epoch 108/500, Batch Size: 32, Loss: 0.0588, Train Acc: 50.98%, Val Acc: 28.43%\n",
            "Epoch 109/500, Batch Size: 32, Loss: 0.0595, Train Acc: 51.59%, Val Acc: 28.92%\n",
            "Epoch 110/500, Batch Size: 32, Loss: 0.0594, Train Acc: 50.37%, Val Acc: 28.43%\n",
            "Epoch 111/500, Batch Size: 32, Loss: 0.0589, Train Acc: 51.72%, Val Acc: 25.49%\n",
            "Epoch 112/500, Batch Size: 32, Loss: 0.0583, Train Acc: 49.88%, Val Acc: 28.92%\n",
            "Epoch 113/500, Batch Size: 32, Loss: 0.0566, Train Acc: 53.92%, Val Acc: 25.98%\n",
            "Epoch 114/500, Batch Size: 32, Loss: 0.0567, Train Acc: 54.04%, Val Acc: 28.43%\n",
            "Epoch 115/500, Batch Size: 32, Loss: 0.0562, Train Acc: 55.51%, Val Acc: 25.98%\n",
            "Epoch 116/500, Batch Size: 32, Loss: 0.0574, Train Acc: 53.68%, Val Acc: 27.94%\n",
            "Epoch 117/500, Batch Size: 32, Loss: 0.0590, Train Acc: 52.70%, Val Acc: 29.41%\n",
            "Epoch 118/500, Batch Size: 32, Loss: 0.0576, Train Acc: 53.31%, Val Acc: 30.39%\n",
            "Epoch 119/500, Batch Size: 32, Loss: 0.0566, Train Acc: 54.29%, Val Acc: 30.88%\n",
            "Epoch 120/500, Batch Size: 32, Loss: 0.0566, Train Acc: 51.72%, Val Acc: 30.39%\n",
            "Epoch 121/500, Batch Size: 32, Loss: 0.0557, Train Acc: 53.68%, Val Acc: 27.94%\n",
            "Epoch 122/500, Batch Size: 32, Loss: 0.0573, Train Acc: 54.29%, Val Acc: 26.47%\n",
            "Epoch 123/500, Batch Size: 32, Loss: 0.0537, Train Acc: 57.11%, Val Acc: 30.88%\n",
            "Epoch 124/500, Batch Size: 32, Loss: 0.0564, Train Acc: 51.96%, Val Acc: 29.41%\n",
            "Epoch 125/500, Batch Size: 32, Loss: 0.0563, Train Acc: 54.04%, Val Acc: 30.39%\n",
            "Epoch 126/500, Batch Size: 32, Loss: 0.0559, Train Acc: 55.02%, Val Acc: 25.49%\n",
            "Epoch 127/500, Batch Size: 32, Loss: 0.0531, Train Acc: 55.88%, Val Acc: 27.94%\n",
            "Epoch 128/500, Batch Size: 32, Loss: 0.0531, Train Acc: 57.72%, Val Acc: 28.92%\n",
            "Epoch 129/500, Batch Size: 32, Loss: 0.0549, Train Acc: 53.92%, Val Acc: 27.94%\n",
            "Epoch 130/500, Batch Size: 32, Loss: 0.0529, Train Acc: 54.29%, Val Acc: 28.92%\n",
            "Epoch 131/500, Batch Size: 32, Loss: 0.0534, Train Acc: 56.13%, Val Acc: 34.31%\n",
            "Epoch 132/500, Batch Size: 32, Loss: 0.0566, Train Acc: 55.02%, Val Acc: 28.43%\n",
            "Epoch 133/500, Batch Size: 32, Loss: 0.0568, Train Acc: 53.92%, Val Acc: 30.88%\n",
            "Epoch 134/500, Batch Size: 32, Loss: 0.0536, Train Acc: 55.02%, Val Acc: 29.41%\n",
            "Epoch 135/500, Batch Size: 32, Loss: 0.0508, Train Acc: 58.33%, Val Acc: 30.88%\n",
            "Epoch 136/500, Batch Size: 32, Loss: 0.0533, Train Acc: 56.25%, Val Acc: 27.94%\n",
            "Epoch 137/500, Batch Size: 32, Loss: 0.0552, Train Acc: 55.02%, Val Acc: 29.41%\n",
            "Epoch 138/500, Batch Size: 32, Loss: 0.0546, Train Acc: 56.50%, Val Acc: 38.24%\n",
            "Epoch 139/500, Batch Size: 32, Loss: 0.0539, Train Acc: 57.60%, Val Acc: 30.88%\n",
            "Epoch 140/500, Batch Size: 32, Loss: 0.0546, Train Acc: 55.02%, Val Acc: 27.45%\n",
            "Epoch 141/500, Batch Size: 32, Loss: 0.0550, Train Acc: 53.92%, Val Acc: 34.80%\n",
            "Epoch 142/500, Batch Size: 32, Loss: 0.0537, Train Acc: 53.92%, Val Acc: 31.86%\n",
            "Epoch 143/500, Batch Size: 32, Loss: 0.0520, Train Acc: 56.25%, Val Acc: 31.37%\n",
            "Epoch 144/500, Batch Size: 32, Loss: 0.0511, Train Acc: 57.84%, Val Acc: 28.43%\n",
            "Epoch 145/500, Batch Size: 32, Loss: 0.0518, Train Acc: 56.86%, Val Acc: 31.37%\n",
            "Epoch 146/500, Batch Size: 32, Loss: 0.0500, Train Acc: 57.97%, Val Acc: 32.84%\n",
            "Epoch 147/500, Batch Size: 32, Loss: 0.0519, Train Acc: 57.11%, Val Acc: 25.00%\n",
            "Epoch 148/500, Batch Size: 32, Loss: 0.0555, Train Acc: 53.55%, Val Acc: 32.35%\n",
            "Epoch 149/500, Batch Size: 32, Loss: 0.0548, Train Acc: 54.53%, Val Acc: 29.41%\n",
            "Epoch 150/500, Batch Size: 32, Loss: 0.0539, Train Acc: 55.15%, Val Acc: 30.88%\n",
            "Epoch 151/500, Batch Size: 32, Loss: 0.0493, Train Acc: 59.93%, Val Acc: 29.41%\n",
            "Epoch 152/500, Batch Size: 32, Loss: 0.0559, Train Acc: 54.04%, Val Acc: 24.02%\n",
            "Epoch 153/500, Batch Size: 32, Loss: 0.0504, Train Acc: 58.33%, Val Acc: 29.41%\n",
            "Epoch 154/500, Batch Size: 32, Loss: 0.0519, Train Acc: 58.21%, Val Acc: 29.41%\n",
            "Epoch 155/500, Batch Size: 32, Loss: 0.0518, Train Acc: 57.48%, Val Acc: 30.88%\n",
            "Epoch 156/500, Batch Size: 32, Loss: 0.0523, Train Acc: 58.58%, Val Acc: 27.94%\n",
            "Epoch 157/500, Batch Size: 32, Loss: 0.0562, Train Acc: 52.70%, Val Acc: 28.92%\n",
            "Epoch 158/500, Batch Size: 32, Loss: 0.0518, Train Acc: 58.21%, Val Acc: 31.86%\n",
            "Epoch 159/500, Batch Size: 32, Loss: 0.0520, Train Acc: 57.60%, Val Acc: 29.90%\n",
            "Epoch 160/500, Batch Size: 32, Loss: 0.0515, Train Acc: 54.41%, Val Acc: 30.88%\n",
            "Epoch 161/500, Batch Size: 32, Loss: 0.0515, Train Acc: 55.88%, Val Acc: 31.86%\n",
            "Epoch 162/500, Batch Size: 32, Loss: 0.0491, Train Acc: 61.64%, Val Acc: 30.88%\n",
            "Epoch 163/500, Batch Size: 32, Loss: 0.0536, Train Acc: 55.76%, Val Acc: 34.31%\n",
            "Epoch 164/500, Batch Size: 32, Loss: 0.0500, Train Acc: 56.25%, Val Acc: 30.39%\n",
            "Epoch 165/500, Batch Size: 32, Loss: 0.0517, Train Acc: 57.72%, Val Acc: 33.82%\n",
            "Epoch 166/500, Batch Size: 32, Loss: 0.0504, Train Acc: 56.86%, Val Acc: 26.96%\n",
            "Epoch 167/500, Batch Size: 32, Loss: 0.0521, Train Acc: 59.19%, Val Acc: 29.41%\n",
            "Epoch 168/500, Batch Size: 32, Loss: 0.0515, Train Acc: 57.97%, Val Acc: 31.86%\n",
            "Epoch 169/500, Batch Size: 32, Loss: 0.0519, Train Acc: 58.95%, Val Acc: 30.88%\n",
            "Epoch 170/500, Batch Size: 32, Loss: 0.0488, Train Acc: 59.31%, Val Acc: 33.33%\n",
            "Epoch 171/500, Batch Size: 32, Loss: 0.0499, Train Acc: 59.07%, Val Acc: 32.84%\n",
            "Epoch 172/500, Batch Size: 32, Loss: 0.0518, Train Acc: 57.35%, Val Acc: 27.94%\n",
            "Epoch 173/500, Batch Size: 32, Loss: 0.0520, Train Acc: 56.74%, Val Acc: 30.39%\n",
            "Epoch 174/500, Batch Size: 32, Loss: 0.0543, Train Acc: 56.25%, Val Acc: 28.92%\n",
            "Epoch 175/500, Batch Size: 32, Loss: 0.0504, Train Acc: 58.82%, Val Acc: 28.92%\n",
            "Epoch 176/500, Batch Size: 32, Loss: 0.0549, Train Acc: 54.41%, Val Acc: 31.37%\n",
            "Epoch 177/500, Batch Size: 32, Loss: 0.0508, Train Acc: 58.21%, Val Acc: 29.90%\n",
            "Epoch 178/500, Batch Size: 32, Loss: 0.0521, Train Acc: 55.51%, Val Acc: 35.29%\n",
            "Epoch 179/500, Batch Size: 32, Loss: 0.0485, Train Acc: 59.19%, Val Acc: 33.33%\n",
            "Epoch 180/500, Batch Size: 32, Loss: 0.0542, Train Acc: 54.53%, Val Acc: 37.25%\n",
            "Epoch 181/500, Batch Size: 32, Loss: 0.0510, Train Acc: 57.72%, Val Acc: 29.90%\n",
            "Epoch 182/500, Batch Size: 32, Loss: 0.0503, Train Acc: 57.97%, Val Acc: 30.88%\n",
            "Epoch 183/500, Batch Size: 32, Loss: 0.0547, Train Acc: 55.39%, Val Acc: 27.94%\n",
            "Epoch 184/500, Batch Size: 32, Loss: 0.0500, Train Acc: 59.31%, Val Acc: 32.35%\n",
            "Epoch 185/500, Batch Size: 32, Loss: 0.0551, Train Acc: 55.76%, Val Acc: 29.90%\n",
            "Epoch 186/500, Batch Size: 32, Loss: 0.0512, Train Acc: 55.39%, Val Acc: 33.33%\n",
            "Epoch 187/500, Batch Size: 32, Loss: 0.0547, Train Acc: 55.15%, Val Acc: 30.88%\n",
            "Epoch 188/500, Batch Size: 32, Loss: 0.0498, Train Acc: 59.07%, Val Acc: 25.98%\n",
            "Epoch 189/500, Batch Size: 32, Loss: 0.0503, Train Acc: 57.48%, Val Acc: 28.43%\n",
            "Epoch 190/500, Batch Size: 32, Loss: 0.0539, Train Acc: 56.50%, Val Acc: 30.39%\n",
            "Epoch 191/500, Batch Size: 32, Loss: 0.0525, Train Acc: 56.13%, Val Acc: 27.45%\n",
            "Epoch 192/500, Batch Size: 32, Loss: 0.0490, Train Acc: 60.42%, Val Acc: 28.43%\n",
            "Epoch 193/500, Batch Size: 32, Loss: 0.0512, Train Acc: 59.80%, Val Acc: 30.39%\n",
            "Epoch 194/500, Batch Size: 32, Loss: 0.0560, Train Acc: 55.02%, Val Acc: 30.39%\n",
            "Epoch 195/500, Batch Size: 32, Loss: 0.0520, Train Acc: 58.33%, Val Acc: 30.88%\n",
            "Epoch 196/500, Batch Size: 32, Loss: 0.0492, Train Acc: 58.95%, Val Acc: 32.84%\n",
            "Epoch 197/500, Batch Size: 32, Loss: 0.0497, Train Acc: 60.29%, Val Acc: 31.37%\n",
            "Epoch 198/500, Batch Size: 32, Loss: 0.0454, Train Acc: 62.38%, Val Acc: 32.84%\n",
            "Epoch 199/500, Batch Size: 32, Loss: 0.0492, Train Acc: 57.84%, Val Acc: 33.33%\n",
            "Epoch 200/500, Batch Size: 32, Loss: 0.0505, Train Acc: 57.84%, Val Acc: 31.86%\n",
            "Epoch 201/500, Batch Size: 64, Loss: 0.0243, Train Acc: 61.89%, Val Acc: 29.90%\n",
            "Epoch 202/500, Batch Size: 64, Loss: 0.0253, Train Acc: 60.29%, Val Acc: 24.51%\n",
            "Epoch 203/500, Batch Size: 64, Loss: 0.0237, Train Acc: 60.42%, Val Acc: 27.45%\n",
            "Epoch 204/500, Batch Size: 64, Loss: 0.0242, Train Acc: 61.03%, Val Acc: 29.41%\n",
            "Epoch 205/500, Batch Size: 64, Loss: 0.0239, Train Acc: 61.03%, Val Acc: 32.35%\n",
            "Epoch 206/500, Batch Size: 64, Loss: 0.0248, Train Acc: 60.05%, Val Acc: 31.86%\n",
            "Epoch 207/500, Batch Size: 64, Loss: 0.0247, Train Acc: 58.58%, Val Acc: 30.88%\n",
            "Epoch 208/500, Batch Size: 64, Loss: 0.0239, Train Acc: 59.31%, Val Acc: 31.37%\n",
            "Epoch 209/500, Batch Size: 64, Loss: 0.0244, Train Acc: 58.95%, Val Acc: 29.90%\n",
            "Epoch 210/500, Batch Size: 64, Loss: 0.0250, Train Acc: 58.70%, Val Acc: 31.86%\n",
            "Epoch 211/500, Batch Size: 64, Loss: 0.0242, Train Acc: 60.29%, Val Acc: 29.41%\n",
            "Epoch 212/500, Batch Size: 64, Loss: 0.0236, Train Acc: 61.27%, Val Acc: 26.47%\n",
            "Epoch 213/500, Batch Size: 64, Loss: 0.0242, Train Acc: 57.11%, Val Acc: 32.84%\n",
            "Epoch 214/500, Batch Size: 64, Loss: 0.0231, Train Acc: 60.66%, Val Acc: 25.98%\n",
            "Epoch 215/500, Batch Size: 64, Loss: 0.0244, Train Acc: 60.29%, Val Acc: 29.90%\n",
            "Epoch 216/500, Batch Size: 64, Loss: 0.0244, Train Acc: 59.80%, Val Acc: 30.88%\n",
            "Epoch 217/500, Batch Size: 64, Loss: 0.0250, Train Acc: 59.07%, Val Acc: 30.39%\n",
            "Epoch 218/500, Batch Size: 64, Loss: 0.0243, Train Acc: 62.38%, Val Acc: 33.82%\n",
            "Epoch 219/500, Batch Size: 64, Loss: 0.0257, Train Acc: 57.97%, Val Acc: 33.33%\n",
            "Epoch 220/500, Batch Size: 64, Loss: 0.0245, Train Acc: 59.44%, Val Acc: 27.94%\n",
            "Epoch 221/500, Batch Size: 64, Loss: 0.0244, Train Acc: 59.68%, Val Acc: 31.37%\n",
            "Epoch 222/500, Batch Size: 64, Loss: 0.0255, Train Acc: 57.97%, Val Acc: 27.45%\n",
            "Epoch 223/500, Batch Size: 64, Loss: 0.0250, Train Acc: 60.42%, Val Acc: 28.43%\n",
            "Epoch 224/500, Batch Size: 64, Loss: 0.0231, Train Acc: 59.68%, Val Acc: 28.43%\n",
            "Epoch 225/500, Batch Size: 64, Loss: 0.0246, Train Acc: 57.84%, Val Acc: 31.37%\n",
            "Epoch 226/500, Batch Size: 64, Loss: 0.0253, Train Acc: 57.84%, Val Acc: 31.37%\n",
            "Epoch 227/500, Batch Size: 64, Loss: 0.0248, Train Acc: 60.29%, Val Acc: 32.35%\n",
            "Epoch 228/500, Batch Size: 64, Loss: 0.0249, Train Acc: 58.82%, Val Acc: 28.92%\n",
            "Epoch 229/500, Batch Size: 64, Loss: 0.0244, Train Acc: 59.68%, Val Acc: 31.37%\n",
            "Epoch 230/500, Batch Size: 64, Loss: 0.0232, Train Acc: 60.17%, Val Acc: 28.43%\n",
            "Epoch 231/500, Batch Size: 64, Loss: 0.0257, Train Acc: 57.72%, Val Acc: 34.80%\n",
            "Epoch 232/500, Batch Size: 64, Loss: 0.0221, Train Acc: 64.95%, Val Acc: 29.41%\n",
            "Epoch 233/500, Batch Size: 64, Loss: 0.0245, Train Acc: 57.97%, Val Acc: 30.39%\n",
            "Epoch 234/500, Batch Size: 64, Loss: 0.0235, Train Acc: 61.03%, Val Acc: 31.86%\n",
            "Epoch 235/500, Batch Size: 64, Loss: 0.0234, Train Acc: 61.64%, Val Acc: 31.37%\n",
            "Epoch 236/500, Batch Size: 64, Loss: 0.0250, Train Acc: 57.23%, Val Acc: 28.92%\n",
            "Epoch 237/500, Batch Size: 64, Loss: 0.0235, Train Acc: 60.05%, Val Acc: 31.37%\n",
            "Epoch 238/500, Batch Size: 64, Loss: 0.0236, Train Acc: 59.93%, Val Acc: 32.84%\n",
            "Epoch 239/500, Batch Size: 64, Loss: 0.0237, Train Acc: 59.93%, Val Acc: 33.33%\n",
            "Epoch 240/500, Batch Size: 64, Loss: 0.0231, Train Acc: 61.64%, Val Acc: 30.39%\n",
            "Epoch 241/500, Batch Size: 64, Loss: 0.0246, Train Acc: 60.29%, Val Acc: 29.41%\n",
            "Epoch 242/500, Batch Size: 64, Loss: 0.0241, Train Acc: 60.42%, Val Acc: 36.76%\n",
            "Epoch 243/500, Batch Size: 64, Loss: 0.0239, Train Acc: 61.15%, Val Acc: 29.41%\n",
            "Epoch 244/500, Batch Size: 64, Loss: 0.0237, Train Acc: 61.15%, Val Acc: 27.45%\n",
            "Epoch 245/500, Batch Size: 64, Loss: 0.0244, Train Acc: 59.80%, Val Acc: 29.90%\n",
            "Epoch 246/500, Batch Size: 64, Loss: 0.0247, Train Acc: 59.07%, Val Acc: 26.96%\n",
            "Epoch 247/500, Batch Size: 64, Loss: 0.0229, Train Acc: 62.25%, Val Acc: 27.94%\n",
            "Epoch 248/500, Batch Size: 64, Loss: 0.0233, Train Acc: 61.03%, Val Acc: 31.86%\n",
            "Epoch 249/500, Batch Size: 64, Loss: 0.0219, Train Acc: 63.85%, Val Acc: 30.39%\n",
            "Epoch 250/500, Batch Size: 64, Loss: 0.0242, Train Acc: 62.01%, Val Acc: 29.41%\n",
            "Epoch 251/500, Batch Size: 64, Loss: 0.0240, Train Acc: 59.56%, Val Acc: 32.84%\n",
            "Epoch 252/500, Batch Size: 64, Loss: 0.0241, Train Acc: 61.27%, Val Acc: 27.94%\n",
            "Epoch 253/500, Batch Size: 64, Loss: 0.0237, Train Acc: 60.29%, Val Acc: 30.88%\n",
            "Epoch 254/500, Batch Size: 64, Loss: 0.0237, Train Acc: 58.46%, Val Acc: 26.96%\n",
            "Epoch 255/500, Batch Size: 64, Loss: 0.0253, Train Acc: 57.97%, Val Acc: 31.86%\n",
            "Epoch 256/500, Batch Size: 64, Loss: 0.0244, Train Acc: 58.46%, Val Acc: 29.41%\n",
            "Epoch 257/500, Batch Size: 64, Loss: 0.0234, Train Acc: 60.91%, Val Acc: 29.90%\n",
            "Epoch 258/500, Batch Size: 64, Loss: 0.0244, Train Acc: 60.54%, Val Acc: 31.86%\n",
            "Epoch 259/500, Batch Size: 64, Loss: 0.0253, Train Acc: 60.91%, Val Acc: 31.86%\n",
            "Epoch 260/500, Batch Size: 64, Loss: 0.0234, Train Acc: 61.15%, Val Acc: 28.43%\n",
            "Epoch 261/500, Batch Size: 64, Loss: 0.0237, Train Acc: 61.52%, Val Acc: 27.45%\n",
            "Epoch 262/500, Batch Size: 64, Loss: 0.0230, Train Acc: 60.78%, Val Acc: 28.43%\n",
            "Epoch 263/500, Batch Size: 64, Loss: 0.0245, Train Acc: 59.31%, Val Acc: 31.86%\n",
            "Epoch 264/500, Batch Size: 64, Loss: 0.0232, Train Acc: 61.40%, Val Acc: 29.90%\n",
            "Epoch 265/500, Batch Size: 64, Loss: 0.0234, Train Acc: 61.40%, Val Acc: 31.37%\n",
            "Epoch 266/500, Batch Size: 64, Loss: 0.0216, Train Acc: 62.62%, Val Acc: 26.96%\n",
            "Epoch 267/500, Batch Size: 64, Loss: 0.0246, Train Acc: 58.70%, Val Acc: 32.35%\n",
            "Epoch 268/500, Batch Size: 64, Loss: 0.0242, Train Acc: 59.80%, Val Acc: 33.33%\n",
            "Epoch 269/500, Batch Size: 64, Loss: 0.0239, Train Acc: 60.29%, Val Acc: 33.82%\n",
            "Epoch 270/500, Batch Size: 64, Loss: 0.0240, Train Acc: 61.03%, Val Acc: 29.90%\n",
            "Epoch 271/500, Batch Size: 64, Loss: 0.0249, Train Acc: 58.09%, Val Acc: 32.35%\n",
            "Epoch 272/500, Batch Size: 64, Loss: 0.0244, Train Acc: 58.21%, Val Acc: 36.27%\n",
            "Epoch 273/500, Batch Size: 64, Loss: 0.0247, Train Acc: 57.72%, Val Acc: 28.92%\n",
            "Epoch 274/500, Batch Size: 64, Loss: 0.0254, Train Acc: 56.25%, Val Acc: 31.37%\n",
            "Epoch 275/500, Batch Size: 64, Loss: 0.0239, Train Acc: 60.42%, Val Acc: 30.39%\n",
            "Epoch 276/500, Batch Size: 64, Loss: 0.0242, Train Acc: 60.05%, Val Acc: 32.35%\n",
            "Epoch 277/500, Batch Size: 64, Loss: 0.0234, Train Acc: 60.54%, Val Acc: 33.82%\n",
            "Epoch 278/500, Batch Size: 64, Loss: 0.0242, Train Acc: 60.54%, Val Acc: 31.86%\n",
            "Epoch 279/500, Batch Size: 64, Loss: 0.0239, Train Acc: 60.78%, Val Acc: 27.45%\n",
            "Epoch 280/500, Batch Size: 64, Loss: 0.0240, Train Acc: 63.73%, Val Acc: 31.37%\n",
            "Epoch 281/500, Batch Size: 64, Loss: 0.0237, Train Acc: 60.78%, Val Acc: 30.39%\n",
            "Epoch 282/500, Batch Size: 64, Loss: 0.0235, Train Acc: 61.64%, Val Acc: 25.98%\n",
            "Epoch 283/500, Batch Size: 64, Loss: 0.0259, Train Acc: 59.19%, Val Acc: 29.90%\n",
            "Epoch 284/500, Batch Size: 64, Loss: 0.0250, Train Acc: 57.72%, Val Acc: 24.51%\n",
            "Epoch 285/500, Batch Size: 64, Loss: 0.0242, Train Acc: 60.42%, Val Acc: 29.90%\n",
            "Epoch 286/500, Batch Size: 64, Loss: 0.0254, Train Acc: 59.31%, Val Acc: 31.37%\n",
            "Epoch 287/500, Batch Size: 64, Loss: 0.0242, Train Acc: 60.66%, Val Acc: 29.90%\n",
            "Epoch 288/500, Batch Size: 64, Loss: 0.0246, Train Acc: 58.46%, Val Acc: 31.86%\n",
            "Epoch 289/500, Batch Size: 64, Loss: 0.0237, Train Acc: 61.40%, Val Acc: 33.82%\n",
            "Epoch 290/500, Batch Size: 64, Loss: 0.0245, Train Acc: 58.46%, Val Acc: 29.90%\n",
            "Epoch 291/500, Batch Size: 64, Loss: 0.0254, Train Acc: 58.82%, Val Acc: 32.35%\n",
            "Epoch 292/500, Batch Size: 64, Loss: 0.0240, Train Acc: 59.31%, Val Acc: 28.92%\n",
            "Epoch 293/500, Batch Size: 64, Loss: 0.0235, Train Acc: 63.36%, Val Acc: 27.94%\n",
            "Epoch 294/500, Batch Size: 64, Loss: 0.0252, Train Acc: 58.09%, Val Acc: 32.35%\n",
            "Epoch 295/500, Batch Size: 64, Loss: 0.0241, Train Acc: 59.80%, Val Acc: 26.96%\n",
            "Epoch 296/500, Batch Size: 64, Loss: 0.0249, Train Acc: 61.03%, Val Acc: 26.96%\n",
            "Epoch 297/500, Batch Size: 64, Loss: 0.0238, Train Acc: 60.78%, Val Acc: 23.53%\n",
            "Epoch 298/500, Batch Size: 64, Loss: 0.0244, Train Acc: 58.33%, Val Acc: 32.35%\n",
            "Epoch 299/500, Batch Size: 64, Loss: 0.0242, Train Acc: 59.31%, Val Acc: 28.43%\n",
            "Epoch 300/500, Batch Size: 64, Loss: 0.0251, Train Acc: 58.58%, Val Acc: 32.84%\n",
            "Epoch 301/500, Batch Size: 128, Loss: 0.0134, Train Acc: 59.80%, Val Acc: 31.86%\n",
            "Epoch 302/500, Batch Size: 128, Loss: 0.0131, Train Acc: 59.31%, Val Acc: 28.43%\n",
            "Epoch 303/500, Batch Size: 128, Loss: 0.0122, Train Acc: 63.85%, Val Acc: 26.96%\n",
            "Epoch 304/500, Batch Size: 128, Loss: 0.0135, Train Acc: 57.97%, Val Acc: 29.41%\n",
            "Epoch 305/500, Batch Size: 128, Loss: 0.0133, Train Acc: 60.91%, Val Acc: 29.41%\n",
            "Epoch 306/500, Batch Size: 128, Loss: 0.0130, Train Acc: 58.70%, Val Acc: 31.37%\n",
            "Epoch 307/500, Batch Size: 128, Loss: 0.0128, Train Acc: 59.80%, Val Acc: 27.94%\n",
            "Epoch 308/500, Batch Size: 128, Loss: 0.0131, Train Acc: 60.91%, Val Acc: 30.39%\n",
            "Epoch 309/500, Batch Size: 128, Loss: 0.0127, Train Acc: 63.24%, Val Acc: 29.90%\n",
            "Epoch 310/500, Batch Size: 128, Loss: 0.0123, Train Acc: 59.31%, Val Acc: 31.37%\n",
            "Epoch 311/500, Batch Size: 128, Loss: 0.0132, Train Acc: 59.31%, Val Acc: 30.88%\n",
            "Epoch 312/500, Batch Size: 128, Loss: 0.0135, Train Acc: 59.93%, Val Acc: 31.86%\n",
            "Epoch 313/500, Batch Size: 128, Loss: 0.0121, Train Acc: 61.64%, Val Acc: 27.45%\n",
            "Epoch 314/500, Batch Size: 128, Loss: 0.0112, Train Acc: 62.99%, Val Acc: 31.37%\n",
            "Epoch 315/500, Batch Size: 128, Loss: 0.0131, Train Acc: 58.58%, Val Acc: 32.84%\n",
            "Epoch 316/500, Batch Size: 128, Loss: 0.0133, Train Acc: 58.46%, Val Acc: 32.35%\n",
            "Epoch 317/500, Batch Size: 128, Loss: 0.0128, Train Acc: 60.66%, Val Acc: 29.41%\n",
            "Epoch 318/500, Batch Size: 128, Loss: 0.0123, Train Acc: 61.64%, Val Acc: 29.90%\n",
            "Epoch 319/500, Batch Size: 128, Loss: 0.0126, Train Acc: 60.78%, Val Acc: 30.88%\n",
            "Epoch 320/500, Batch Size: 128, Loss: 0.0123, Train Acc: 61.15%, Val Acc: 32.35%\n",
            "Epoch 321/500, Batch Size: 128, Loss: 0.0123, Train Acc: 63.48%, Val Acc: 31.37%\n",
            "Epoch 322/500, Batch Size: 128, Loss: 0.0124, Train Acc: 61.15%, Val Acc: 33.82%\n",
            "Epoch 323/500, Batch Size: 128, Loss: 0.0133, Train Acc: 61.03%, Val Acc: 30.39%\n",
            "Epoch 324/500, Batch Size: 128, Loss: 0.0122, Train Acc: 62.87%, Val Acc: 22.55%\n",
            "Epoch 325/500, Batch Size: 128, Loss: 0.0132, Train Acc: 60.42%, Val Acc: 31.86%\n",
            "Epoch 326/500, Batch Size: 128, Loss: 0.0133, Train Acc: 60.66%, Val Acc: 29.90%\n",
            "Epoch 327/500, Batch Size: 128, Loss: 0.0119, Train Acc: 62.13%, Val Acc: 34.31%\n",
            "Epoch 328/500, Batch Size: 128, Loss: 0.0126, Train Acc: 60.66%, Val Acc: 35.29%\n",
            "Epoch 329/500, Batch Size: 128, Loss: 0.0132, Train Acc: 58.70%, Val Acc: 30.39%\n",
            "Epoch 330/500, Batch Size: 128, Loss: 0.0124, Train Acc: 62.50%, Val Acc: 33.33%\n",
            "Epoch 331/500, Batch Size: 128, Loss: 0.0127, Train Acc: 61.27%, Val Acc: 30.39%\n",
            "Epoch 332/500, Batch Size: 128, Loss: 0.0124, Train Acc: 60.66%, Val Acc: 30.39%\n",
            "Epoch 333/500, Batch Size: 128, Loss: 0.0126, Train Acc: 60.66%, Val Acc: 26.47%\n",
            "Epoch 334/500, Batch Size: 128, Loss: 0.0124, Train Acc: 62.01%, Val Acc: 29.41%\n",
            "Epoch 335/500, Batch Size: 128, Loss: 0.0140, Train Acc: 58.33%, Val Acc: 32.84%\n",
            "Epoch 336/500, Batch Size: 128, Loss: 0.0125, Train Acc: 62.50%, Val Acc: 31.37%\n",
            "Epoch 337/500, Batch Size: 128, Loss: 0.0129, Train Acc: 61.64%, Val Acc: 29.90%\n",
            "Epoch 338/500, Batch Size: 128, Loss: 0.0134, Train Acc: 61.52%, Val Acc: 28.92%\n",
            "Epoch 339/500, Batch Size: 128, Loss: 0.0132, Train Acc: 59.80%, Val Acc: 31.37%\n",
            "Epoch 340/500, Batch Size: 128, Loss: 0.0125, Train Acc: 62.01%, Val Acc: 30.39%\n",
            "Epoch 341/500, Batch Size: 128, Loss: 0.0122, Train Acc: 62.38%, Val Acc: 28.43%\n",
            "Epoch 342/500, Batch Size: 128, Loss: 0.0134, Train Acc: 59.68%, Val Acc: 35.29%\n",
            "Epoch 343/500, Batch Size: 128, Loss: 0.0125, Train Acc: 62.01%, Val Acc: 29.41%\n",
            "Epoch 344/500, Batch Size: 128, Loss: 0.0121, Train Acc: 63.11%, Val Acc: 31.37%\n",
            "Epoch 345/500, Batch Size: 128, Loss: 0.0125, Train Acc: 60.17%, Val Acc: 37.75%\n",
            "Epoch 346/500, Batch Size: 128, Loss: 0.0126, Train Acc: 60.05%, Val Acc: 32.84%\n",
            "Epoch 347/500, Batch Size: 128, Loss: 0.0126, Train Acc: 60.42%, Val Acc: 28.43%\n",
            "Epoch 348/500, Batch Size: 128, Loss: 0.0124, Train Acc: 61.15%, Val Acc: 29.41%\n",
            "Epoch 349/500, Batch Size: 128, Loss: 0.0132, Train Acc: 61.03%, Val Acc: 32.35%\n",
            "Epoch 350/500, Batch Size: 128, Loss: 0.0132, Train Acc: 60.66%, Val Acc: 28.92%\n",
            "Epoch 351/500, Batch Size: 128, Loss: 0.0129, Train Acc: 60.91%, Val Acc: 27.94%\n",
            "Epoch 352/500, Batch Size: 128, Loss: 0.0131, Train Acc: 57.84%, Val Acc: 31.86%\n",
            "Epoch 353/500, Batch Size: 128, Loss: 0.0125, Train Acc: 59.44%, Val Acc: 30.39%\n",
            "Epoch 354/500, Batch Size: 128, Loss: 0.0125, Train Acc: 58.70%, Val Acc: 26.96%\n",
            "Epoch 355/500, Batch Size: 128, Loss: 0.0130, Train Acc: 60.78%, Val Acc: 27.45%\n",
            "Epoch 356/500, Batch Size: 128, Loss: 0.0134, Train Acc: 58.58%, Val Acc: 27.94%\n",
            "Epoch 357/500, Batch Size: 128, Loss: 0.0131, Train Acc: 60.05%, Val Acc: 30.39%\n",
            "Epoch 358/500, Batch Size: 128, Loss: 0.0130, Train Acc: 60.42%, Val Acc: 29.41%\n",
            "Epoch 359/500, Batch Size: 128, Loss: 0.0121, Train Acc: 62.62%, Val Acc: 33.33%\n",
            "Epoch 360/500, Batch Size: 128, Loss: 0.0126, Train Acc: 62.62%, Val Acc: 30.88%\n",
            "Epoch 361/500, Batch Size: 128, Loss: 0.0135, Train Acc: 60.66%, Val Acc: 30.39%\n",
            "Epoch 362/500, Batch Size: 128, Loss: 0.0129, Train Acc: 60.78%, Val Acc: 28.92%\n",
            "Epoch 363/500, Batch Size: 128, Loss: 0.0126, Train Acc: 61.76%, Val Acc: 29.90%\n",
            "Epoch 364/500, Batch Size: 128, Loss: 0.0129, Train Acc: 61.52%, Val Acc: 27.45%\n",
            "Epoch 365/500, Batch Size: 128, Loss: 0.0128, Train Acc: 59.19%, Val Acc: 28.92%\n",
            "Epoch 366/500, Batch Size: 128, Loss: 0.0132, Train Acc: 60.91%, Val Acc: 33.33%\n",
            "Epoch 367/500, Batch Size: 128, Loss: 0.0127, Train Acc: 61.52%, Val Acc: 29.90%\n",
            "Epoch 368/500, Batch Size: 128, Loss: 0.0126, Train Acc: 61.40%, Val Acc: 32.35%\n",
            "Epoch 369/500, Batch Size: 128, Loss: 0.0125, Train Acc: 60.91%, Val Acc: 32.35%\n",
            "Epoch 370/500, Batch Size: 128, Loss: 0.0123, Train Acc: 63.24%, Val Acc: 29.41%\n",
            "Epoch 371/500, Batch Size: 128, Loss: 0.0131, Train Acc: 60.42%, Val Acc: 28.92%\n",
            "Epoch 372/500, Batch Size: 128, Loss: 0.0120, Train Acc: 62.99%, Val Acc: 31.37%\n",
            "Epoch 373/500, Batch Size: 128, Loss: 0.0138, Train Acc: 58.46%, Val Acc: 27.94%\n",
            "Epoch 374/500, Batch Size: 128, Loss: 0.0131, Train Acc: 58.95%, Val Acc: 33.82%\n",
            "Epoch 375/500, Batch Size: 128, Loss: 0.0130, Train Acc: 60.17%, Val Acc: 25.98%\n",
            "Epoch 376/500, Batch Size: 128, Loss: 0.0129, Train Acc: 59.56%, Val Acc: 28.92%\n",
            "Epoch 377/500, Batch Size: 128, Loss: 0.0122, Train Acc: 62.87%, Val Acc: 36.76%\n",
            "Epoch 378/500, Batch Size: 128, Loss: 0.0128, Train Acc: 61.40%, Val Acc: 29.41%\n",
            "Epoch 379/500, Batch Size: 128, Loss: 0.0133, Train Acc: 62.01%, Val Acc: 31.37%\n",
            "Epoch 380/500, Batch Size: 128, Loss: 0.0122, Train Acc: 62.01%, Val Acc: 28.92%\n",
            "Epoch 381/500, Batch Size: 128, Loss: 0.0127, Train Acc: 59.93%, Val Acc: 29.41%\n",
            "Epoch 382/500, Batch Size: 128, Loss: 0.0129, Train Acc: 61.89%, Val Acc: 33.82%\n",
            "Epoch 383/500, Batch Size: 128, Loss: 0.0131, Train Acc: 59.19%, Val Acc: 29.41%\n",
            "Epoch 384/500, Batch Size: 128, Loss: 0.0128, Train Acc: 60.78%, Val Acc: 33.33%\n",
            "Epoch 385/500, Batch Size: 128, Loss: 0.0130, Train Acc: 57.48%, Val Acc: 28.92%\n",
            "Epoch 386/500, Batch Size: 128, Loss: 0.0132, Train Acc: 59.44%, Val Acc: 34.31%\n",
            "Epoch 387/500, Batch Size: 128, Loss: 0.0120, Train Acc: 62.50%, Val Acc: 26.96%\n",
            "Epoch 388/500, Batch Size: 128, Loss: 0.0132, Train Acc: 59.19%, Val Acc: 27.45%\n",
            "Epoch 389/500, Batch Size: 128, Loss: 0.0127, Train Acc: 60.78%, Val Acc: 31.86%\n",
            "Epoch 390/500, Batch Size: 128, Loss: 0.0129, Train Acc: 59.93%, Val Acc: 32.35%\n",
            "Epoch 391/500, Batch Size: 128, Loss: 0.0124, Train Acc: 60.54%, Val Acc: 28.43%\n",
            "Epoch 392/500, Batch Size: 128, Loss: 0.0132, Train Acc: 59.93%, Val Acc: 30.39%\n",
            "Epoch 393/500, Batch Size: 128, Loss: 0.0130, Train Acc: 59.31%, Val Acc: 29.90%\n",
            "Epoch 394/500, Batch Size: 128, Loss: 0.0121, Train Acc: 61.15%, Val Acc: 31.37%\n",
            "Epoch 395/500, Batch Size: 128, Loss: 0.0128, Train Acc: 60.29%, Val Acc: 30.39%\n",
            "Epoch 396/500, Batch Size: 128, Loss: 0.0139, Train Acc: 58.58%, Val Acc: 28.43%\n",
            "Epoch 397/500, Batch Size: 128, Loss: 0.0122, Train Acc: 63.73%, Val Acc: 33.33%\n",
            "Epoch 398/500, Batch Size: 128, Loss: 0.0125, Train Acc: 60.78%, Val Acc: 30.39%\n",
            "Epoch 399/500, Batch Size: 128, Loss: 0.0124, Train Acc: 61.52%, Val Acc: 32.84%\n",
            "Epoch 400/500, Batch Size: 128, Loss: 0.0118, Train Acc: 64.58%, Val Acc: 28.92%\n",
            "Epoch 401/500, Batch Size: 128, Loss: 0.0127, Train Acc: 60.17%, Val Acc: 37.25%\n",
            "Epoch 402/500, Batch Size: 128, Loss: 0.0126, Train Acc: 59.68%, Val Acc: 28.92%\n",
            "Epoch 403/500, Batch Size: 128, Loss: 0.0135, Train Acc: 58.95%, Val Acc: 33.33%\n",
            "Epoch 404/500, Batch Size: 128, Loss: 0.0126, Train Acc: 59.44%, Val Acc: 29.41%\n",
            "Epoch 405/500, Batch Size: 128, Loss: 0.0124, Train Acc: 60.91%, Val Acc: 33.33%\n",
            "Epoch 406/500, Batch Size: 128, Loss: 0.0121, Train Acc: 63.73%, Val Acc: 30.88%\n",
            "Epoch 407/500, Batch Size: 128, Loss: 0.0130, Train Acc: 60.54%, Val Acc: 31.37%\n",
            "Epoch 408/500, Batch Size: 128, Loss: 0.0121, Train Acc: 62.13%, Val Acc: 29.41%\n",
            "Epoch 409/500, Batch Size: 128, Loss: 0.0131, Train Acc: 58.82%, Val Acc: 30.88%\n",
            "Epoch 410/500, Batch Size: 128, Loss: 0.0125, Train Acc: 61.40%, Val Acc: 26.96%\n",
            "Epoch 411/500, Batch Size: 128, Loss: 0.0125, Train Acc: 59.93%, Val Acc: 30.39%\n",
            "Epoch 412/500, Batch Size: 128, Loss: 0.0129, Train Acc: 60.42%, Val Acc: 31.86%\n",
            "Epoch 413/500, Batch Size: 128, Loss: 0.0129, Train Acc: 61.76%, Val Acc: 29.90%\n",
            "Epoch 414/500, Batch Size: 128, Loss: 0.0130, Train Acc: 61.52%, Val Acc: 31.86%\n",
            "Epoch 415/500, Batch Size: 128, Loss: 0.0129, Train Acc: 61.40%, Val Acc: 26.47%\n",
            "Epoch 416/500, Batch Size: 128, Loss: 0.0123, Train Acc: 61.27%, Val Acc: 29.41%\n",
            "Epoch 417/500, Batch Size: 128, Loss: 0.0131, Train Acc: 59.44%, Val Acc: 31.86%\n",
            "Epoch 418/500, Batch Size: 128, Loss: 0.0132, Train Acc: 59.19%, Val Acc: 35.29%\n",
            "Epoch 419/500, Batch Size: 128, Loss: 0.0133, Train Acc: 59.56%, Val Acc: 29.41%\n",
            "Epoch 420/500, Batch Size: 128, Loss: 0.0125, Train Acc: 62.38%, Val Acc: 33.33%\n",
            "Epoch 421/500, Batch Size: 128, Loss: 0.0125, Train Acc: 63.85%, Val Acc: 33.82%\n",
            "Epoch 422/500, Batch Size: 128, Loss: 0.0121, Train Acc: 62.01%, Val Acc: 29.90%\n",
            "Epoch 423/500, Batch Size: 128, Loss: 0.0131, Train Acc: 56.86%, Val Acc: 32.84%\n",
            "Epoch 424/500, Batch Size: 128, Loss: 0.0130, Train Acc: 59.31%, Val Acc: 35.78%\n",
            "Epoch 425/500, Batch Size: 128, Loss: 0.0129, Train Acc: 62.75%, Val Acc: 34.80%\n",
            "Epoch 426/500, Batch Size: 128, Loss: 0.0124, Train Acc: 60.78%, Val Acc: 31.37%\n",
            "Epoch 427/500, Batch Size: 128, Loss: 0.0127, Train Acc: 61.40%, Val Acc: 34.31%\n",
            "Epoch 428/500, Batch Size: 128, Loss: 0.0128, Train Acc: 61.76%, Val Acc: 30.39%\n",
            "Epoch 429/500, Batch Size: 128, Loss: 0.0120, Train Acc: 61.52%, Val Acc: 30.39%\n",
            "Epoch 430/500, Batch Size: 128, Loss: 0.0131, Train Acc: 59.56%, Val Acc: 33.82%\n",
            "Epoch 431/500, Batch Size: 128, Loss: 0.0128, Train Acc: 62.87%, Val Acc: 27.94%\n",
            "Epoch 432/500, Batch Size: 128, Loss: 0.0131, Train Acc: 60.05%, Val Acc: 35.78%\n",
            "Epoch 433/500, Batch Size: 128, Loss: 0.0127, Train Acc: 61.89%, Val Acc: 29.90%\n",
            "Epoch 434/500, Batch Size: 128, Loss: 0.0128, Train Acc: 62.01%, Val Acc: 31.86%\n",
            "Epoch 435/500, Batch Size: 128, Loss: 0.0130, Train Acc: 58.46%, Val Acc: 30.39%\n",
            "Epoch 436/500, Batch Size: 128, Loss: 0.0125, Train Acc: 58.82%, Val Acc: 29.90%\n",
            "Epoch 437/500, Batch Size: 128, Loss: 0.0123, Train Acc: 62.38%, Val Acc: 27.94%\n",
            "Epoch 438/500, Batch Size: 128, Loss: 0.0132, Train Acc: 60.54%, Val Acc: 26.47%\n",
            "Epoch 439/500, Batch Size: 128, Loss: 0.0130, Train Acc: 59.07%, Val Acc: 34.31%\n",
            "Epoch 440/500, Batch Size: 128, Loss: 0.0128, Train Acc: 60.42%, Val Acc: 30.88%\n",
            "Epoch 441/500, Batch Size: 128, Loss: 0.0130, Train Acc: 60.91%, Val Acc: 29.90%\n",
            "Epoch 442/500, Batch Size: 128, Loss: 0.0125, Train Acc: 63.24%, Val Acc: 32.84%\n",
            "Epoch 443/500, Batch Size: 128, Loss: 0.0124, Train Acc: 63.73%, Val Acc: 29.90%\n",
            "Epoch 444/500, Batch Size: 128, Loss: 0.0127, Train Acc: 62.75%, Val Acc: 23.53%\n",
            "Epoch 445/500, Batch Size: 128, Loss: 0.0125, Train Acc: 61.64%, Val Acc: 30.88%\n",
            "Epoch 446/500, Batch Size: 128, Loss: 0.0121, Train Acc: 64.09%, Val Acc: 28.43%\n",
            "Epoch 447/500, Batch Size: 128, Loss: 0.0121, Train Acc: 61.52%, Val Acc: 25.00%\n",
            "Epoch 448/500, Batch Size: 128, Loss: 0.0124, Train Acc: 61.64%, Val Acc: 27.94%\n",
            "Epoch 449/500, Batch Size: 128, Loss: 0.0127, Train Acc: 59.68%, Val Acc: 27.45%\n",
            "Epoch 450/500, Batch Size: 128, Loss: 0.0126, Train Acc: 60.78%, Val Acc: 25.98%\n",
            "Epoch 451/500, Batch Size: 128, Loss: 0.0127, Train Acc: 61.76%, Val Acc: 31.37%\n",
            "Epoch 452/500, Batch Size: 128, Loss: 0.0127, Train Acc: 60.91%, Val Acc: 37.25%\n",
            "Epoch 453/500, Batch Size: 128, Loss: 0.0122, Train Acc: 63.11%, Val Acc: 36.76%\n",
            "Epoch 454/500, Batch Size: 128, Loss: 0.0125, Train Acc: 61.76%, Val Acc: 35.29%\n",
            "Epoch 455/500, Batch Size: 128, Loss: 0.0127, Train Acc: 60.66%, Val Acc: 31.37%\n",
            "Epoch 456/500, Batch Size: 128, Loss: 0.0132, Train Acc: 59.19%, Val Acc: 27.45%\n",
            "Epoch 457/500, Batch Size: 128, Loss: 0.0129, Train Acc: 59.56%, Val Acc: 29.41%\n",
            "Epoch 458/500, Batch Size: 128, Loss: 0.0129, Train Acc: 61.89%, Val Acc: 33.82%\n",
            "Epoch 459/500, Batch Size: 128, Loss: 0.0122, Train Acc: 63.11%, Val Acc: 27.94%\n",
            "Epoch 460/500, Batch Size: 128, Loss: 0.0122, Train Acc: 62.25%, Val Acc: 32.84%\n",
            "Epoch 461/500, Batch Size: 128, Loss: 0.0137, Train Acc: 60.05%, Val Acc: 32.35%\n",
            "Epoch 462/500, Batch Size: 128, Loss: 0.0135, Train Acc: 58.09%, Val Acc: 25.49%\n",
            "Epoch 463/500, Batch Size: 128, Loss: 0.0122, Train Acc: 61.15%, Val Acc: 25.98%\n",
            "Epoch 464/500, Batch Size: 128, Loss: 0.0134, Train Acc: 58.95%, Val Acc: 32.84%\n",
            "Epoch 465/500, Batch Size: 128, Loss: 0.0126, Train Acc: 62.01%, Val Acc: 27.45%\n",
            "Epoch 466/500, Batch Size: 128, Loss: 0.0124, Train Acc: 61.64%, Val Acc: 31.86%\n",
            "Epoch 467/500, Batch Size: 128, Loss: 0.0130, Train Acc: 59.31%, Val Acc: 30.39%\n",
            "Epoch 468/500, Batch Size: 128, Loss: 0.0129, Train Acc: 57.60%, Val Acc: 28.43%\n",
            "Epoch 469/500, Batch Size: 128, Loss: 0.0128, Train Acc: 62.13%, Val Acc: 31.37%\n",
            "Epoch 470/500, Batch Size: 128, Loss: 0.0127, Train Acc: 58.95%, Val Acc: 34.31%\n",
            "Epoch 471/500, Batch Size: 128, Loss: 0.0131, Train Acc: 59.80%, Val Acc: 27.94%\n",
            "Epoch 472/500, Batch Size: 128, Loss: 0.0126, Train Acc: 62.01%, Val Acc: 26.96%\n",
            "Epoch 473/500, Batch Size: 128, Loss: 0.0129, Train Acc: 61.15%, Val Acc: 33.33%\n",
            "Epoch 474/500, Batch Size: 128, Loss: 0.0128, Train Acc: 60.54%, Val Acc: 32.84%\n",
            "Epoch 475/500, Batch Size: 128, Loss: 0.0127, Train Acc: 61.52%, Val Acc: 32.35%\n",
            "Epoch 476/500, Batch Size: 128, Loss: 0.0128, Train Acc: 61.03%, Val Acc: 28.43%\n",
            "Epoch 477/500, Batch Size: 128, Loss: 0.0128, Train Acc: 61.03%, Val Acc: 31.37%\n",
            "Epoch 478/500, Batch Size: 128, Loss: 0.0118, Train Acc: 64.34%, Val Acc: 24.51%\n",
            "Epoch 479/500, Batch Size: 128, Loss: 0.0125, Train Acc: 61.52%, Val Acc: 33.33%\n",
            "Epoch 480/500, Batch Size: 128, Loss: 0.0124, Train Acc: 63.24%, Val Acc: 27.94%\n",
            "Epoch 481/500, Batch Size: 128, Loss: 0.0128, Train Acc: 59.93%, Val Acc: 33.82%\n",
            "Epoch 482/500, Batch Size: 128, Loss: 0.0126, Train Acc: 63.24%, Val Acc: 29.90%\n",
            "Epoch 483/500, Batch Size: 128, Loss: 0.0129, Train Acc: 61.76%, Val Acc: 28.43%\n",
            "Epoch 484/500, Batch Size: 128, Loss: 0.0123, Train Acc: 62.50%, Val Acc: 32.84%\n",
            "Epoch 485/500, Batch Size: 128, Loss: 0.0122, Train Acc: 63.60%, Val Acc: 29.90%\n",
            "Epoch 486/500, Batch Size: 128, Loss: 0.0127, Train Acc: 60.54%, Val Acc: 30.39%\n",
            "Epoch 487/500, Batch Size: 128, Loss: 0.0126, Train Acc: 60.54%, Val Acc: 31.86%\n",
            "Epoch 488/500, Batch Size: 128, Loss: 0.0138, Train Acc: 59.68%, Val Acc: 31.86%\n",
            "Epoch 489/500, Batch Size: 128, Loss: 0.0124, Train Acc: 61.89%, Val Acc: 32.35%\n",
            "Epoch 490/500, Batch Size: 128, Loss: 0.0129, Train Acc: 60.91%, Val Acc: 29.41%\n",
            "Epoch 491/500, Batch Size: 128, Loss: 0.0130, Train Acc: 59.19%, Val Acc: 29.90%\n",
            "Epoch 492/500, Batch Size: 128, Loss: 0.0133, Train Acc: 61.76%, Val Acc: 28.92%\n",
            "Epoch 493/500, Batch Size: 128, Loss: 0.0135, Train Acc: 59.07%, Val Acc: 31.37%\n",
            "Epoch 494/500, Batch Size: 128, Loss: 0.0124, Train Acc: 62.25%, Val Acc: 35.78%\n",
            "Epoch 495/500, Batch Size: 128, Loss: 0.0128, Train Acc: 60.54%, Val Acc: 30.39%\n",
            "Epoch 496/500, Batch Size: 128, Loss: 0.0114, Train Acc: 65.81%, Val Acc: 31.86%\n",
            "Epoch 497/500, Batch Size: 128, Loss: 0.0126, Train Acc: 61.15%, Val Acc: 34.31%\n",
            "Epoch 498/500, Batch Size: 128, Loss: 0.0137, Train Acc: 58.33%, Val Acc: 25.49%\n",
            "Epoch 499/500, Batch Size: 128, Loss: 0.0128, Train Acc: 60.29%, Val Acc: 28.92%\n",
            "Epoch 500/500, Batch Size: 128, Loss: 0.0136, Train Acc: 59.31%, Val Acc: 35.29%\n",
            "Training complete\n",
            "Test Accuracy: 40.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(192),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(192),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(512 * 12 * 12, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 512 * 12 * 12)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "learning_rate = 0.0005\n",
        "weight_decay = 0.01\n",
        "dropout_rate = 0.5\n",
        "\n",
        "\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "            labels = labels.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "\n",
        "def train_model(model, train_dataset, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=300):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        batch_size = 64 if epoch < 100 else 32 if epoch < 200 else 16\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "        test_accuracy = calculate_accuracy(test_loader, model)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    return model\n",
        "\n",
        "\n",
        "trained_model = train_model(model, train_dataset, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=500)\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsywvcnhPgSJ",
        "outputId": "4c58a394-f1b9-41ac-cd8f-2504e36941f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500, Batch Size: 64, Loss: 0.3478, Train Acc: 1.35%, Val Acc: 3.92%\n",
            "Epoch 2/500, Batch Size: 64, Loss: 0.2110, Train Acc: 2.57%, Val Acc: 1.96%\n",
            "Epoch 3/500, Batch Size: 64, Loss: 0.1541, Train Acc: 3.31%, Val Acc: 1.96%\n",
            "Epoch 4/500, Batch Size: 64, Loss: 0.1222, Train Acc: 4.17%, Val Acc: 2.45%\n",
            "Epoch 5/500, Batch Size: 64, Loss: 0.0952, Train Acc: 4.90%, Val Acc: 4.41%\n",
            "Epoch 6/500, Batch Size: 64, Loss: 0.0878, Train Acc: 6.25%, Val Acc: 3.43%\n",
            "Epoch 7/500, Batch Size: 64, Loss: 0.0841, Train Acc: 5.64%, Val Acc: 7.35%\n",
            "Epoch 8/500, Batch Size: 64, Loss: 0.0765, Train Acc: 9.31%, Val Acc: 6.37%\n",
            "Epoch 9/500, Batch Size: 64, Loss: 0.0809, Train Acc: 6.62%, Val Acc: 4.41%\n",
            "Epoch 10/500, Batch Size: 64, Loss: 0.0767, Train Acc: 8.21%, Val Acc: 7.35%\n",
            "Epoch 11/500, Batch Size: 64, Loss: 0.0800, Train Acc: 7.84%, Val Acc: 3.92%\n",
            "Epoch 12/500, Batch Size: 64, Loss: 0.0742, Train Acc: 9.93%, Val Acc: 5.39%\n",
            "Epoch 13/500, Batch Size: 64, Loss: 0.0705, Train Acc: 11.03%, Val Acc: 6.37%\n",
            "Epoch 14/500, Batch Size: 64, Loss: 0.0670, Train Acc: 12.62%, Val Acc: 5.88%\n",
            "Epoch 15/500, Batch Size: 64, Loss: 0.0677, Train Acc: 13.97%, Val Acc: 11.27%\n",
            "Epoch 16/500, Batch Size: 64, Loss: 0.0676, Train Acc: 13.11%, Val Acc: 6.86%\n",
            "Epoch 17/500, Batch Size: 64, Loss: 0.0700, Train Acc: 12.38%, Val Acc: 9.31%\n",
            "Epoch 18/500, Batch Size: 64, Loss: 0.0685, Train Acc: 12.01%, Val Acc: 8.82%\n",
            "Epoch 19/500, Batch Size: 64, Loss: 0.0663, Train Acc: 14.71%, Val Acc: 10.29%\n",
            "Epoch 20/500, Batch Size: 64, Loss: 0.0637, Train Acc: 14.09%, Val Acc: 7.35%\n",
            "Epoch 21/500, Batch Size: 64, Loss: 0.0595, Train Acc: 19.36%, Val Acc: 12.25%\n",
            "Epoch 22/500, Batch Size: 64, Loss: 0.0541, Train Acc: 20.47%, Val Acc: 13.24%\n",
            "Epoch 23/500, Batch Size: 64, Loss: 0.0517, Train Acc: 21.20%, Val Acc: 8.82%\n",
            "Epoch 24/500, Batch Size: 64, Loss: 0.0511, Train Acc: 22.43%, Val Acc: 11.27%\n",
            "Epoch 25/500, Batch Size: 64, Loss: 0.0475, Train Acc: 26.72%, Val Acc: 15.20%\n",
            "Epoch 26/500, Batch Size: 64, Loss: 0.0495, Train Acc: 24.51%, Val Acc: 17.16%\n",
            "Epoch 27/500, Batch Size: 64, Loss: 0.0476, Train Acc: 26.23%, Val Acc: 16.18%\n",
            "Epoch 28/500, Batch Size: 64, Loss: 0.0479, Train Acc: 26.10%, Val Acc: 16.67%\n",
            "Epoch 29/500, Batch Size: 64, Loss: 0.0482, Train Acc: 25.98%, Val Acc: 14.71%\n",
            "Epoch 30/500, Batch Size: 64, Loss: 0.0479, Train Acc: 27.82%, Val Acc: 13.73%\n",
            "Epoch 31/500, Batch Size: 64, Loss: 0.0470, Train Acc: 28.92%, Val Acc: 14.71%\n",
            "Epoch 32/500, Batch Size: 64, Loss: 0.0461, Train Acc: 27.70%, Val Acc: 14.71%\n",
            "Epoch 33/500, Batch Size: 64, Loss: 0.0499, Train Acc: 24.26%, Val Acc: 13.24%\n",
            "Epoch 34/500, Batch Size: 64, Loss: 0.0454, Train Acc: 29.66%, Val Acc: 13.24%\n",
            "Epoch 35/500, Batch Size: 64, Loss: 0.0463, Train Acc: 26.84%, Val Acc: 17.16%\n",
            "Epoch 36/500, Batch Size: 64, Loss: 0.0461, Train Acc: 26.47%, Val Acc: 19.12%\n",
            "Epoch 37/500, Batch Size: 64, Loss: 0.0459, Train Acc: 29.17%, Val Acc: 14.71%\n",
            "Epoch 38/500, Batch Size: 64, Loss: 0.0438, Train Acc: 30.39%, Val Acc: 19.12%\n",
            "Epoch 39/500, Batch Size: 64, Loss: 0.0434, Train Acc: 32.60%, Val Acc: 12.25%\n",
            "Epoch 40/500, Batch Size: 64, Loss: 0.0460, Train Acc: 29.66%, Val Acc: 15.20%\n",
            "Epoch 41/500, Batch Size: 64, Loss: 0.0427, Train Acc: 31.99%, Val Acc: 16.18%\n",
            "Epoch 42/500, Batch Size: 64, Loss: 0.0395, Train Acc: 35.42%, Val Acc: 17.16%\n",
            "Epoch 43/500, Batch Size: 64, Loss: 0.0380, Train Acc: 37.13%, Val Acc: 19.12%\n",
            "Epoch 44/500, Batch Size: 64, Loss: 0.0383, Train Acc: 38.97%, Val Acc: 19.61%\n",
            "Epoch 45/500, Batch Size: 64, Loss: 0.0389, Train Acc: 38.48%, Val Acc: 21.08%\n",
            "Epoch 46/500, Batch Size: 64, Loss: 0.0370, Train Acc: 41.05%, Val Acc: 19.12%\n",
            "Epoch 47/500, Batch Size: 64, Loss: 0.0384, Train Acc: 40.32%, Val Acc: 16.18%\n",
            "Epoch 48/500, Batch Size: 64, Loss: 0.0376, Train Acc: 39.58%, Val Acc: 19.12%\n",
            "Epoch 49/500, Batch Size: 64, Loss: 0.0363, Train Acc: 42.28%, Val Acc: 21.08%\n",
            "Epoch 50/500, Batch Size: 64, Loss: 0.0364, Train Acc: 41.79%, Val Acc: 21.57%\n",
            "Epoch 51/500, Batch Size: 64, Loss: 0.0349, Train Acc: 41.30%, Val Acc: 18.14%\n",
            "Epoch 52/500, Batch Size: 64, Loss: 0.0374, Train Acc: 41.18%, Val Acc: 19.12%\n",
            "Epoch 53/500, Batch Size: 64, Loss: 0.0356, Train Acc: 43.14%, Val Acc: 23.04%\n",
            "Epoch 54/500, Batch Size: 64, Loss: 0.0362, Train Acc: 41.30%, Val Acc: 16.18%\n",
            "Epoch 55/500, Batch Size: 64, Loss: 0.0349, Train Acc: 41.79%, Val Acc: 17.16%\n",
            "Epoch 56/500, Batch Size: 64, Loss: 0.0375, Train Acc: 40.93%, Val Acc: 22.06%\n",
            "Epoch 57/500, Batch Size: 64, Loss: 0.0357, Train Acc: 42.77%, Val Acc: 25.00%\n",
            "Epoch 58/500, Batch Size: 64, Loss: 0.0345, Train Acc: 44.00%, Val Acc: 19.61%\n",
            "Epoch 59/500, Batch Size: 64, Loss: 0.0344, Train Acc: 43.63%, Val Acc: 20.59%\n",
            "Epoch 60/500, Batch Size: 64, Loss: 0.0353, Train Acc: 44.12%, Val Acc: 19.12%\n",
            "Epoch 61/500, Batch Size: 64, Loss: 0.0334, Train Acc: 45.83%, Val Acc: 22.55%\n",
            "Epoch 62/500, Batch Size: 64, Loss: 0.0317, Train Acc: 47.92%, Val Acc: 24.02%\n",
            "Epoch 63/500, Batch Size: 64, Loss: 0.0333, Train Acc: 46.08%, Val Acc: 22.06%\n",
            "Epoch 64/500, Batch Size: 64, Loss: 0.0316, Train Acc: 46.94%, Val Acc: 23.04%\n",
            "Epoch 65/500, Batch Size: 64, Loss: 0.0313, Train Acc: 48.65%, Val Acc: 27.45%\n",
            "Epoch 66/500, Batch Size: 64, Loss: 0.0318, Train Acc: 47.43%, Val Acc: 20.10%\n",
            "Epoch 67/500, Batch Size: 64, Loss: 0.0307, Train Acc: 48.53%, Val Acc: 20.10%\n",
            "Epoch 68/500, Batch Size: 64, Loss: 0.0317, Train Acc: 49.26%, Val Acc: 25.98%\n",
            "Epoch 69/500, Batch Size: 64, Loss: 0.0330, Train Acc: 46.08%, Val Acc: 20.10%\n",
            "Epoch 70/500, Batch Size: 64, Loss: 0.0305, Train Acc: 47.92%, Val Acc: 20.10%\n",
            "Epoch 71/500, Batch Size: 64, Loss: 0.0293, Train Acc: 50.12%, Val Acc: 24.51%\n",
            "Epoch 72/500, Batch Size: 64, Loss: 0.0301, Train Acc: 50.25%, Val Acc: 30.88%\n",
            "Epoch 73/500, Batch Size: 64, Loss: 0.0299, Train Acc: 50.37%, Val Acc: 23.04%\n",
            "Epoch 74/500, Batch Size: 64, Loss: 0.0292, Train Acc: 50.49%, Val Acc: 17.16%\n",
            "Epoch 75/500, Batch Size: 64, Loss: 0.0296, Train Acc: 48.65%, Val Acc: 21.57%\n",
            "Epoch 76/500, Batch Size: 64, Loss: 0.0297, Train Acc: 49.26%, Val Acc: 25.00%\n",
            "Epoch 77/500, Batch Size: 64, Loss: 0.0297, Train Acc: 50.37%, Val Acc: 23.04%\n",
            "Epoch 78/500, Batch Size: 64, Loss: 0.0336, Train Acc: 46.08%, Val Acc: 21.08%\n",
            "Epoch 79/500, Batch Size: 64, Loss: 0.0290, Train Acc: 53.55%, Val Acc: 22.55%\n",
            "Epoch 80/500, Batch Size: 64, Loss: 0.0293, Train Acc: 51.23%, Val Acc: 17.65%\n",
            "Epoch 81/500, Batch Size: 64, Loss: 0.0298, Train Acc: 51.47%, Val Acc: 24.51%\n",
            "Epoch 82/500, Batch Size: 64, Loss: 0.0289, Train Acc: 53.43%, Val Acc: 25.00%\n",
            "Epoch 83/500, Batch Size: 64, Loss: 0.0286, Train Acc: 50.74%, Val Acc: 23.53%\n",
            "Epoch 84/500, Batch Size: 64, Loss: 0.0291, Train Acc: 52.70%, Val Acc: 24.51%\n",
            "Epoch 85/500, Batch Size: 64, Loss: 0.0288, Train Acc: 54.17%, Val Acc: 22.55%\n",
            "Epoch 86/500, Batch Size: 64, Loss: 0.0275, Train Acc: 54.53%, Val Acc: 25.00%\n",
            "Epoch 87/500, Batch Size: 64, Loss: 0.0274, Train Acc: 52.33%, Val Acc: 24.51%\n",
            "Epoch 88/500, Batch Size: 64, Loss: 0.0268, Train Acc: 55.39%, Val Acc: 25.98%\n",
            "Epoch 89/500, Batch Size: 64, Loss: 0.0264, Train Acc: 55.64%, Val Acc: 22.06%\n",
            "Epoch 90/500, Batch Size: 64, Loss: 0.0269, Train Acc: 56.25%, Val Acc: 22.06%\n",
            "Epoch 91/500, Batch Size: 64, Loss: 0.0281, Train Acc: 54.53%, Val Acc: 26.47%\n",
            "Epoch 92/500, Batch Size: 64, Loss: 0.0276, Train Acc: 55.15%, Val Acc: 25.00%\n",
            "Epoch 93/500, Batch Size: 64, Loss: 0.0269, Train Acc: 54.66%, Val Acc: 21.57%\n",
            "Epoch 94/500, Batch Size: 64, Loss: 0.0260, Train Acc: 56.13%, Val Acc: 25.49%\n",
            "Epoch 95/500, Batch Size: 64, Loss: 0.0276, Train Acc: 54.29%, Val Acc: 22.06%\n",
            "Epoch 96/500, Batch Size: 64, Loss: 0.0271, Train Acc: 55.51%, Val Acc: 23.53%\n",
            "Epoch 97/500, Batch Size: 64, Loss: 0.0273, Train Acc: 54.04%, Val Acc: 24.02%\n",
            "Epoch 98/500, Batch Size: 64, Loss: 0.0262, Train Acc: 56.50%, Val Acc: 31.86%\n",
            "Epoch 99/500, Batch Size: 64, Loss: 0.0256, Train Acc: 56.86%, Val Acc: 26.47%\n",
            "Epoch 100/500, Batch Size: 64, Loss: 0.0264, Train Acc: 58.82%, Val Acc: 23.04%\n",
            "Epoch 101/500, Batch Size: 32, Loss: 0.0597, Train Acc: 53.19%, Val Acc: 24.02%\n",
            "Epoch 102/500, Batch Size: 32, Loss: 0.0552, Train Acc: 52.33%, Val Acc: 28.92%\n",
            "Epoch 103/500, Batch Size: 32, Loss: 0.0562, Train Acc: 52.82%, Val Acc: 22.55%\n",
            "Epoch 104/500, Batch Size: 32, Loss: 0.0576, Train Acc: 55.02%, Val Acc: 25.00%\n",
            "Epoch 105/500, Batch Size: 32, Loss: 0.0542, Train Acc: 54.53%, Val Acc: 24.02%\n",
            "Epoch 106/500, Batch Size: 32, Loss: 0.0555, Train Acc: 53.80%, Val Acc: 21.57%\n",
            "Epoch 107/500, Batch Size: 32, Loss: 0.0546, Train Acc: 55.15%, Val Acc: 25.49%\n",
            "Epoch 108/500, Batch Size: 32, Loss: 0.0514, Train Acc: 55.76%, Val Acc: 22.06%\n",
            "Epoch 109/500, Batch Size: 32, Loss: 0.0550, Train Acc: 54.66%, Val Acc: 20.10%\n",
            "Epoch 110/500, Batch Size: 32, Loss: 0.0539, Train Acc: 55.27%, Val Acc: 30.39%\n",
            "Epoch 111/500, Batch Size: 32, Loss: 0.0530, Train Acc: 53.68%, Val Acc: 25.98%\n",
            "Epoch 112/500, Batch Size: 32, Loss: 0.0531, Train Acc: 55.15%, Val Acc: 27.94%\n",
            "Epoch 113/500, Batch Size: 32, Loss: 0.0557, Train Acc: 54.17%, Val Acc: 24.51%\n",
            "Epoch 114/500, Batch Size: 32, Loss: 0.0539, Train Acc: 55.15%, Val Acc: 26.47%\n",
            "Epoch 115/500, Batch Size: 32, Loss: 0.0548, Train Acc: 51.72%, Val Acc: 22.55%\n",
            "Epoch 116/500, Batch Size: 32, Loss: 0.0524, Train Acc: 55.64%, Val Acc: 26.96%\n",
            "Epoch 117/500, Batch Size: 32, Loss: 0.0525, Train Acc: 56.50%, Val Acc: 21.57%\n",
            "Epoch 118/500, Batch Size: 32, Loss: 0.0584, Train Acc: 52.45%, Val Acc: 23.53%\n",
            "Epoch 119/500, Batch Size: 32, Loss: 0.0509, Train Acc: 56.50%, Val Acc: 26.96%\n",
            "Epoch 120/500, Batch Size: 32, Loss: 0.0545, Train Acc: 53.43%, Val Acc: 25.98%\n",
            "Epoch 121/500, Batch Size: 32, Loss: 0.0511, Train Acc: 57.48%, Val Acc: 23.53%\n",
            "Epoch 122/500, Batch Size: 32, Loss: 0.0512, Train Acc: 57.48%, Val Acc: 22.55%\n",
            "Epoch 123/500, Batch Size: 32, Loss: 0.0511, Train Acc: 58.21%, Val Acc: 25.98%\n",
            "Epoch 124/500, Batch Size: 32, Loss: 0.0540, Train Acc: 54.78%, Val Acc: 25.49%\n",
            "Epoch 125/500, Batch Size: 32, Loss: 0.0551, Train Acc: 57.35%, Val Acc: 23.04%\n",
            "Epoch 126/500, Batch Size: 32, Loss: 0.0512, Train Acc: 58.95%, Val Acc: 25.00%\n",
            "Epoch 127/500, Batch Size: 32, Loss: 0.0522, Train Acc: 56.25%, Val Acc: 25.00%\n",
            "Epoch 128/500, Batch Size: 32, Loss: 0.0534, Train Acc: 57.84%, Val Acc: 25.98%\n",
            "Epoch 129/500, Batch Size: 32, Loss: 0.0514, Train Acc: 57.48%, Val Acc: 21.57%\n",
            "Epoch 130/500, Batch Size: 32, Loss: 0.0520, Train Acc: 58.21%, Val Acc: 22.06%\n",
            "Epoch 131/500, Batch Size: 32, Loss: 0.0517, Train Acc: 57.35%, Val Acc: 24.51%\n",
            "Epoch 132/500, Batch Size: 32, Loss: 0.0521, Train Acc: 55.15%, Val Acc: 29.90%\n",
            "Epoch 133/500, Batch Size: 32, Loss: 0.0508, Train Acc: 55.76%, Val Acc: 26.47%\n",
            "Epoch 134/500, Batch Size: 32, Loss: 0.0507, Train Acc: 59.68%, Val Acc: 29.41%\n",
            "Epoch 135/500, Batch Size: 32, Loss: 0.0523, Train Acc: 57.60%, Val Acc: 25.98%\n",
            "Epoch 136/500, Batch Size: 32, Loss: 0.0520, Train Acc: 56.50%, Val Acc: 23.53%\n",
            "Epoch 137/500, Batch Size: 32, Loss: 0.0507, Train Acc: 57.97%, Val Acc: 22.55%\n",
            "Epoch 138/500, Batch Size: 32, Loss: 0.0516, Train Acc: 57.35%, Val Acc: 26.47%\n",
            "Epoch 139/500, Batch Size: 32, Loss: 0.0510, Train Acc: 59.07%, Val Acc: 26.96%\n",
            "Epoch 140/500, Batch Size: 32, Loss: 0.0515, Train Acc: 59.07%, Val Acc: 25.00%\n",
            "Epoch 141/500, Batch Size: 32, Loss: 0.0489, Train Acc: 60.17%, Val Acc: 25.98%\n",
            "Epoch 142/500, Batch Size: 32, Loss: 0.0486, Train Acc: 59.68%, Val Acc: 26.96%\n",
            "Epoch 143/500, Batch Size: 32, Loss: 0.0469, Train Acc: 60.54%, Val Acc: 28.43%\n",
            "Epoch 144/500, Batch Size: 32, Loss: 0.0514, Train Acc: 57.60%, Val Acc: 27.94%\n",
            "Epoch 145/500, Batch Size: 32, Loss: 0.0491, Train Acc: 59.44%, Val Acc: 28.43%\n",
            "Epoch 146/500, Batch Size: 32, Loss: 0.0510, Train Acc: 58.70%, Val Acc: 25.00%\n",
            "Epoch 147/500, Batch Size: 32, Loss: 0.0509, Train Acc: 56.74%, Val Acc: 28.43%\n",
            "Epoch 148/500, Batch Size: 32, Loss: 0.0461, Train Acc: 60.05%, Val Acc: 25.49%\n",
            "Epoch 149/500, Batch Size: 32, Loss: 0.0489, Train Acc: 58.82%, Val Acc: 22.55%\n",
            "Epoch 150/500, Batch Size: 32, Loss: 0.0477, Train Acc: 59.56%, Val Acc: 25.49%\n",
            "Epoch 151/500, Batch Size: 32, Loss: 0.0479, Train Acc: 58.58%, Val Acc: 25.00%\n",
            "Epoch 152/500, Batch Size: 32, Loss: 0.0498, Train Acc: 57.60%, Val Acc: 23.04%\n",
            "Epoch 153/500, Batch Size: 32, Loss: 0.0548, Train Acc: 55.02%, Val Acc: 27.45%\n",
            "Epoch 154/500, Batch Size: 32, Loss: 0.0511, Train Acc: 57.72%, Val Acc: 21.08%\n",
            "Epoch 155/500, Batch Size: 32, Loss: 0.0491, Train Acc: 58.46%, Val Acc: 28.43%\n",
            "Epoch 156/500, Batch Size: 32, Loss: 0.0483, Train Acc: 58.95%, Val Acc: 25.49%\n",
            "Epoch 157/500, Batch Size: 32, Loss: 0.0484, Train Acc: 58.46%, Val Acc: 27.45%\n",
            "Epoch 158/500, Batch Size: 32, Loss: 0.0479, Train Acc: 60.42%, Val Acc: 25.98%\n",
            "Epoch 159/500, Batch Size: 32, Loss: 0.0509, Train Acc: 60.05%, Val Acc: 21.57%\n",
            "Epoch 160/500, Batch Size: 32, Loss: 0.0507, Train Acc: 58.58%, Val Acc: 18.63%\n",
            "Epoch 161/500, Batch Size: 32, Loss: 0.0523, Train Acc: 56.86%, Val Acc: 31.37%\n",
            "Epoch 162/500, Batch Size: 32, Loss: 0.0504, Train Acc: 59.68%, Val Acc: 26.47%\n",
            "Epoch 163/500, Batch Size: 32, Loss: 0.0478, Train Acc: 60.17%, Val Acc: 21.57%\n",
            "Epoch 164/500, Batch Size: 32, Loss: 0.0475, Train Acc: 61.52%, Val Acc: 29.90%\n",
            "Epoch 165/500, Batch Size: 32, Loss: 0.0503, Train Acc: 58.46%, Val Acc: 27.45%\n",
            "Epoch 166/500, Batch Size: 32, Loss: 0.0518, Train Acc: 59.68%, Val Acc: 29.41%\n",
            "Epoch 167/500, Batch Size: 32, Loss: 0.0520, Train Acc: 55.76%, Val Acc: 24.51%\n",
            "Epoch 168/500, Batch Size: 32, Loss: 0.0499, Train Acc: 58.09%, Val Acc: 25.00%\n",
            "Epoch 169/500, Batch Size: 32, Loss: 0.0498, Train Acc: 58.70%, Val Acc: 25.49%\n",
            "Epoch 170/500, Batch Size: 32, Loss: 0.0495, Train Acc: 61.15%, Val Acc: 24.51%\n",
            "Epoch 171/500, Batch Size: 32, Loss: 0.0467, Train Acc: 60.54%, Val Acc: 23.04%\n",
            "Epoch 172/500, Batch Size: 32, Loss: 0.0507, Train Acc: 57.84%, Val Acc: 26.96%\n",
            "Epoch 173/500, Batch Size: 32, Loss: 0.0500, Train Acc: 58.70%, Val Acc: 29.90%\n",
            "Epoch 174/500, Batch Size: 32, Loss: 0.0529, Train Acc: 56.37%, Val Acc: 29.41%\n",
            "Epoch 175/500, Batch Size: 32, Loss: 0.0493, Train Acc: 59.56%, Val Acc: 26.96%\n",
            "Epoch 176/500, Batch Size: 32, Loss: 0.0467, Train Acc: 62.62%, Val Acc: 27.45%\n",
            "Epoch 177/500, Batch Size: 32, Loss: 0.0500, Train Acc: 58.58%, Val Acc: 27.45%\n",
            "Epoch 178/500, Batch Size: 32, Loss: 0.0473, Train Acc: 61.52%, Val Acc: 25.98%\n",
            "Epoch 179/500, Batch Size: 32, Loss: 0.0491, Train Acc: 60.29%, Val Acc: 27.45%\n",
            "Epoch 180/500, Batch Size: 32, Loss: 0.0499, Train Acc: 59.56%, Val Acc: 25.98%\n",
            "Epoch 181/500, Batch Size: 32, Loss: 0.0460, Train Acc: 64.34%, Val Acc: 28.92%\n",
            "Epoch 182/500, Batch Size: 32, Loss: 0.0474, Train Acc: 61.64%, Val Acc: 30.39%\n",
            "Epoch 183/500, Batch Size: 32, Loss: 0.0475, Train Acc: 60.17%, Val Acc: 25.98%\n",
            "Epoch 184/500, Batch Size: 32, Loss: 0.0501, Train Acc: 58.70%, Val Acc: 32.35%\n",
            "Epoch 185/500, Batch Size: 32, Loss: 0.0498, Train Acc: 59.44%, Val Acc: 25.49%\n",
            "Epoch 186/500, Batch Size: 32, Loss: 0.0480, Train Acc: 60.91%, Val Acc: 29.90%\n",
            "Epoch 187/500, Batch Size: 32, Loss: 0.0460, Train Acc: 61.76%, Val Acc: 30.39%\n",
            "Epoch 188/500, Batch Size: 32, Loss: 0.0440, Train Acc: 62.62%, Val Acc: 25.98%\n",
            "Epoch 189/500, Batch Size: 32, Loss: 0.0454, Train Acc: 60.78%, Val Acc: 25.98%\n",
            "Epoch 190/500, Batch Size: 32, Loss: 0.0475, Train Acc: 59.68%, Val Acc: 26.47%\n",
            "Epoch 191/500, Batch Size: 32, Loss: 0.0486, Train Acc: 59.68%, Val Acc: 26.96%\n",
            "Epoch 192/500, Batch Size: 32, Loss: 0.0454, Train Acc: 62.75%, Val Acc: 23.53%\n",
            "Epoch 193/500, Batch Size: 32, Loss: 0.0526, Train Acc: 58.46%, Val Acc: 25.98%\n",
            "Epoch 194/500, Batch Size: 32, Loss: 0.0469, Train Acc: 62.62%, Val Acc: 29.41%\n",
            "Epoch 195/500, Batch Size: 32, Loss: 0.0465, Train Acc: 59.80%, Val Acc: 21.57%\n",
            "Epoch 196/500, Batch Size: 32, Loss: 0.0461, Train Acc: 61.03%, Val Acc: 27.45%\n",
            "Epoch 197/500, Batch Size: 32, Loss: 0.0484, Train Acc: 60.05%, Val Acc: 27.94%\n",
            "Epoch 198/500, Batch Size: 32, Loss: 0.0453, Train Acc: 63.36%, Val Acc: 27.45%\n",
            "Epoch 199/500, Batch Size: 32, Loss: 0.0477, Train Acc: 61.64%, Val Acc: 28.92%\n",
            "Epoch 200/500, Batch Size: 32, Loss: 0.0483, Train Acc: 59.44%, Val Acc: 23.04%\n",
            "Epoch 201/500, Batch Size: 16, Loss: 0.1022, Train Acc: 58.21%, Val Acc: 25.98%\n",
            "Epoch 202/500, Batch Size: 16, Loss: 0.1067, Train Acc: 56.86%, Val Acc: 25.00%\n",
            "Epoch 203/500, Batch Size: 16, Loss: 0.1093, Train Acc: 54.41%, Val Acc: 27.94%\n",
            "Epoch 204/500, Batch Size: 16, Loss: 0.1024, Train Acc: 55.02%, Val Acc: 24.51%\n",
            "Epoch 205/500, Batch Size: 16, Loss: 0.1010, Train Acc: 58.82%, Val Acc: 26.96%\n",
            "Epoch 206/500, Batch Size: 16, Loss: 0.1085, Train Acc: 57.35%, Val Acc: 25.49%\n",
            "Epoch 207/500, Batch Size: 16, Loss: 0.0999, Train Acc: 57.35%, Val Acc: 26.96%\n",
            "Epoch 208/500, Batch Size: 16, Loss: 0.1020, Train Acc: 56.50%, Val Acc: 27.45%\n",
            "Epoch 209/500, Batch Size: 16, Loss: 0.1016, Train Acc: 60.42%, Val Acc: 23.53%\n",
            "Epoch 210/500, Batch Size: 16, Loss: 0.1030, Train Acc: 57.97%, Val Acc: 19.61%\n",
            "Epoch 211/500, Batch Size: 16, Loss: 0.1062, Train Acc: 55.27%, Val Acc: 23.53%\n",
            "Epoch 212/500, Batch Size: 16, Loss: 0.1016, Train Acc: 55.51%, Val Acc: 29.41%\n",
            "Epoch 213/500, Batch Size: 16, Loss: 0.1049, Train Acc: 56.50%, Val Acc: 26.47%\n",
            "Epoch 214/500, Batch Size: 16, Loss: 0.1009, Train Acc: 57.23%, Val Acc: 27.94%\n",
            "Epoch 215/500, Batch Size: 16, Loss: 0.0983, Train Acc: 57.35%, Val Acc: 26.47%\n",
            "Epoch 216/500, Batch Size: 16, Loss: 0.1089, Train Acc: 55.15%, Val Acc: 26.47%\n",
            "Epoch 217/500, Batch Size: 16, Loss: 0.1008, Train Acc: 58.09%, Val Acc: 31.37%\n",
            "Epoch 218/500, Batch Size: 16, Loss: 0.1033, Train Acc: 56.62%, Val Acc: 28.92%\n",
            "Epoch 219/500, Batch Size: 16, Loss: 0.1089, Train Acc: 54.78%, Val Acc: 20.59%\n",
            "Epoch 220/500, Batch Size: 16, Loss: 0.0968, Train Acc: 58.21%, Val Acc: 25.00%\n",
            "Epoch 221/500, Batch Size: 16, Loss: 0.1065, Train Acc: 54.66%, Val Acc: 25.00%\n",
            "Epoch 222/500, Batch Size: 16, Loss: 0.1108, Train Acc: 55.02%, Val Acc: 26.47%\n",
            "Epoch 223/500, Batch Size: 16, Loss: 0.0980, Train Acc: 58.09%, Val Acc: 28.43%\n",
            "Epoch 224/500, Batch Size: 16, Loss: 0.1083, Train Acc: 55.51%, Val Acc: 22.55%\n",
            "Epoch 225/500, Batch Size: 16, Loss: 0.1037, Train Acc: 56.50%, Val Acc: 25.00%\n",
            "Epoch 226/500, Batch Size: 16, Loss: 0.1023, Train Acc: 56.62%, Val Acc: 26.47%\n",
            "Epoch 227/500, Batch Size: 16, Loss: 0.0940, Train Acc: 61.03%, Val Acc: 28.43%\n",
            "Epoch 228/500, Batch Size: 16, Loss: 0.1062, Train Acc: 55.51%, Val Acc: 28.43%\n",
            "Epoch 229/500, Batch Size: 16, Loss: 0.1013, Train Acc: 55.76%, Val Acc: 27.94%\n",
            "Epoch 230/500, Batch Size: 16, Loss: 0.1040, Train Acc: 56.99%, Val Acc: 25.98%\n",
            "Epoch 231/500, Batch Size: 16, Loss: 0.1026, Train Acc: 58.46%, Val Acc: 25.00%\n",
            "Epoch 232/500, Batch Size: 16, Loss: 0.1045, Train Acc: 56.74%, Val Acc: 23.04%\n",
            "Epoch 233/500, Batch Size: 16, Loss: 0.1028, Train Acc: 56.37%, Val Acc: 21.57%\n",
            "Epoch 234/500, Batch Size: 16, Loss: 0.0971, Train Acc: 57.72%, Val Acc: 25.00%\n",
            "Epoch 235/500, Batch Size: 16, Loss: 0.1030, Train Acc: 57.97%, Val Acc: 25.49%\n",
            "Epoch 236/500, Batch Size: 16, Loss: 0.0980, Train Acc: 57.60%, Val Acc: 27.45%\n",
            "Epoch 237/500, Batch Size: 16, Loss: 0.0974, Train Acc: 58.33%, Val Acc: 21.57%\n",
            "Epoch 238/500, Batch Size: 16, Loss: 0.1072, Train Acc: 55.27%, Val Acc: 24.51%\n",
            "Epoch 239/500, Batch Size: 16, Loss: 0.0990, Train Acc: 58.09%, Val Acc: 28.43%\n",
            "Epoch 240/500, Batch Size: 16, Loss: 0.0988, Train Acc: 57.60%, Val Acc: 29.90%\n",
            "Epoch 241/500, Batch Size: 16, Loss: 0.1037, Train Acc: 55.76%, Val Acc: 27.45%\n",
            "Epoch 242/500, Batch Size: 16, Loss: 0.1122, Train Acc: 52.45%, Val Acc: 21.08%\n",
            "Epoch 243/500, Batch Size: 16, Loss: 0.0999, Train Acc: 60.17%, Val Acc: 26.47%\n",
            "Epoch 244/500, Batch Size: 16, Loss: 0.1006, Train Acc: 57.72%, Val Acc: 27.94%\n",
            "Epoch 245/500, Batch Size: 16, Loss: 0.0969, Train Acc: 59.56%, Val Acc: 26.47%\n",
            "Epoch 246/500, Batch Size: 16, Loss: 0.1040, Train Acc: 59.31%, Val Acc: 24.51%\n",
            "Epoch 247/500, Batch Size: 16, Loss: 0.1097, Train Acc: 54.41%, Val Acc: 29.90%\n",
            "Epoch 248/500, Batch Size: 16, Loss: 0.1085, Train Acc: 56.00%, Val Acc: 25.00%\n",
            "Epoch 249/500, Batch Size: 16, Loss: 0.1005, Train Acc: 56.74%, Val Acc: 29.41%\n",
            "Epoch 250/500, Batch Size: 16, Loss: 0.1012, Train Acc: 57.97%, Val Acc: 28.43%\n",
            "Epoch 251/500, Batch Size: 16, Loss: 0.1005, Train Acc: 57.35%, Val Acc: 27.45%\n",
            "Epoch 252/500, Batch Size: 16, Loss: 0.1020, Train Acc: 57.35%, Val Acc: 25.49%\n",
            "Epoch 253/500, Batch Size: 16, Loss: 0.0944, Train Acc: 59.31%, Val Acc: 27.45%\n",
            "Epoch 254/500, Batch Size: 16, Loss: 0.0941, Train Acc: 60.29%, Val Acc: 27.94%\n",
            "Epoch 255/500, Batch Size: 16, Loss: 0.1021, Train Acc: 57.35%, Val Acc: 25.49%\n",
            "Epoch 256/500, Batch Size: 16, Loss: 0.1118, Train Acc: 54.78%, Val Acc: 25.98%\n",
            "Epoch 257/500, Batch Size: 16, Loss: 0.1005, Train Acc: 59.31%, Val Acc: 20.59%\n",
            "Epoch 258/500, Batch Size: 16, Loss: 0.0947, Train Acc: 59.56%, Val Acc: 24.51%\n",
            "Epoch 259/500, Batch Size: 16, Loss: 0.0996, Train Acc: 57.97%, Val Acc: 29.90%\n",
            "Epoch 260/500, Batch Size: 16, Loss: 0.0981, Train Acc: 58.46%, Val Acc: 25.98%\n",
            "Epoch 261/500, Batch Size: 16, Loss: 0.1013, Train Acc: 57.84%, Val Acc: 25.49%\n",
            "Epoch 262/500, Batch Size: 16, Loss: 0.0998, Train Acc: 59.44%, Val Acc: 24.51%\n",
            "Epoch 263/500, Batch Size: 16, Loss: 0.1012, Train Acc: 57.97%, Val Acc: 27.45%\n",
            "Epoch 264/500, Batch Size: 16, Loss: 0.1005, Train Acc: 57.84%, Val Acc: 28.43%\n",
            "Epoch 265/500, Batch Size: 16, Loss: 0.1063, Train Acc: 56.99%, Val Acc: 31.37%\n",
            "Epoch 266/500, Batch Size: 16, Loss: 0.1029, Train Acc: 58.33%, Val Acc: 25.00%\n",
            "Epoch 267/500, Batch Size: 16, Loss: 0.1112, Train Acc: 53.80%, Val Acc: 25.98%\n",
            "Epoch 268/500, Batch Size: 16, Loss: 0.1044, Train Acc: 57.48%, Val Acc: 26.96%\n",
            "Epoch 269/500, Batch Size: 16, Loss: 0.1124, Train Acc: 51.35%, Val Acc: 30.88%\n",
            "Epoch 270/500, Batch Size: 16, Loss: 0.1010, Train Acc: 58.21%, Val Acc: 23.53%\n",
            "Epoch 271/500, Batch Size: 16, Loss: 0.1067, Train Acc: 55.15%, Val Acc: 29.90%\n",
            "Epoch 272/500, Batch Size: 16, Loss: 0.1021, Train Acc: 56.86%, Val Acc: 26.96%\n",
            "Epoch 273/500, Batch Size: 16, Loss: 0.1016, Train Acc: 59.07%, Val Acc: 26.47%\n",
            "Epoch 274/500, Batch Size: 16, Loss: 0.0999, Train Acc: 58.33%, Val Acc: 28.43%\n",
            "Epoch 275/500, Batch Size: 16, Loss: 0.1015, Train Acc: 56.62%, Val Acc: 30.39%\n",
            "Epoch 276/500, Batch Size: 16, Loss: 0.0971, Train Acc: 58.46%, Val Acc: 23.04%\n",
            "Epoch 277/500, Batch Size: 16, Loss: 0.1045, Train Acc: 56.00%, Val Acc: 24.02%\n",
            "Epoch 278/500, Batch Size: 16, Loss: 0.0950, Train Acc: 59.31%, Val Acc: 27.45%\n",
            "Epoch 279/500, Batch Size: 16, Loss: 0.0976, Train Acc: 59.93%, Val Acc: 26.96%\n",
            "Epoch 280/500, Batch Size: 16, Loss: 0.1023, Train Acc: 56.50%, Val Acc: 26.47%\n",
            "Epoch 281/500, Batch Size: 16, Loss: 0.1072, Train Acc: 58.58%, Val Acc: 27.94%\n",
            "Epoch 282/500, Batch Size: 16, Loss: 0.1058, Train Acc: 54.41%, Val Acc: 28.43%\n",
            "Epoch 283/500, Batch Size: 16, Loss: 0.0980, Train Acc: 59.19%, Val Acc: 29.90%\n",
            "Epoch 284/500, Batch Size: 16, Loss: 0.1136, Train Acc: 53.80%, Val Acc: 28.92%\n",
            "Epoch 285/500, Batch Size: 16, Loss: 0.1053, Train Acc: 55.76%, Val Acc: 27.45%\n",
            "Epoch 286/500, Batch Size: 16, Loss: 0.1022, Train Acc: 58.21%, Val Acc: 28.43%\n",
            "Epoch 287/500, Batch Size: 16, Loss: 0.0961, Train Acc: 58.95%, Val Acc: 24.51%\n",
            "Epoch 288/500, Batch Size: 16, Loss: 0.1044, Train Acc: 55.76%, Val Acc: 26.47%\n",
            "Epoch 289/500, Batch Size: 16, Loss: 0.0915, Train Acc: 59.80%, Val Acc: 29.41%\n",
            "Epoch 290/500, Batch Size: 16, Loss: 0.1017, Train Acc: 57.11%, Val Acc: 26.96%\n",
            "Epoch 291/500, Batch Size: 16, Loss: 0.1021, Train Acc: 56.13%, Val Acc: 29.41%\n",
            "Epoch 292/500, Batch Size: 16, Loss: 0.0964, Train Acc: 60.42%, Val Acc: 22.55%\n",
            "Epoch 293/500, Batch Size: 16, Loss: 0.1057, Train Acc: 57.84%, Val Acc: 25.49%\n",
            "Epoch 294/500, Batch Size: 16, Loss: 0.1063, Train Acc: 57.60%, Val Acc: 23.53%\n",
            "Epoch 295/500, Batch Size: 16, Loss: 0.1005, Train Acc: 57.97%, Val Acc: 22.06%\n",
            "Epoch 296/500, Batch Size: 16, Loss: 0.1063, Train Acc: 56.25%, Val Acc: 26.96%\n",
            "Epoch 297/500, Batch Size: 16, Loss: 0.1054, Train Acc: 55.51%, Val Acc: 28.43%\n",
            "Epoch 298/500, Batch Size: 16, Loss: 0.1020, Train Acc: 57.35%, Val Acc: 24.02%\n",
            "Epoch 299/500, Batch Size: 16, Loss: 0.0937, Train Acc: 61.03%, Val Acc: 22.06%\n",
            "Epoch 300/500, Batch Size: 16, Loss: 0.1019, Train Acc: 56.62%, Val Acc: 25.49%\n",
            "Epoch 301/500, Batch Size: 16, Loss: 0.0998, Train Acc: 58.21%, Val Acc: 24.51%\n",
            "Epoch 302/500, Batch Size: 16, Loss: 0.1090, Train Acc: 56.86%, Val Acc: 25.00%\n",
            "Epoch 303/500, Batch Size: 16, Loss: 0.1008, Train Acc: 56.99%, Val Acc: 27.94%\n",
            "Epoch 304/500, Batch Size: 16, Loss: 0.1039, Train Acc: 56.37%, Val Acc: 26.47%\n",
            "Epoch 305/500, Batch Size: 16, Loss: 0.1033, Train Acc: 56.25%, Val Acc: 22.06%\n",
            "Epoch 306/500, Batch Size: 16, Loss: 0.1004, Train Acc: 56.99%, Val Acc: 27.94%\n",
            "Epoch 307/500, Batch Size: 16, Loss: 0.1017, Train Acc: 56.25%, Val Acc: 26.47%\n",
            "Epoch 308/500, Batch Size: 16, Loss: 0.0980, Train Acc: 56.00%, Val Acc: 27.45%\n",
            "Epoch 309/500, Batch Size: 16, Loss: 0.1063, Train Acc: 56.50%, Val Acc: 27.94%\n",
            "Epoch 310/500, Batch Size: 16, Loss: 0.0971, Train Acc: 58.33%, Val Acc: 30.88%\n",
            "Epoch 311/500, Batch Size: 16, Loss: 0.0999, Train Acc: 58.09%, Val Acc: 25.98%\n",
            "Epoch 312/500, Batch Size: 16, Loss: 0.1001, Train Acc: 57.48%, Val Acc: 28.92%\n",
            "Epoch 313/500, Batch Size: 16, Loss: 0.0989, Train Acc: 58.21%, Val Acc: 24.02%\n",
            "Epoch 314/500, Batch Size: 16, Loss: 0.1023, Train Acc: 58.33%, Val Acc: 26.47%\n",
            "Epoch 315/500, Batch Size: 16, Loss: 0.1110, Train Acc: 55.15%, Val Acc: 27.94%\n",
            "Epoch 316/500, Batch Size: 16, Loss: 0.1095, Train Acc: 54.17%, Val Acc: 25.98%\n",
            "Epoch 317/500, Batch Size: 16, Loss: 0.1047, Train Acc: 55.76%, Val Acc: 26.47%\n",
            "Epoch 318/500, Batch Size: 16, Loss: 0.1103, Train Acc: 55.27%, Val Acc: 26.47%\n",
            "Epoch 319/500, Batch Size: 16, Loss: 0.1049, Train Acc: 56.99%, Val Acc: 25.98%\n",
            "Epoch 320/500, Batch Size: 16, Loss: 0.1010, Train Acc: 57.48%, Val Acc: 24.02%\n",
            "Epoch 321/500, Batch Size: 16, Loss: 0.1012, Train Acc: 57.11%, Val Acc: 32.35%\n",
            "Epoch 322/500, Batch Size: 16, Loss: 0.0986, Train Acc: 59.93%, Val Acc: 26.96%\n",
            "Epoch 323/500, Batch Size: 16, Loss: 0.1074, Train Acc: 52.94%, Val Acc: 24.02%\n",
            "Epoch 324/500, Batch Size: 16, Loss: 0.1017, Train Acc: 57.97%, Val Acc: 31.86%\n",
            "Epoch 325/500, Batch Size: 16, Loss: 0.1035, Train Acc: 56.00%, Val Acc: 22.55%\n",
            "Epoch 326/500, Batch Size: 16, Loss: 0.1019, Train Acc: 58.70%, Val Acc: 23.53%\n",
            "Epoch 327/500, Batch Size: 16, Loss: 0.1022, Train Acc: 56.99%, Val Acc: 26.96%\n",
            "Epoch 328/500, Batch Size: 16, Loss: 0.1078, Train Acc: 53.68%, Val Acc: 28.43%\n",
            "Epoch 329/500, Batch Size: 16, Loss: 0.0980, Train Acc: 57.60%, Val Acc: 26.47%\n",
            "Epoch 330/500, Batch Size: 16, Loss: 0.0999, Train Acc: 56.99%, Val Acc: 31.86%\n",
            "Epoch 331/500, Batch Size: 16, Loss: 0.1082, Train Acc: 55.27%, Val Acc: 26.96%\n",
            "Epoch 332/500, Batch Size: 16, Loss: 0.1047, Train Acc: 58.58%, Val Acc: 25.49%\n",
            "Epoch 333/500, Batch Size: 16, Loss: 0.1086, Train Acc: 54.29%, Val Acc: 25.98%\n",
            "Epoch 334/500, Batch Size: 16, Loss: 0.1040, Train Acc: 54.41%, Val Acc: 26.96%\n",
            "Epoch 335/500, Batch Size: 16, Loss: 0.1025, Train Acc: 57.97%, Val Acc: 22.55%\n",
            "Epoch 336/500, Batch Size: 16, Loss: 0.1003, Train Acc: 56.13%, Val Acc: 25.00%\n",
            "Epoch 337/500, Batch Size: 16, Loss: 0.1022, Train Acc: 58.46%, Val Acc: 26.47%\n",
            "Epoch 338/500, Batch Size: 16, Loss: 0.0990, Train Acc: 58.46%, Val Acc: 26.47%\n",
            "Epoch 339/500, Batch Size: 16, Loss: 0.0979, Train Acc: 58.33%, Val Acc: 26.96%\n",
            "Epoch 340/500, Batch Size: 16, Loss: 0.1006, Train Acc: 58.33%, Val Acc: 27.45%\n",
            "Epoch 341/500, Batch Size: 16, Loss: 0.1065, Train Acc: 55.15%, Val Acc: 25.98%\n",
            "Epoch 342/500, Batch Size: 16, Loss: 0.1046, Train Acc: 55.39%, Val Acc: 25.49%\n",
            "Epoch 343/500, Batch Size: 16, Loss: 0.1089, Train Acc: 54.04%, Val Acc: 28.92%\n",
            "Epoch 344/500, Batch Size: 16, Loss: 0.1026, Train Acc: 56.50%, Val Acc: 23.53%\n",
            "Epoch 345/500, Batch Size: 16, Loss: 0.1054, Train Acc: 54.90%, Val Acc: 30.39%\n",
            "Epoch 346/500, Batch Size: 16, Loss: 0.1047, Train Acc: 55.51%, Val Acc: 29.41%\n",
            "Epoch 347/500, Batch Size: 16, Loss: 0.1027, Train Acc: 55.88%, Val Acc: 27.45%\n",
            "Epoch 348/500, Batch Size: 16, Loss: 0.1047, Train Acc: 57.11%, Val Acc: 27.94%\n",
            "Epoch 349/500, Batch Size: 16, Loss: 0.1010, Train Acc: 55.27%, Val Acc: 27.94%\n",
            "Epoch 350/500, Batch Size: 16, Loss: 0.1063, Train Acc: 55.64%, Val Acc: 25.00%\n",
            "Epoch 351/500, Batch Size: 16, Loss: 0.1014, Train Acc: 55.88%, Val Acc: 25.00%\n",
            "Epoch 352/500, Batch Size: 16, Loss: 0.1025, Train Acc: 56.99%, Val Acc: 28.43%\n",
            "Epoch 353/500, Batch Size: 16, Loss: 0.0952, Train Acc: 60.54%, Val Acc: 24.02%\n",
            "Epoch 354/500, Batch Size: 16, Loss: 0.1058, Train Acc: 57.97%, Val Acc: 24.02%\n",
            "Epoch 355/500, Batch Size: 16, Loss: 0.1037, Train Acc: 56.00%, Val Acc: 30.39%\n",
            "Epoch 356/500, Batch Size: 16, Loss: 0.1116, Train Acc: 53.19%, Val Acc: 28.43%\n",
            "Epoch 357/500, Batch Size: 16, Loss: 0.1028, Train Acc: 57.11%, Val Acc: 27.45%\n",
            "Epoch 358/500, Batch Size: 16, Loss: 0.1049, Train Acc: 55.27%, Val Acc: 28.92%\n",
            "Epoch 359/500, Batch Size: 16, Loss: 0.1058, Train Acc: 56.13%, Val Acc: 25.98%\n",
            "Epoch 360/500, Batch Size: 16, Loss: 0.1008, Train Acc: 55.51%, Val Acc: 25.98%\n",
            "Epoch 361/500, Batch Size: 16, Loss: 0.1055, Train Acc: 57.11%, Val Acc: 23.53%\n",
            "Epoch 362/500, Batch Size: 16, Loss: 0.1018, Train Acc: 57.48%, Val Acc: 20.59%\n",
            "Epoch 363/500, Batch Size: 16, Loss: 0.1058, Train Acc: 57.48%, Val Acc: 24.51%\n",
            "Epoch 364/500, Batch Size: 16, Loss: 0.1028, Train Acc: 57.60%, Val Acc: 26.47%\n",
            "Epoch 365/500, Batch Size: 16, Loss: 0.0998, Train Acc: 57.35%, Val Acc: 23.53%\n",
            "Epoch 366/500, Batch Size: 16, Loss: 0.1101, Train Acc: 55.88%, Val Acc: 23.53%\n",
            "Epoch 367/500, Batch Size: 16, Loss: 0.1051, Train Acc: 56.50%, Val Acc: 24.51%\n",
            "Epoch 368/500, Batch Size: 16, Loss: 0.1016, Train Acc: 56.86%, Val Acc: 22.06%\n",
            "Epoch 369/500, Batch Size: 16, Loss: 0.1111, Train Acc: 53.43%, Val Acc: 30.39%\n",
            "Epoch 370/500, Batch Size: 16, Loss: 0.1034, Train Acc: 57.72%, Val Acc: 25.49%\n",
            "Epoch 371/500, Batch Size: 16, Loss: 0.1027, Train Acc: 55.76%, Val Acc: 25.00%\n",
            "Epoch 372/500, Batch Size: 16, Loss: 0.1075, Train Acc: 54.17%, Val Acc: 21.08%\n",
            "Epoch 373/500, Batch Size: 16, Loss: 0.1068, Train Acc: 56.13%, Val Acc: 33.33%\n",
            "Epoch 374/500, Batch Size: 16, Loss: 0.1140, Train Acc: 52.82%, Val Acc: 22.55%\n",
            "Epoch 375/500, Batch Size: 16, Loss: 0.1007, Train Acc: 59.31%, Val Acc: 30.39%\n",
            "Epoch 376/500, Batch Size: 16, Loss: 0.1069, Train Acc: 54.53%, Val Acc: 25.00%\n",
            "Epoch 377/500, Batch Size: 16, Loss: 0.0972, Train Acc: 59.68%, Val Acc: 26.47%\n",
            "Epoch 378/500, Batch Size: 16, Loss: 0.1068, Train Acc: 56.37%, Val Acc: 28.43%\n",
            "Epoch 379/500, Batch Size: 16, Loss: 0.1019, Train Acc: 57.23%, Val Acc: 25.98%\n",
            "Epoch 380/500, Batch Size: 16, Loss: 0.1110, Train Acc: 51.96%, Val Acc: 27.94%\n",
            "Epoch 381/500, Batch Size: 16, Loss: 0.1117, Train Acc: 53.80%, Val Acc: 29.41%\n",
            "Epoch 382/500, Batch Size: 16, Loss: 0.1010, Train Acc: 58.46%, Val Acc: 21.08%\n",
            "Epoch 383/500, Batch Size: 16, Loss: 0.1102, Train Acc: 54.17%, Val Acc: 21.57%\n",
            "Epoch 384/500, Batch Size: 16, Loss: 0.0957, Train Acc: 60.29%, Val Acc: 25.98%\n",
            "Epoch 385/500, Batch Size: 16, Loss: 0.1010, Train Acc: 58.58%, Val Acc: 25.98%\n",
            "Epoch 386/500, Batch Size: 16, Loss: 0.1035, Train Acc: 56.25%, Val Acc: 28.43%\n",
            "Epoch 387/500, Batch Size: 16, Loss: 0.1035, Train Acc: 56.99%, Val Acc: 30.39%\n",
            "Epoch 388/500, Batch Size: 16, Loss: 0.0997, Train Acc: 59.44%, Val Acc: 27.45%\n",
            "Epoch 389/500, Batch Size: 16, Loss: 0.1015, Train Acc: 56.50%, Val Acc: 30.39%\n",
            "Epoch 390/500, Batch Size: 16, Loss: 0.1033, Train Acc: 56.86%, Val Acc: 23.04%\n",
            "Epoch 391/500, Batch Size: 16, Loss: 0.0973, Train Acc: 58.95%, Val Acc: 23.53%\n",
            "Epoch 392/500, Batch Size: 16, Loss: 0.0965, Train Acc: 58.46%, Val Acc: 22.55%\n",
            "Epoch 393/500, Batch Size: 16, Loss: 0.1019, Train Acc: 59.80%, Val Acc: 27.94%\n",
            "Epoch 394/500, Batch Size: 16, Loss: 0.0945, Train Acc: 60.66%, Val Acc: 27.45%\n",
            "Epoch 395/500, Batch Size: 16, Loss: 0.1070, Train Acc: 53.92%, Val Acc: 28.43%\n",
            "Epoch 396/500, Batch Size: 16, Loss: 0.1048, Train Acc: 56.62%, Val Acc: 26.96%\n",
            "Epoch 397/500, Batch Size: 16, Loss: 0.1077, Train Acc: 56.00%, Val Acc: 25.00%\n",
            "Epoch 398/500, Batch Size: 16, Loss: 0.0996, Train Acc: 56.74%, Val Acc: 28.92%\n",
            "Epoch 399/500, Batch Size: 16, Loss: 0.1042, Train Acc: 56.62%, Val Acc: 27.94%\n",
            "Epoch 400/500, Batch Size: 16, Loss: 0.0987, Train Acc: 59.07%, Val Acc: 30.88%\n",
            "Epoch 401/500, Batch Size: 16, Loss: 0.1065, Train Acc: 54.29%, Val Acc: 27.45%\n",
            "Epoch 402/500, Batch Size: 16, Loss: 0.1007, Train Acc: 57.11%, Val Acc: 26.96%\n",
            "Epoch 403/500, Batch Size: 16, Loss: 0.0935, Train Acc: 59.93%, Val Acc: 26.96%\n",
            "Epoch 404/500, Batch Size: 16, Loss: 0.1016, Train Acc: 56.00%, Val Acc: 25.98%\n",
            "Epoch 405/500, Batch Size: 16, Loss: 0.0948, Train Acc: 60.66%, Val Acc: 27.94%\n",
            "Epoch 406/500, Batch Size: 16, Loss: 0.1061, Train Acc: 54.90%, Val Acc: 23.04%\n",
            "Epoch 407/500, Batch Size: 16, Loss: 0.1061, Train Acc: 55.27%, Val Acc: 26.96%\n",
            "Epoch 408/500, Batch Size: 16, Loss: 0.1082, Train Acc: 54.66%, Val Acc: 26.47%\n",
            "Epoch 409/500, Batch Size: 16, Loss: 0.1021, Train Acc: 58.70%, Val Acc: 25.49%\n",
            "Epoch 410/500, Batch Size: 16, Loss: 0.1063, Train Acc: 59.68%, Val Acc: 22.06%\n",
            "Epoch 411/500, Batch Size: 16, Loss: 0.0964, Train Acc: 55.88%, Val Acc: 27.94%\n",
            "Epoch 412/500, Batch Size: 16, Loss: 0.0965, Train Acc: 57.97%, Val Acc: 26.96%\n",
            "Epoch 413/500, Batch Size: 16, Loss: 0.0988, Train Acc: 56.74%, Val Acc: 29.41%\n",
            "Epoch 414/500, Batch Size: 16, Loss: 0.1094, Train Acc: 53.06%, Val Acc: 27.94%\n",
            "Epoch 415/500, Batch Size: 16, Loss: 0.1065, Train Acc: 55.39%, Val Acc: 28.43%\n",
            "Epoch 416/500, Batch Size: 16, Loss: 0.1096, Train Acc: 54.04%, Val Acc: 30.39%\n",
            "Epoch 417/500, Batch Size: 16, Loss: 0.1078, Train Acc: 56.25%, Val Acc: 27.45%\n",
            "Epoch 418/500, Batch Size: 16, Loss: 0.1066, Train Acc: 55.39%, Val Acc: 24.02%\n",
            "Epoch 419/500, Batch Size: 16, Loss: 0.1001, Train Acc: 58.70%, Val Acc: 26.96%\n",
            "Epoch 420/500, Batch Size: 16, Loss: 0.0892, Train Acc: 61.40%, Val Acc: 26.47%\n",
            "Epoch 421/500, Batch Size: 16, Loss: 0.0975, Train Acc: 59.93%, Val Acc: 23.04%\n",
            "Epoch 422/500, Batch Size: 16, Loss: 0.1044, Train Acc: 55.39%, Val Acc: 24.51%\n",
            "Epoch 423/500, Batch Size: 16, Loss: 0.1043, Train Acc: 58.46%, Val Acc: 25.49%\n",
            "Epoch 424/500, Batch Size: 16, Loss: 0.1019, Train Acc: 56.86%, Val Acc: 24.02%\n",
            "Epoch 425/500, Batch Size: 16, Loss: 0.1043, Train Acc: 56.86%, Val Acc: 25.98%\n",
            "Epoch 426/500, Batch Size: 16, Loss: 0.1007, Train Acc: 56.50%, Val Acc: 25.00%\n",
            "Epoch 427/500, Batch Size: 16, Loss: 0.1059, Train Acc: 56.50%, Val Acc: 29.90%\n",
            "Epoch 428/500, Batch Size: 16, Loss: 0.1062, Train Acc: 55.15%, Val Acc: 25.00%\n",
            "Epoch 429/500, Batch Size: 16, Loss: 0.1036, Train Acc: 59.93%, Val Acc: 21.08%\n",
            "Epoch 430/500, Batch Size: 16, Loss: 0.1003, Train Acc: 58.82%, Val Acc: 28.92%\n",
            "Epoch 431/500, Batch Size: 16, Loss: 0.0936, Train Acc: 58.82%, Val Acc: 28.92%\n",
            "Epoch 432/500, Batch Size: 16, Loss: 0.1028, Train Acc: 57.97%, Val Acc: 20.59%\n",
            "Epoch 433/500, Batch Size: 16, Loss: 0.1027, Train Acc: 56.00%, Val Acc: 25.00%\n",
            "Epoch 434/500, Batch Size: 16, Loss: 0.0965, Train Acc: 58.95%, Val Acc: 30.39%\n",
            "Epoch 435/500, Batch Size: 16, Loss: 0.1040, Train Acc: 58.09%, Val Acc: 23.53%\n",
            "Epoch 436/500, Batch Size: 16, Loss: 0.1017, Train Acc: 58.46%, Val Acc: 25.49%\n",
            "Epoch 437/500, Batch Size: 16, Loss: 0.1022, Train Acc: 55.39%, Val Acc: 23.53%\n",
            "Epoch 438/500, Batch Size: 16, Loss: 0.1038, Train Acc: 56.86%, Val Acc: 26.96%\n",
            "Epoch 439/500, Batch Size: 16, Loss: 0.1017, Train Acc: 58.21%, Val Acc: 26.96%\n",
            "Epoch 440/500, Batch Size: 16, Loss: 0.1029, Train Acc: 55.02%, Val Acc: 28.92%\n",
            "Epoch 441/500, Batch Size: 16, Loss: 0.1004, Train Acc: 56.86%, Val Acc: 27.45%\n",
            "Epoch 442/500, Batch Size: 16, Loss: 0.0982, Train Acc: 58.70%, Val Acc: 28.43%\n",
            "Epoch 443/500, Batch Size: 16, Loss: 0.0989, Train Acc: 59.56%, Val Acc: 22.55%\n",
            "Epoch 444/500, Batch Size: 16, Loss: 0.0972, Train Acc: 60.66%, Val Acc: 28.43%\n",
            "Epoch 445/500, Batch Size: 16, Loss: 0.0964, Train Acc: 58.95%, Val Acc: 30.39%\n",
            "Epoch 446/500, Batch Size: 16, Loss: 0.1014, Train Acc: 57.60%, Val Acc: 24.02%\n",
            "Epoch 447/500, Batch Size: 16, Loss: 0.1031, Train Acc: 57.23%, Val Acc: 22.06%\n",
            "Epoch 448/500, Batch Size: 16, Loss: 0.1046, Train Acc: 57.11%, Val Acc: 27.94%\n",
            "Epoch 449/500, Batch Size: 16, Loss: 0.1023, Train Acc: 57.84%, Val Acc: 25.98%\n",
            "Epoch 450/500, Batch Size: 16, Loss: 0.1083, Train Acc: 56.86%, Val Acc: 28.43%\n",
            "Epoch 451/500, Batch Size: 16, Loss: 0.1055, Train Acc: 54.41%, Val Acc: 28.43%\n",
            "Epoch 452/500, Batch Size: 16, Loss: 0.0999, Train Acc: 58.95%, Val Acc: 24.51%\n",
            "Epoch 453/500, Batch Size: 16, Loss: 0.1016, Train Acc: 57.23%, Val Acc: 23.53%\n",
            "Epoch 454/500, Batch Size: 16, Loss: 0.1023, Train Acc: 56.86%, Val Acc: 23.04%\n",
            "Epoch 455/500, Batch Size: 16, Loss: 0.1021, Train Acc: 57.97%, Val Acc: 28.92%\n",
            "Epoch 456/500, Batch Size: 16, Loss: 0.1119, Train Acc: 54.17%, Val Acc: 29.90%\n",
            "Epoch 457/500, Batch Size: 16, Loss: 0.1037, Train Acc: 56.50%, Val Acc: 25.98%\n",
            "Epoch 458/500, Batch Size: 16, Loss: 0.1048, Train Acc: 54.66%, Val Acc: 29.90%\n",
            "Epoch 459/500, Batch Size: 16, Loss: 0.0951, Train Acc: 60.17%, Val Acc: 29.90%\n",
            "Epoch 460/500, Batch Size: 16, Loss: 0.1088, Train Acc: 53.68%, Val Acc: 26.96%\n",
            "Epoch 461/500, Batch Size: 16, Loss: 0.1013, Train Acc: 58.46%, Val Acc: 24.51%\n",
            "Epoch 462/500, Batch Size: 16, Loss: 0.0994, Train Acc: 58.09%, Val Acc: 30.39%\n",
            "Epoch 463/500, Batch Size: 16, Loss: 0.0968, Train Acc: 58.46%, Val Acc: 22.06%\n",
            "Epoch 464/500, Batch Size: 16, Loss: 0.1004, Train Acc: 60.42%, Val Acc: 23.53%\n",
            "Epoch 465/500, Batch Size: 16, Loss: 0.0991, Train Acc: 58.46%, Val Acc: 25.49%\n",
            "Epoch 466/500, Batch Size: 16, Loss: 0.1029, Train Acc: 56.50%, Val Acc: 24.51%\n",
            "Epoch 467/500, Batch Size: 16, Loss: 0.1023, Train Acc: 58.95%, Val Acc: 28.92%\n",
            "Epoch 468/500, Batch Size: 16, Loss: 0.1040, Train Acc: 56.74%, Val Acc: 19.12%\n",
            "Epoch 469/500, Batch Size: 16, Loss: 0.1005, Train Acc: 55.88%, Val Acc: 23.53%\n",
            "Epoch 470/500, Batch Size: 16, Loss: 0.0972, Train Acc: 58.46%, Val Acc: 25.49%\n",
            "Epoch 471/500, Batch Size: 16, Loss: 0.1016, Train Acc: 57.11%, Val Acc: 31.37%\n",
            "Epoch 472/500, Batch Size: 16, Loss: 0.0978, Train Acc: 57.23%, Val Acc: 24.02%\n",
            "Epoch 473/500, Batch Size: 16, Loss: 0.0939, Train Acc: 59.80%, Val Acc: 26.96%\n",
            "Epoch 474/500, Batch Size: 16, Loss: 0.0982, Train Acc: 59.19%, Val Acc: 29.41%\n",
            "Epoch 475/500, Batch Size: 16, Loss: 0.1115, Train Acc: 53.80%, Val Acc: 24.51%\n",
            "Epoch 476/500, Batch Size: 16, Loss: 0.1084, Train Acc: 56.50%, Val Acc: 30.88%\n",
            "Epoch 477/500, Batch Size: 16, Loss: 0.1023, Train Acc: 57.72%, Val Acc: 20.59%\n",
            "Epoch 478/500, Batch Size: 16, Loss: 0.1016, Train Acc: 56.25%, Val Acc: 26.47%\n",
            "Epoch 479/500, Batch Size: 16, Loss: 0.1088, Train Acc: 55.15%, Val Acc: 30.39%\n",
            "Epoch 480/500, Batch Size: 16, Loss: 0.0991, Train Acc: 59.07%, Val Acc: 27.45%\n",
            "Epoch 481/500, Batch Size: 16, Loss: 0.1045, Train Acc: 55.88%, Val Acc: 24.51%\n",
            "Epoch 482/500, Batch Size: 16, Loss: 0.1023, Train Acc: 56.13%, Val Acc: 26.47%\n",
            "Epoch 483/500, Batch Size: 16, Loss: 0.1062, Train Acc: 52.57%, Val Acc: 26.96%\n",
            "Epoch 484/500, Batch Size: 16, Loss: 0.1056, Train Acc: 57.60%, Val Acc: 25.98%\n",
            "Epoch 485/500, Batch Size: 16, Loss: 0.1048, Train Acc: 54.78%, Val Acc: 26.47%\n",
            "Epoch 486/500, Batch Size: 16, Loss: 0.0923, Train Acc: 60.05%, Val Acc: 29.41%\n",
            "Epoch 487/500, Batch Size: 16, Loss: 0.1028, Train Acc: 58.95%, Val Acc: 24.51%\n",
            "Epoch 488/500, Batch Size: 16, Loss: 0.1012, Train Acc: 56.13%, Val Acc: 25.98%\n",
            "Epoch 489/500, Batch Size: 16, Loss: 0.1015, Train Acc: 57.23%, Val Acc: 26.96%\n",
            "Epoch 490/500, Batch Size: 16, Loss: 0.1017, Train Acc: 56.25%, Val Acc: 27.45%\n",
            "Epoch 491/500, Batch Size: 16, Loss: 0.1070, Train Acc: 55.76%, Val Acc: 27.45%\n",
            "Epoch 492/500, Batch Size: 16, Loss: 0.1065, Train Acc: 54.04%, Val Acc: 25.49%\n",
            "Epoch 493/500, Batch Size: 16, Loss: 0.0955, Train Acc: 60.05%, Val Acc: 27.94%\n",
            "Epoch 494/500, Batch Size: 16, Loss: 0.1005, Train Acc: 58.21%, Val Acc: 27.94%\n",
            "Epoch 495/500, Batch Size: 16, Loss: 0.0968, Train Acc: 59.68%, Val Acc: 25.00%\n",
            "Epoch 496/500, Batch Size: 16, Loss: 0.1155, Train Acc: 55.51%, Val Acc: 24.02%\n",
            "Epoch 497/500, Batch Size: 16, Loss: 0.1107, Train Acc: 55.02%, Val Acc: 27.45%\n",
            "Epoch 498/500, Batch Size: 16, Loss: 0.1043, Train Acc: 55.39%, Val Acc: 27.45%\n",
            "Epoch 499/500, Batch Size: 16, Loss: 0.0944, Train Acc: 59.80%, Val Acc: 24.51%\n",
            "Epoch 500/500, Batch Size: 16, Loss: 0.0980, Train Acc: 58.21%, Val Acc: 28.92%\n",
            "Training complete\n",
            "Test Accuracy: 40.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(192),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(192),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(512 * 12 * 12, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 512 * 12 * 12)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.01\n",
        "dropout_rate = 0.5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=1000):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    best_val_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "\n",
        "\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    return model\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=1000)\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48LrYKAtSDzU",
        "outputId": "9576dab8-7a70-46e0-cb28-2d84971fb8d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000, Batch Size: 64, Loss: 0.6909, Train Acc: 1.10%, Val Acc: 2.94%\n",
            "Epoch 2/1000, Batch Size: 64, Loss: 0.4310, Train Acc: 1.84%, Val Acc: 1.96%\n",
            "Epoch 3/1000, Batch Size: 64, Loss: 0.3617, Train Acc: 1.59%, Val Acc: 2.94%\n",
            "Epoch 4/1000, Batch Size: 64, Loss: 0.3304, Train Acc: 1.84%, Val Acc: 5.39%\n",
            "Epoch 5/1000, Batch Size: 64, Loss: 0.2490, Train Acc: 2.08%, Val Acc: 2.94%\n",
            "Epoch 6/1000, Batch Size: 64, Loss: 0.1594, Train Acc: 3.80%, Val Acc: 4.41%\n",
            "Epoch 7/1000, Batch Size: 64, Loss: 0.1134, Train Acc: 3.80%, Val Acc: 2.45%\n",
            "Epoch 8/1000, Batch Size: 64, Loss: 0.0936, Train Acc: 4.53%, Val Acc: 1.96%\n",
            "Epoch 9/1000, Batch Size: 64, Loss: 0.0847, Train Acc: 5.15%, Val Acc: 3.43%\n",
            "Epoch 10/1000, Batch Size: 64, Loss: 0.0811, Train Acc: 6.00%, Val Acc: 5.88%\n",
            "Epoch 11/1000, Batch Size: 64, Loss: 0.0758, Train Acc: 6.86%, Val Acc: 3.43%\n",
            "Epoch 12/1000, Batch Size: 64, Loss: 0.0734, Train Acc: 8.58%, Val Acc: 7.84%\n",
            "Epoch 13/1000, Batch Size: 64, Loss: 0.0725, Train Acc: 6.86%, Val Acc: 9.80%\n",
            "Epoch 14/1000, Batch Size: 64, Loss: 0.0691, Train Acc: 8.70%, Val Acc: 5.88%\n",
            "Epoch 15/1000, Batch Size: 64, Loss: 0.0669, Train Acc: 7.97%, Val Acc: 8.33%\n",
            "Epoch 16/1000, Batch Size: 64, Loss: 0.0666, Train Acc: 10.66%, Val Acc: 6.37%\n",
            "Epoch 17/1000, Batch Size: 64, Loss: 0.0643, Train Acc: 10.42%, Val Acc: 4.41%\n",
            "Epoch 18/1000, Batch Size: 64, Loss: 0.0618, Train Acc: 11.89%, Val Acc: 9.31%\n",
            "Epoch 19/1000, Batch Size: 64, Loss: 0.0633, Train Acc: 13.24%, Val Acc: 8.82%\n",
            "Epoch 20/1000, Batch Size: 64, Loss: 0.0621, Train Acc: 16.54%, Val Acc: 8.82%\n",
            "Epoch 21/1000, Batch Size: 64, Loss: 0.0595, Train Acc: 14.46%, Val Acc: 12.25%\n",
            "Epoch 22/1000, Batch Size: 64, Loss: 0.0576, Train Acc: 16.79%, Val Acc: 12.25%\n",
            "Epoch 23/1000, Batch Size: 64, Loss: 0.0579, Train Acc: 15.93%, Val Acc: 13.24%\n",
            "Epoch 24/1000, Batch Size: 64, Loss: 0.0544, Train Acc: 19.12%, Val Acc: 10.78%\n",
            "Epoch 25/1000, Batch Size: 64, Loss: 0.0572, Train Acc: 20.47%, Val Acc: 16.67%\n",
            "Epoch 26/1000, Batch Size: 64, Loss: 0.0561, Train Acc: 20.34%, Val Acc: 9.31%\n",
            "Epoch 27/1000, Batch Size: 64, Loss: 0.0573, Train Acc: 20.10%, Val Acc: 13.73%\n",
            "Epoch 28/1000, Batch Size: 64, Loss: 0.0556, Train Acc: 20.22%, Val Acc: 13.24%\n",
            "Epoch 29/1000, Batch Size: 64, Loss: 0.0569, Train Acc: 18.63%, Val Acc: 13.73%\n",
            "Epoch 30/1000, Batch Size: 64, Loss: 0.0532, Train Acc: 21.20%, Val Acc: 15.69%\n",
            "Epoch 31/1000, Batch Size: 64, Loss: 0.0531, Train Acc: 23.28%, Val Acc: 15.69%\n",
            "Epoch 32/1000, Batch Size: 64, Loss: 0.0521, Train Acc: 23.28%, Val Acc: 9.80%\n",
            "Epoch 33/1000, Batch Size: 64, Loss: 0.0504, Train Acc: 24.63%, Val Acc: 8.82%\n",
            "Epoch 34/1000, Batch Size: 64, Loss: 0.0476, Train Acc: 29.29%, Val Acc: 16.18%\n",
            "Epoch 35/1000, Batch Size: 64, Loss: 0.0539, Train Acc: 22.79%, Val Acc: 13.24%\n",
            "Epoch 36/1000, Batch Size: 64, Loss: 0.0520, Train Acc: 22.55%, Val Acc: 15.69%\n",
            "Epoch 37/1000, Batch Size: 64, Loss: 0.0500, Train Acc: 24.75%, Val Acc: 13.73%\n",
            "Epoch 38/1000, Batch Size: 64, Loss: 0.0496, Train Acc: 24.51%, Val Acc: 16.18%\n",
            "Epoch 39/1000, Batch Size: 64, Loss: 0.0488, Train Acc: 25.74%, Val Acc: 13.73%\n",
            "Epoch 40/1000, Batch Size: 64, Loss: 0.0497, Train Acc: 25.49%, Val Acc: 18.63%\n",
            "Epoch 41/1000, Batch Size: 64, Loss: 0.0498, Train Acc: 25.12%, Val Acc: 16.67%\n",
            "Epoch 42/1000, Batch Size: 64, Loss: 0.0494, Train Acc: 26.47%, Val Acc: 15.69%\n",
            "Epoch 43/1000, Batch Size: 64, Loss: 0.0490, Train Acc: 26.72%, Val Acc: 14.71%\n",
            "Epoch 44/1000, Batch Size: 64, Loss: 0.0474, Train Acc: 30.64%, Val Acc: 15.20%\n",
            "Epoch 45/1000, Batch Size: 64, Loss: 0.0491, Train Acc: 31.00%, Val Acc: 15.20%\n",
            "Epoch 46/1000, Batch Size: 64, Loss: 0.0487, Train Acc: 29.66%, Val Acc: 21.08%\n",
            "Epoch 47/1000, Batch Size: 64, Loss: 0.0479, Train Acc: 30.51%, Val Acc: 15.20%\n",
            "Epoch 48/1000, Batch Size: 64, Loss: 0.0471, Train Acc: 31.25%, Val Acc: 18.14%\n",
            "Epoch 49/1000, Batch Size: 64, Loss: 0.0500, Train Acc: 28.55%, Val Acc: 19.12%\n",
            "Epoch 50/1000, Batch Size: 64, Loss: 0.0486, Train Acc: 27.94%, Val Acc: 17.65%\n",
            "Epoch 51/1000, Batch Size: 64, Loss: 0.0472, Train Acc: 32.97%, Val Acc: 16.67%\n",
            "Epoch 52/1000, Batch Size: 64, Loss: 0.0470, Train Acc: 30.39%, Val Acc: 16.67%\n",
            "Epoch 53/1000, Batch Size: 64, Loss: 0.0467, Train Acc: 31.37%, Val Acc: 18.14%\n",
            "Epoch 54/1000, Batch Size: 64, Loss: 0.0437, Train Acc: 34.31%, Val Acc: 18.14%\n",
            "Epoch 55/1000, Batch Size: 64, Loss: 0.0429, Train Acc: 35.54%, Val Acc: 18.14%\n",
            "Epoch 56/1000, Batch Size: 64, Loss: 0.0404, Train Acc: 39.09%, Val Acc: 20.10%\n",
            "Epoch 57/1000, Batch Size: 64, Loss: 0.0412, Train Acc: 34.44%, Val Acc: 15.69%\n",
            "Epoch 58/1000, Batch Size: 64, Loss: 0.0411, Train Acc: 36.27%, Val Acc: 19.12%\n",
            "Epoch 59/1000, Batch Size: 64, Loss: 0.0408, Train Acc: 36.27%, Val Acc: 23.04%\n",
            "Epoch 60/1000, Batch Size: 64, Loss: 0.0412, Train Acc: 36.15%, Val Acc: 18.63%\n",
            "Epoch 61/1000, Batch Size: 64, Loss: 0.0395, Train Acc: 37.01%, Val Acc: 16.67%\n",
            "Epoch 62/1000, Batch Size: 64, Loss: 0.0401, Train Acc: 37.75%, Val Acc: 23.53%\n",
            "Epoch 63/1000, Batch Size: 64, Loss: 0.0397, Train Acc: 37.50%, Val Acc: 20.59%\n",
            "Epoch 64/1000, Batch Size: 64, Loss: 0.0384, Train Acc: 37.38%, Val Acc: 19.61%\n",
            "Epoch 65/1000, Batch Size: 64, Loss: 0.0388, Train Acc: 37.38%, Val Acc: 18.63%\n",
            "Epoch 66/1000, Batch Size: 64, Loss: 0.0392, Train Acc: 38.60%, Val Acc: 24.02%\n",
            "Epoch 67/1000, Batch Size: 64, Loss: 0.0399, Train Acc: 40.20%, Val Acc: 24.51%\n",
            "Epoch 68/1000, Batch Size: 64, Loss: 0.0401, Train Acc: 38.73%, Val Acc: 19.12%\n",
            "Epoch 69/1000, Batch Size: 64, Loss: 0.0390, Train Acc: 37.13%, Val Acc: 19.61%\n",
            "Epoch 70/1000, Batch Size: 64, Loss: 0.0381, Train Acc: 40.44%, Val Acc: 22.55%\n",
            "Epoch 71/1000, Batch Size: 64, Loss: 0.0370, Train Acc: 41.91%, Val Acc: 17.16%\n",
            "Epoch 72/1000, Batch Size: 64, Loss: 0.0376, Train Acc: 40.07%, Val Acc: 17.16%\n",
            "Epoch 73/1000, Batch Size: 64, Loss: 0.0374, Train Acc: 41.67%, Val Acc: 24.02%\n",
            "Epoch 74/1000, Batch Size: 64, Loss: 0.0369, Train Acc: 42.40%, Val Acc: 21.57%\n",
            "Epoch 75/1000, Batch Size: 64, Loss: 0.0362, Train Acc: 44.49%, Val Acc: 27.45%\n",
            "Epoch 76/1000, Batch Size: 64, Loss: 0.0354, Train Acc: 44.49%, Val Acc: 23.04%\n",
            "Epoch 77/1000, Batch Size: 64, Loss: 0.0385, Train Acc: 39.58%, Val Acc: 18.14%\n",
            "Epoch 78/1000, Batch Size: 64, Loss: 0.0370, Train Acc: 42.03%, Val Acc: 15.20%\n",
            "Epoch 79/1000, Batch Size: 64, Loss: 0.0356, Train Acc: 42.28%, Val Acc: 22.06%\n",
            "Epoch 80/1000, Batch Size: 64, Loss: 0.0383, Train Acc: 39.71%, Val Acc: 24.51%\n",
            "Epoch 81/1000, Batch Size: 64, Loss: 0.0365, Train Acc: 41.79%, Val Acc: 20.59%\n",
            "Epoch 82/1000, Batch Size: 64, Loss: 0.0342, Train Acc: 45.96%, Val Acc: 24.02%\n",
            "Epoch 83/1000, Batch Size: 64, Loss: 0.0353, Train Acc: 45.10%, Val Acc: 20.10%\n",
            "Epoch 84/1000, Batch Size: 64, Loss: 0.0366, Train Acc: 41.42%, Val Acc: 22.55%\n",
            "Epoch 85/1000, Batch Size: 64, Loss: 0.0360, Train Acc: 43.26%, Val Acc: 18.63%\n",
            "Epoch 86/1000, Batch Size: 64, Loss: 0.0375, Train Acc: 43.01%, Val Acc: 23.04%\n",
            "Epoch 87/1000, Batch Size: 64, Loss: 0.0370, Train Acc: 42.77%, Val Acc: 24.51%\n",
            "Epoch 88/1000, Batch Size: 64, Loss: 0.0374, Train Acc: 44.24%, Val Acc: 23.04%\n",
            "Epoch 89/1000, Batch Size: 64, Loss: 0.0373, Train Acc: 42.16%, Val Acc: 20.10%\n",
            "Epoch 90/1000, Batch Size: 64, Loss: 0.0368, Train Acc: 42.89%, Val Acc: 21.57%\n",
            "Epoch 91/1000, Batch Size: 64, Loss: 0.0337, Train Acc: 44.36%, Val Acc: 22.55%\n",
            "Epoch 92/1000, Batch Size: 64, Loss: 0.0326, Train Acc: 47.79%, Val Acc: 23.53%\n",
            "Epoch 93/1000, Batch Size: 64, Loss: 0.0311, Train Acc: 47.55%, Val Acc: 16.18%\n",
            "Epoch 94/1000, Batch Size: 64, Loss: 0.0331, Train Acc: 46.69%, Val Acc: 24.51%\n",
            "Epoch 95/1000, Batch Size: 64, Loss: 0.0324, Train Acc: 48.16%, Val Acc: 23.53%\n",
            "Epoch 96/1000, Batch Size: 64, Loss: 0.0310, Train Acc: 48.16%, Val Acc: 25.00%\n",
            "Epoch 97/1000, Batch Size: 64, Loss: 0.0344, Train Acc: 46.20%, Val Acc: 22.55%\n",
            "Epoch 98/1000, Batch Size: 64, Loss: 0.0313, Train Acc: 48.65%, Val Acc: 22.55%\n",
            "Epoch 99/1000, Batch Size: 64, Loss: 0.0314, Train Acc: 48.53%, Val Acc: 24.51%\n",
            "Epoch 100/1000, Batch Size: 64, Loss: 0.0351, Train Acc: 43.38%, Val Acc: 23.04%\n",
            "Epoch 101/1000, Batch Size: 64, Loss: 0.0324, Train Acc: 47.06%, Val Acc: 30.88%\n",
            "Epoch 102/1000, Batch Size: 64, Loss: 0.0317, Train Acc: 51.10%, Val Acc: 21.08%\n",
            "Epoch 103/1000, Batch Size: 64, Loss: 0.0321, Train Acc: 49.75%, Val Acc: 23.53%\n",
            "Epoch 104/1000, Batch Size: 64, Loss: 0.0307, Train Acc: 51.47%, Val Acc: 23.04%\n",
            "Epoch 105/1000, Batch Size: 64, Loss: 0.0314, Train Acc: 50.37%, Val Acc: 23.04%\n",
            "Epoch 106/1000, Batch Size: 64, Loss: 0.0330, Train Acc: 47.43%, Val Acc: 26.47%\n",
            "Epoch 107/1000, Batch Size: 64, Loss: 0.0310, Train Acc: 48.90%, Val Acc: 25.98%\n",
            "Epoch 108/1000, Batch Size: 64, Loss: 0.0306, Train Acc: 50.00%, Val Acc: 20.59%\n",
            "Epoch 109/1000, Batch Size: 64, Loss: 0.0293, Train Acc: 52.82%, Val Acc: 21.08%\n",
            "Epoch 110/1000, Batch Size: 64, Loss: 0.0302, Train Acc: 50.12%, Val Acc: 23.53%\n",
            "Epoch 111/1000, Batch Size: 64, Loss: 0.0297, Train Acc: 51.35%, Val Acc: 25.49%\n",
            "Epoch 112/1000, Batch Size: 64, Loss: 0.0278, Train Acc: 57.11%, Val Acc: 26.47%\n",
            "Epoch 113/1000, Batch Size: 64, Loss: 0.0284, Train Acc: 52.57%, Val Acc: 25.98%\n",
            "Epoch 114/1000, Batch Size: 64, Loss: 0.0292, Train Acc: 53.06%, Val Acc: 27.45%\n",
            "Epoch 115/1000, Batch Size: 64, Loss: 0.0308, Train Acc: 51.35%, Val Acc: 24.02%\n",
            "Epoch 116/1000, Batch Size: 64, Loss: 0.0288, Train Acc: 52.08%, Val Acc: 23.53%\n",
            "Epoch 117/1000, Batch Size: 64, Loss: 0.0291, Train Acc: 52.08%, Val Acc: 24.51%\n",
            "Epoch 118/1000, Batch Size: 64, Loss: 0.0314, Train Acc: 51.35%, Val Acc: 23.53%\n",
            "Epoch 119/1000, Batch Size: 64, Loss: 0.0307, Train Acc: 51.96%, Val Acc: 29.90%\n",
            "Epoch 120/1000, Batch Size: 64, Loss: 0.0294, Train Acc: 51.10%, Val Acc: 25.98%\n",
            "Epoch 121/1000, Batch Size: 64, Loss: 0.0280, Train Acc: 54.53%, Val Acc: 22.55%\n",
            "Epoch 122/1000, Batch Size: 64, Loss: 0.0267, Train Acc: 55.76%, Val Acc: 29.41%\n",
            "Epoch 123/1000, Batch Size: 64, Loss: 0.0298, Train Acc: 52.08%, Val Acc: 21.08%\n",
            "Epoch 124/1000, Batch Size: 64, Loss: 0.0279, Train Acc: 53.06%, Val Acc: 25.98%\n",
            "Epoch 125/1000, Batch Size: 64, Loss: 0.0292, Train Acc: 51.35%, Val Acc: 26.47%\n",
            "Epoch 126/1000, Batch Size: 64, Loss: 0.0284, Train Acc: 53.43%, Val Acc: 27.94%\n",
            "Epoch 127/1000, Batch Size: 64, Loss: 0.0262, Train Acc: 56.37%, Val Acc: 26.47%\n",
            "Epoch 128/1000, Batch Size: 64, Loss: 0.0264, Train Acc: 55.76%, Val Acc: 27.94%\n",
            "Epoch 129/1000, Batch Size: 64, Loss: 0.0263, Train Acc: 56.25%, Val Acc: 28.92%\n",
            "Epoch 130/1000, Batch Size: 64, Loss: 0.0271, Train Acc: 55.64%, Val Acc: 28.92%\n",
            "Epoch 131/1000, Batch Size: 64, Loss: 0.0281, Train Acc: 56.62%, Val Acc: 31.37%\n",
            "Epoch 132/1000, Batch Size: 64, Loss: 0.0259, Train Acc: 56.86%, Val Acc: 27.45%\n",
            "Epoch 133/1000, Batch Size: 64, Loss: 0.0252, Train Acc: 57.35%, Val Acc: 29.41%\n",
            "Epoch 134/1000, Batch Size: 64, Loss: 0.0259, Train Acc: 57.72%, Val Acc: 25.98%\n",
            "Epoch 135/1000, Batch Size: 64, Loss: 0.0261, Train Acc: 56.74%, Val Acc: 29.41%\n",
            "Epoch 136/1000, Batch Size: 64, Loss: 0.0255, Train Acc: 57.23%, Val Acc: 25.98%\n",
            "Epoch 137/1000, Batch Size: 64, Loss: 0.0259, Train Acc: 55.76%, Val Acc: 27.45%\n",
            "Epoch 138/1000, Batch Size: 64, Loss: 0.0259, Train Acc: 56.86%, Val Acc: 30.88%\n",
            "Epoch 139/1000, Batch Size: 64, Loss: 0.0247, Train Acc: 57.11%, Val Acc: 27.45%\n",
            "Epoch 140/1000, Batch Size: 64, Loss: 0.0243, Train Acc: 59.44%, Val Acc: 28.92%\n",
            "Epoch 141/1000, Batch Size: 64, Loss: 0.0260, Train Acc: 58.82%, Val Acc: 26.96%\n",
            "Epoch 142/1000, Batch Size: 64, Loss: 0.0253, Train Acc: 60.42%, Val Acc: 28.92%\n",
            "Epoch 143/1000, Batch Size: 64, Loss: 0.0257, Train Acc: 57.84%, Val Acc: 27.94%\n",
            "Epoch 144/1000, Batch Size: 64, Loss: 0.0254, Train Acc: 58.21%, Val Acc: 28.92%\n",
            "Epoch 145/1000, Batch Size: 64, Loss: 0.0271, Train Acc: 54.29%, Val Acc: 25.98%\n",
            "Epoch 146/1000, Batch Size: 64, Loss: 0.0230, Train Acc: 61.76%, Val Acc: 29.90%\n",
            "Epoch 147/1000, Batch Size: 64, Loss: 0.0233, Train Acc: 60.05%, Val Acc: 29.41%\n",
            "Epoch 148/1000, Batch Size: 64, Loss: 0.0245, Train Acc: 58.82%, Val Acc: 26.47%\n",
            "Epoch 149/1000, Batch Size: 64, Loss: 0.0232, Train Acc: 59.93%, Val Acc: 27.45%\n",
            "Epoch 150/1000, Batch Size: 64, Loss: 0.0258, Train Acc: 57.72%, Val Acc: 24.02%\n",
            "Epoch 151/1000, Batch Size: 64, Loss: 0.0238, Train Acc: 59.19%, Val Acc: 24.51%\n",
            "Epoch 152/1000, Batch Size: 64, Loss: 0.0249, Train Acc: 58.46%, Val Acc: 25.49%\n",
            "Epoch 153/1000, Batch Size: 64, Loss: 0.0242, Train Acc: 58.33%, Val Acc: 31.86%\n",
            "Epoch 154/1000, Batch Size: 64, Loss: 0.0220, Train Acc: 63.73%, Val Acc: 33.82%\n",
            "Epoch 155/1000, Batch Size: 64, Loss: 0.0226, Train Acc: 62.62%, Val Acc: 33.82%\n",
            "Epoch 156/1000, Batch Size: 64, Loss: 0.0223, Train Acc: 62.87%, Val Acc: 29.90%\n",
            "Epoch 157/1000, Batch Size: 64, Loss: 0.0233, Train Acc: 62.01%, Val Acc: 33.82%\n",
            "Epoch 158/1000, Batch Size: 64, Loss: 0.0225, Train Acc: 63.48%, Val Acc: 28.92%\n",
            "Epoch 159/1000, Batch Size: 64, Loss: 0.0233, Train Acc: 60.42%, Val Acc: 26.96%\n",
            "Epoch 160/1000, Batch Size: 64, Loss: 0.0240, Train Acc: 60.66%, Val Acc: 27.45%\n",
            "Epoch 161/1000, Batch Size: 64, Loss: 0.0237, Train Acc: 61.64%, Val Acc: 35.29%\n",
            "Epoch 162/1000, Batch Size: 64, Loss: 0.0234, Train Acc: 61.27%, Val Acc: 28.43%\n",
            "Epoch 163/1000, Batch Size: 64, Loss: 0.0214, Train Acc: 63.97%, Val Acc: 33.82%\n",
            "Epoch 164/1000, Batch Size: 64, Loss: 0.0214, Train Acc: 64.58%, Val Acc: 25.98%\n",
            "Epoch 165/1000, Batch Size: 64, Loss: 0.0221, Train Acc: 62.50%, Val Acc: 28.92%\n",
            "Epoch 166/1000, Batch Size: 64, Loss: 0.0230, Train Acc: 62.13%, Val Acc: 29.90%\n",
            "Epoch 167/1000, Batch Size: 64, Loss: 0.0221, Train Acc: 61.89%, Val Acc: 28.43%\n",
            "Epoch 168/1000, Batch Size: 64, Loss: 0.0225, Train Acc: 63.11%, Val Acc: 31.37%\n",
            "Epoch 169/1000, Batch Size: 64, Loss: 0.0213, Train Acc: 64.22%, Val Acc: 28.43%\n",
            "Epoch 170/1000, Batch Size: 64, Loss: 0.0228, Train Acc: 62.99%, Val Acc: 39.71%\n",
            "Epoch 171/1000, Batch Size: 64, Loss: 0.0212, Train Acc: 65.69%, Val Acc: 33.82%\n",
            "Epoch 172/1000, Batch Size: 64, Loss: 0.0215, Train Acc: 62.75%, Val Acc: 29.90%\n",
            "Epoch 173/1000, Batch Size: 64, Loss: 0.0210, Train Acc: 65.32%, Val Acc: 33.33%\n",
            "Epoch 174/1000, Batch Size: 64, Loss: 0.0214, Train Acc: 64.95%, Val Acc: 28.43%\n",
            "Epoch 175/1000, Batch Size: 64, Loss: 0.0213, Train Acc: 64.22%, Val Acc: 29.41%\n",
            "Epoch 176/1000, Batch Size: 64, Loss: 0.0199, Train Acc: 65.44%, Val Acc: 33.33%\n",
            "Epoch 177/1000, Batch Size: 64, Loss: 0.0222, Train Acc: 64.34%, Val Acc: 32.84%\n",
            "Epoch 178/1000, Batch Size: 64, Loss: 0.0202, Train Acc: 66.79%, Val Acc: 32.35%\n",
            "Epoch 179/1000, Batch Size: 64, Loss: 0.0198, Train Acc: 67.89%, Val Acc: 28.92%\n",
            "Epoch 180/1000, Batch Size: 64, Loss: 0.0201, Train Acc: 64.83%, Val Acc: 29.41%\n",
            "Epoch 181/1000, Batch Size: 64, Loss: 0.0212, Train Acc: 64.34%, Val Acc: 30.88%\n",
            "Epoch 182/1000, Batch Size: 64, Loss: 0.0209, Train Acc: 64.09%, Val Acc: 30.88%\n",
            "Epoch 183/1000, Batch Size: 64, Loss: 0.0192, Train Acc: 67.03%, Val Acc: 31.86%\n",
            "Epoch 184/1000, Batch Size: 64, Loss: 0.0197, Train Acc: 66.30%, Val Acc: 31.86%\n",
            "Epoch 185/1000, Batch Size: 64, Loss: 0.0208, Train Acc: 65.07%, Val Acc: 29.41%\n",
            "Epoch 186/1000, Batch Size: 64, Loss: 0.0204, Train Acc: 64.95%, Val Acc: 35.29%\n",
            "Epoch 187/1000, Batch Size: 64, Loss: 0.0197, Train Acc: 67.28%, Val Acc: 25.49%\n",
            "Epoch 188/1000, Batch Size: 64, Loss: 0.0202, Train Acc: 66.79%, Val Acc: 31.86%\n",
            "Epoch 189/1000, Batch Size: 64, Loss: 0.0189, Train Acc: 69.61%, Val Acc: 31.37%\n",
            "Epoch 190/1000, Batch Size: 64, Loss: 0.0202, Train Acc: 65.56%, Val Acc: 33.82%\n",
            "Epoch 191/1000, Batch Size: 64, Loss: 0.0178, Train Acc: 69.00%, Val Acc: 27.45%\n",
            "Epoch 192/1000, Batch Size: 64, Loss: 0.0177, Train Acc: 69.24%, Val Acc: 36.27%\n",
            "Epoch 193/1000, Batch Size: 64, Loss: 0.0198, Train Acc: 69.12%, Val Acc: 26.47%\n",
            "Epoch 194/1000, Batch Size: 64, Loss: 0.0189, Train Acc: 67.77%, Val Acc: 29.90%\n",
            "Epoch 195/1000, Batch Size: 64, Loss: 0.0181, Train Acc: 69.00%, Val Acc: 26.96%\n",
            "Epoch 196/1000, Batch Size: 64, Loss: 0.0185, Train Acc: 67.52%, Val Acc: 33.82%\n",
            "Epoch 197/1000, Batch Size: 64, Loss: 0.0189, Train Acc: 66.54%, Val Acc: 34.31%\n",
            "Epoch 198/1000, Batch Size: 64, Loss: 0.0192, Train Acc: 67.28%, Val Acc: 29.90%\n",
            "Epoch 199/1000, Batch Size: 64, Loss: 0.0180, Train Acc: 70.59%, Val Acc: 29.41%\n",
            "Epoch 200/1000, Batch Size: 64, Loss: 0.0175, Train Acc: 70.34%, Val Acc: 26.96%\n",
            "Epoch 201/1000, Batch Size: 64, Loss: 0.0179, Train Acc: 68.75%, Val Acc: 35.78%\n",
            "Epoch 202/1000, Batch Size: 64, Loss: 0.0181, Train Acc: 69.73%, Val Acc: 35.78%\n",
            "Epoch 203/1000, Batch Size: 64, Loss: 0.0160, Train Acc: 72.92%, Val Acc: 33.82%\n",
            "Epoch 204/1000, Batch Size: 64, Loss: 0.0177, Train Acc: 70.83%, Val Acc: 36.76%\n",
            "Epoch 205/1000, Batch Size: 64, Loss: 0.0176, Train Acc: 70.71%, Val Acc: 31.37%\n",
            "Epoch 206/1000, Batch Size: 64, Loss: 0.0187, Train Acc: 69.73%, Val Acc: 32.84%\n",
            "Epoch 207/1000, Batch Size: 64, Loss: 0.0189, Train Acc: 71.69%, Val Acc: 35.29%\n",
            "Epoch 208/1000, Batch Size: 64, Loss: 0.0178, Train Acc: 70.22%, Val Acc: 28.43%\n",
            "Epoch 209/1000, Batch Size: 64, Loss: 0.0155, Train Acc: 73.41%, Val Acc: 33.82%\n",
            "Epoch 210/1000, Batch Size: 64, Loss: 0.0164, Train Acc: 71.69%, Val Acc: 33.82%\n",
            "Epoch 211/1000, Batch Size: 64, Loss: 0.0180, Train Acc: 69.36%, Val Acc: 31.37%\n",
            "Epoch 212/1000, Batch Size: 64, Loss: 0.0166, Train Acc: 70.47%, Val Acc: 35.29%\n",
            "Epoch 213/1000, Batch Size: 64, Loss: 0.0161, Train Acc: 74.39%, Val Acc: 34.80%\n",
            "Epoch 214/1000, Batch Size: 64, Loss: 0.0171, Train Acc: 70.83%, Val Acc: 32.84%\n",
            "Epoch 215/1000, Batch Size: 64, Loss: 0.0154, Train Acc: 74.39%, Val Acc: 33.82%\n",
            "Epoch 216/1000, Batch Size: 64, Loss: 0.0180, Train Acc: 68.63%, Val Acc: 35.78%\n",
            "Epoch 217/1000, Batch Size: 64, Loss: 0.0160, Train Acc: 73.28%, Val Acc: 33.82%\n",
            "Epoch 218/1000, Batch Size: 64, Loss: 0.0169, Train Acc: 71.69%, Val Acc: 35.78%\n",
            "Epoch 219/1000, Batch Size: 64, Loss: 0.0165, Train Acc: 69.98%, Val Acc: 34.31%\n",
            "Epoch 220/1000, Batch Size: 64, Loss: 0.0177, Train Acc: 70.10%, Val Acc: 33.33%\n",
            "Epoch 221/1000, Batch Size: 64, Loss: 0.0165, Train Acc: 71.08%, Val Acc: 28.92%\n",
            "Epoch 222/1000, Batch Size: 64, Loss: 0.0164, Train Acc: 73.65%, Val Acc: 30.88%\n",
            "Epoch 223/1000, Batch Size: 64, Loss: 0.0166, Train Acc: 72.79%, Val Acc: 32.84%\n",
            "Epoch 224/1000, Batch Size: 64, Loss: 0.0145, Train Acc: 74.88%, Val Acc: 33.33%\n",
            "Epoch 225/1000, Batch Size: 64, Loss: 0.0157, Train Acc: 74.88%, Val Acc: 33.33%\n",
            "Epoch 226/1000, Batch Size: 64, Loss: 0.0164, Train Acc: 72.43%, Val Acc: 34.80%\n",
            "Epoch 227/1000, Batch Size: 64, Loss: 0.0162, Train Acc: 73.65%, Val Acc: 34.31%\n",
            "Epoch 228/1000, Batch Size: 64, Loss: 0.0151, Train Acc: 75.49%, Val Acc: 30.88%\n",
            "Epoch 229/1000, Batch Size: 64, Loss: 0.0142, Train Acc: 77.21%, Val Acc: 35.78%\n",
            "Epoch 230/1000, Batch Size: 64, Loss: 0.0166, Train Acc: 69.73%, Val Acc: 36.76%\n",
            "Epoch 231/1000, Batch Size: 64, Loss: 0.0151, Train Acc: 75.61%, Val Acc: 32.35%\n",
            "Epoch 232/1000, Batch Size: 64, Loss: 0.0148, Train Acc: 74.14%, Val Acc: 33.82%\n",
            "Epoch 233/1000, Batch Size: 64, Loss: 0.0149, Train Acc: 74.26%, Val Acc: 34.80%\n",
            "Epoch 234/1000, Batch Size: 64, Loss: 0.0146, Train Acc: 75.74%, Val Acc: 32.84%\n",
            "Epoch 235/1000, Batch Size: 64, Loss: 0.0166, Train Acc: 70.71%, Val Acc: 33.82%\n",
            "Epoch 236/1000, Batch Size: 64, Loss: 0.0164, Train Acc: 71.94%, Val Acc: 34.80%\n",
            "Epoch 237/1000, Batch Size: 64, Loss: 0.0154, Train Acc: 72.55%, Val Acc: 37.75%\n",
            "Epoch 238/1000, Batch Size: 64, Loss: 0.0158, Train Acc: 70.71%, Val Acc: 37.75%\n",
            "Epoch 239/1000, Batch Size: 64, Loss: 0.0160, Train Acc: 74.63%, Val Acc: 34.80%\n",
            "Epoch 240/1000, Batch Size: 64, Loss: 0.0163, Train Acc: 71.45%, Val Acc: 32.35%\n",
            "Epoch 241/1000, Batch Size: 64, Loss: 0.0153, Train Acc: 72.55%, Val Acc: 34.31%\n",
            "Epoch 242/1000, Batch Size: 64, Loss: 0.0144, Train Acc: 77.21%, Val Acc: 34.80%\n",
            "Epoch 243/1000, Batch Size: 64, Loss: 0.0138, Train Acc: 75.86%, Val Acc: 40.69%\n",
            "Epoch 244/1000, Batch Size: 64, Loss: 0.0136, Train Acc: 77.45%, Val Acc: 38.24%\n",
            "Epoch 245/1000, Batch Size: 64, Loss: 0.0146, Train Acc: 75.86%, Val Acc: 32.84%\n",
            "Epoch 246/1000, Batch Size: 64, Loss: 0.0154, Train Acc: 73.65%, Val Acc: 28.92%\n",
            "Epoch 247/1000, Batch Size: 64, Loss: 0.0142, Train Acc: 75.61%, Val Acc: 33.33%\n",
            "Epoch 248/1000, Batch Size: 64, Loss: 0.0149, Train Acc: 76.23%, Val Acc: 35.78%\n",
            "Epoch 249/1000, Batch Size: 64, Loss: 0.0141, Train Acc: 76.10%, Val Acc: 34.31%\n",
            "Epoch 250/1000, Batch Size: 64, Loss: 0.0141, Train Acc: 75.37%, Val Acc: 34.31%\n",
            "Epoch 251/1000, Batch Size: 64, Loss: 0.0131, Train Acc: 77.45%, Val Acc: 30.88%\n",
            "Epoch 252/1000, Batch Size: 64, Loss: 0.0125, Train Acc: 79.17%, Val Acc: 34.80%\n",
            "Epoch 253/1000, Batch Size: 64, Loss: 0.0136, Train Acc: 77.08%, Val Acc: 33.33%\n",
            "Epoch 254/1000, Batch Size: 64, Loss: 0.0142, Train Acc: 77.33%, Val Acc: 34.80%\n",
            "Epoch 255/1000, Batch Size: 64, Loss: 0.0132, Train Acc: 77.82%, Val Acc: 35.29%\n",
            "Epoch 256/1000, Batch Size: 64, Loss: 0.0137, Train Acc: 75.98%, Val Acc: 37.25%\n",
            "Epoch 257/1000, Batch Size: 64, Loss: 0.0146, Train Acc: 75.25%, Val Acc: 34.31%\n",
            "Epoch 258/1000, Batch Size: 64, Loss: 0.0138, Train Acc: 76.10%, Val Acc: 34.80%\n",
            "Epoch 259/1000, Batch Size: 64, Loss: 0.0143, Train Acc: 74.88%, Val Acc: 33.82%\n",
            "Epoch 260/1000, Batch Size: 64, Loss: 0.0133, Train Acc: 77.33%, Val Acc: 35.29%\n",
            "Epoch 261/1000, Batch Size: 64, Loss: 0.0148, Train Acc: 75.00%, Val Acc: 35.78%\n",
            "Epoch 262/1000, Batch Size: 64, Loss: 0.0151, Train Acc: 75.00%, Val Acc: 31.37%\n",
            "Epoch 263/1000, Batch Size: 64, Loss: 0.0128, Train Acc: 77.70%, Val Acc: 37.75%\n",
            "Epoch 264/1000, Batch Size: 64, Loss: 0.0135, Train Acc: 77.82%, Val Acc: 36.27%\n",
            "Epoch 265/1000, Batch Size: 64, Loss: 0.0142, Train Acc: 74.88%, Val Acc: 37.25%\n",
            "Epoch 266/1000, Batch Size: 64, Loss: 0.0128, Train Acc: 79.90%, Val Acc: 37.25%\n",
            "Epoch 267/1000, Batch Size: 64, Loss: 0.0120, Train Acc: 78.31%, Val Acc: 35.29%\n",
            "Epoch 268/1000, Batch Size: 64, Loss: 0.0130, Train Acc: 78.31%, Val Acc: 35.78%\n",
            "Epoch 269/1000, Batch Size: 64, Loss: 0.0135, Train Acc: 75.86%, Val Acc: 35.29%\n",
            "Epoch 270/1000, Batch Size: 64, Loss: 0.0136, Train Acc: 77.45%, Val Acc: 34.80%\n",
            "Epoch 271/1000, Batch Size: 64, Loss: 0.0136, Train Acc: 77.57%, Val Acc: 36.27%\n",
            "Epoch 272/1000, Batch Size: 64, Loss: 0.0124, Train Acc: 79.53%, Val Acc: 31.86%\n",
            "Epoch 273/1000, Batch Size: 64, Loss: 0.0130, Train Acc: 77.94%, Val Acc: 33.33%\n",
            "Epoch 274/1000, Batch Size: 64, Loss: 0.0123, Train Acc: 79.04%, Val Acc: 36.27%\n",
            "Epoch 275/1000, Batch Size: 64, Loss: 0.0136, Train Acc: 78.55%, Val Acc: 32.35%\n",
            "Epoch 276/1000, Batch Size: 64, Loss: 0.0131, Train Acc: 77.45%, Val Acc: 40.20%\n",
            "Epoch 277/1000, Batch Size: 64, Loss: 0.0118, Train Acc: 78.92%, Val Acc: 42.65%\n",
            "Epoch 278/1000, Batch Size: 64, Loss: 0.0133, Train Acc: 78.43%, Val Acc: 38.24%\n",
            "Epoch 279/1000, Batch Size: 64, Loss: 0.0110, Train Acc: 81.99%, Val Acc: 34.80%\n",
            "Epoch 280/1000, Batch Size: 64, Loss: 0.0125, Train Acc: 77.82%, Val Acc: 36.76%\n",
            "Epoch 281/1000, Batch Size: 64, Loss: 0.0135, Train Acc: 76.96%, Val Acc: 33.33%\n",
            "Epoch 282/1000, Batch Size: 64, Loss: 0.0134, Train Acc: 78.06%, Val Acc: 38.73%\n",
            "Epoch 283/1000, Batch Size: 64, Loss: 0.0122, Train Acc: 80.39%, Val Acc: 36.27%\n",
            "Epoch 284/1000, Batch Size: 64, Loss: 0.0134, Train Acc: 77.33%, Val Acc: 31.37%\n",
            "Epoch 285/1000, Batch Size: 64, Loss: 0.0118, Train Acc: 79.29%, Val Acc: 35.78%\n",
            "Epoch 286/1000, Batch Size: 64, Loss: 0.0118, Train Acc: 80.64%, Val Acc: 35.29%\n",
            "Epoch 287/1000, Batch Size: 64, Loss: 0.0129, Train Acc: 78.92%, Val Acc: 39.22%\n",
            "Epoch 288/1000, Batch Size: 64, Loss: 0.0115, Train Acc: 81.00%, Val Acc: 38.73%\n",
            "Epoch 289/1000, Batch Size: 64, Loss: 0.0125, Train Acc: 78.92%, Val Acc: 37.75%\n",
            "Epoch 290/1000, Batch Size: 64, Loss: 0.0125, Train Acc: 78.06%, Val Acc: 36.27%\n",
            "Epoch 291/1000, Batch Size: 64, Loss: 0.0130, Train Acc: 78.31%, Val Acc: 38.73%\n",
            "Epoch 292/1000, Batch Size: 64, Loss: 0.0117, Train Acc: 79.78%, Val Acc: 36.76%\n",
            "Epoch 293/1000, Batch Size: 64, Loss: 0.0107, Train Acc: 81.25%, Val Acc: 37.75%\n",
            "Epoch 294/1000, Batch Size: 64, Loss: 0.0126, Train Acc: 79.17%, Val Acc: 40.20%\n",
            "Epoch 295/1000, Batch Size: 64, Loss: 0.0106, Train Acc: 81.99%, Val Acc: 35.78%\n",
            "Epoch 296/1000, Batch Size: 64, Loss: 0.0118, Train Acc: 80.15%, Val Acc: 35.78%\n",
            "Epoch 297/1000, Batch Size: 64, Loss: 0.0124, Train Acc: 79.66%, Val Acc: 32.35%\n",
            "Epoch 298/1000, Batch Size: 64, Loss: 0.0123, Train Acc: 77.45%, Val Acc: 35.78%\n",
            "Epoch 299/1000, Batch Size: 64, Loss: 0.0128, Train Acc: 80.02%, Val Acc: 35.78%\n",
            "Epoch 300/1000, Batch Size: 64, Loss: 0.0113, Train Acc: 80.76%, Val Acc: 40.69%\n",
            "Epoch 301/1000, Batch Size: 64, Loss: 0.0131, Train Acc: 77.70%, Val Acc: 38.73%\n",
            "Epoch 302/1000, Batch Size: 64, Loss: 0.0108, Train Acc: 82.11%, Val Acc: 37.25%\n",
            "Epoch 303/1000, Batch Size: 64, Loss: 0.0119, Train Acc: 79.53%, Val Acc: 38.24%\n",
            "Epoch 304/1000, Batch Size: 64, Loss: 0.0115, Train Acc: 81.50%, Val Acc: 40.69%\n",
            "Epoch 305/1000, Batch Size: 64, Loss: 0.0107, Train Acc: 81.99%, Val Acc: 33.82%\n",
            "Epoch 306/1000, Batch Size: 64, Loss: 0.0112, Train Acc: 82.23%, Val Acc: 38.24%\n",
            "Epoch 307/1000, Batch Size: 64, Loss: 0.0113, Train Acc: 81.00%, Val Acc: 36.27%\n",
            "Epoch 308/1000, Batch Size: 64, Loss: 0.0116, Train Acc: 80.64%, Val Acc: 39.22%\n",
            "Epoch 309/1000, Batch Size: 64, Loss: 0.0144, Train Acc: 78.68%, Val Acc: 40.20%\n",
            "Epoch 310/1000, Batch Size: 64, Loss: 0.0119, Train Acc: 78.31%, Val Acc: 34.80%\n",
            "Epoch 311/1000, Batch Size: 64, Loss: 0.0114, Train Acc: 80.39%, Val Acc: 38.24%\n",
            "Epoch 312/1000, Batch Size: 64, Loss: 0.0117, Train Acc: 79.78%, Val Acc: 34.80%\n",
            "Epoch 313/1000, Batch Size: 64, Loss: 0.0106, Train Acc: 81.37%, Val Acc: 40.20%\n",
            "Epoch 314/1000, Batch Size: 64, Loss: 0.0101, Train Acc: 84.07%, Val Acc: 40.20%\n",
            "Epoch 315/1000, Batch Size: 64, Loss: 0.0118, Train Acc: 79.66%, Val Acc: 36.27%\n",
            "Epoch 316/1000, Batch Size: 64, Loss: 0.0119, Train Acc: 81.00%, Val Acc: 40.69%\n",
            "Epoch 317/1000, Batch Size: 64, Loss: 0.0115, Train Acc: 80.27%, Val Acc: 41.18%\n",
            "Epoch 318/1000, Batch Size: 64, Loss: 0.0112, Train Acc: 81.37%, Val Acc: 38.73%\n",
            "Epoch 319/1000, Batch Size: 64, Loss: 0.0099, Train Acc: 82.60%, Val Acc: 39.22%\n",
            "Epoch 320/1000, Batch Size: 64, Loss: 0.0109, Train Acc: 82.23%, Val Acc: 42.65%\n",
            "Epoch 321/1000, Batch Size: 64, Loss: 0.0111, Train Acc: 82.72%, Val Acc: 38.24%\n",
            "Epoch 322/1000, Batch Size: 64, Loss: 0.0112, Train Acc: 81.00%, Val Acc: 40.69%\n",
            "Epoch 323/1000, Batch Size: 64, Loss: 0.0118, Train Acc: 81.25%, Val Acc: 37.25%\n",
            "Epoch 324/1000, Batch Size: 64, Loss: 0.0110, Train Acc: 81.37%, Val Acc: 33.82%\n",
            "Epoch 325/1000, Batch Size: 64, Loss: 0.0103, Train Acc: 82.72%, Val Acc: 35.78%\n",
            "Epoch 326/1000, Batch Size: 64, Loss: 0.0113, Train Acc: 81.62%, Val Acc: 38.24%\n",
            "Epoch 327/1000, Batch Size: 64, Loss: 0.0106, Train Acc: 83.33%, Val Acc: 36.27%\n",
            "Epoch 328/1000, Batch Size: 64, Loss: 0.0121, Train Acc: 80.39%, Val Acc: 35.78%\n",
            "Epoch 329/1000, Batch Size: 64, Loss: 0.0100, Train Acc: 83.46%, Val Acc: 41.67%\n",
            "Epoch 330/1000, Batch Size: 64, Loss: 0.0104, Train Acc: 81.74%, Val Acc: 38.24%\n",
            "Epoch 331/1000, Batch Size: 64, Loss: 0.0098, Train Acc: 83.70%, Val Acc: 31.37%\n",
            "Epoch 332/1000, Batch Size: 64, Loss: 0.0107, Train Acc: 83.58%, Val Acc: 35.78%\n",
            "Epoch 333/1000, Batch Size: 64, Loss: 0.0098, Train Acc: 83.46%, Val Acc: 34.80%\n",
            "Epoch 334/1000, Batch Size: 64, Loss: 0.0099, Train Acc: 83.58%, Val Acc: 39.71%\n",
            "Epoch 335/1000, Batch Size: 64, Loss: 0.0094, Train Acc: 84.68%, Val Acc: 39.71%\n",
            "Epoch 336/1000, Batch Size: 64, Loss: 0.0102, Train Acc: 84.07%, Val Acc: 38.24%\n",
            "Epoch 337/1000, Batch Size: 64, Loss: 0.0109, Train Acc: 82.35%, Val Acc: 39.71%\n",
            "Epoch 338/1000, Batch Size: 64, Loss: 0.0102, Train Acc: 81.99%, Val Acc: 34.31%\n",
            "Epoch 339/1000, Batch Size: 64, Loss: 0.0113, Train Acc: 81.37%, Val Acc: 36.27%\n",
            "Epoch 340/1000, Batch Size: 64, Loss: 0.0106, Train Acc: 81.74%, Val Acc: 42.16%\n",
            "Epoch 341/1000, Batch Size: 64, Loss: 0.0096, Train Acc: 82.84%, Val Acc: 35.29%\n",
            "Epoch 342/1000, Batch Size: 64, Loss: 0.0096, Train Acc: 83.95%, Val Acc: 34.80%\n",
            "Epoch 343/1000, Batch Size: 64, Loss: 0.0101, Train Acc: 82.48%, Val Acc: 39.22%\n",
            "Epoch 344/1000, Batch Size: 64, Loss: 0.0103, Train Acc: 81.86%, Val Acc: 39.22%\n",
            "Epoch 345/1000, Batch Size: 64, Loss: 0.0101, Train Acc: 83.95%, Val Acc: 39.22%\n",
            "Epoch 346/1000, Batch Size: 64, Loss: 0.0095, Train Acc: 83.21%, Val Acc: 44.61%\n",
            "Epoch 347/1000, Batch Size: 64, Loss: 0.0102, Train Acc: 84.44%, Val Acc: 40.20%\n",
            "Epoch 348/1000, Batch Size: 64, Loss: 0.0100, Train Acc: 83.33%, Val Acc: 40.69%\n",
            "Epoch 349/1000, Batch Size: 64, Loss: 0.0095, Train Acc: 84.80%, Val Acc: 39.22%\n",
            "Epoch 350/1000, Batch Size: 64, Loss: 0.0109, Train Acc: 81.74%, Val Acc: 36.76%\n",
            "Epoch 351/1000, Batch Size: 64, Loss: 0.0095, Train Acc: 83.46%, Val Acc: 37.25%\n",
            "Epoch 352/1000, Batch Size: 64, Loss: 0.0100, Train Acc: 82.97%, Val Acc: 36.76%\n",
            "Epoch 353/1000, Batch Size: 64, Loss: 0.0106, Train Acc: 82.60%, Val Acc: 37.75%\n",
            "Epoch 354/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 86.40%, Val Acc: 36.76%\n",
            "Epoch 355/1000, Batch Size: 64, Loss: 0.0091, Train Acc: 85.05%, Val Acc: 42.16%\n",
            "Epoch 356/1000, Batch Size: 64, Loss: 0.0103, Train Acc: 81.99%, Val Acc: 35.78%\n",
            "Epoch 357/1000, Batch Size: 64, Loss: 0.0107, Train Acc: 81.50%, Val Acc: 33.82%\n",
            "Epoch 358/1000, Batch Size: 64, Loss: 0.0098, Train Acc: 84.44%, Val Acc: 40.20%\n",
            "Epoch 359/1000, Batch Size: 64, Loss: 0.0091, Train Acc: 86.15%, Val Acc: 37.25%\n",
            "Epoch 360/1000, Batch Size: 64, Loss: 0.0100, Train Acc: 84.19%, Val Acc: 38.73%\n",
            "Epoch 361/1000, Batch Size: 64, Loss: 0.0107, Train Acc: 82.84%, Val Acc: 38.24%\n",
            "Epoch 362/1000, Batch Size: 64, Loss: 0.0101, Train Acc: 83.21%, Val Acc: 35.78%\n",
            "Epoch 363/1000, Batch Size: 64, Loss: 0.0109, Train Acc: 83.58%, Val Acc: 39.22%\n",
            "Epoch 364/1000, Batch Size: 64, Loss: 0.0095, Train Acc: 83.09%, Val Acc: 41.18%\n",
            "Epoch 365/1000, Batch Size: 64, Loss: 0.0100, Train Acc: 83.09%, Val Acc: 38.73%\n",
            "Epoch 366/1000, Batch Size: 64, Loss: 0.0098, Train Acc: 83.58%, Val Acc: 39.71%\n",
            "Epoch 367/1000, Batch Size: 64, Loss: 0.0091, Train Acc: 85.05%, Val Acc: 35.29%\n",
            "Epoch 368/1000, Batch Size: 64, Loss: 0.0101, Train Acc: 82.60%, Val Acc: 39.71%\n",
            "Epoch 369/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.89%, Val Acc: 41.18%\n",
            "Epoch 370/1000, Batch Size: 64, Loss: 0.0108, Train Acc: 82.48%, Val Acc: 39.71%\n",
            "Epoch 371/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 85.05%, Val Acc: 40.69%\n",
            "Epoch 372/1000, Batch Size: 64, Loss: 0.0101, Train Acc: 82.97%, Val Acc: 40.69%\n",
            "Epoch 373/1000, Batch Size: 64, Loss: 0.0103, Train Acc: 82.60%, Val Acc: 43.63%\n",
            "Epoch 374/1000, Batch Size: 64, Loss: 0.0099, Train Acc: 82.72%, Val Acc: 45.10%\n",
            "Epoch 375/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 85.78%, Val Acc: 40.69%\n",
            "Epoch 376/1000, Batch Size: 64, Loss: 0.0091, Train Acc: 85.05%, Val Acc: 39.22%\n",
            "Epoch 377/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.40%, Val Acc: 37.75%\n",
            "Epoch 378/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 84.19%, Val Acc: 41.67%\n",
            "Epoch 379/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 86.52%, Val Acc: 36.76%\n",
            "Epoch 380/1000, Batch Size: 64, Loss: 0.0093, Train Acc: 85.42%, Val Acc: 43.63%\n",
            "Epoch 381/1000, Batch Size: 64, Loss: 0.0090, Train Acc: 85.78%, Val Acc: 33.82%\n",
            "Epoch 382/1000, Batch Size: 64, Loss: 0.0096, Train Acc: 84.31%, Val Acc: 42.65%\n",
            "Epoch 383/1000, Batch Size: 64, Loss: 0.0094, Train Acc: 84.56%, Val Acc: 38.73%\n",
            "Epoch 384/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 85.42%, Val Acc: 39.71%\n",
            "Epoch 385/1000, Batch Size: 64, Loss: 0.0093, Train Acc: 86.15%, Val Acc: 39.71%\n",
            "Epoch 386/1000, Batch Size: 64, Loss: 0.0091, Train Acc: 84.93%, Val Acc: 40.20%\n",
            "Epoch 387/1000, Batch Size: 64, Loss: 0.0093, Train Acc: 84.19%, Val Acc: 38.24%\n",
            "Epoch 388/1000, Batch Size: 64, Loss: 0.0098, Train Acc: 84.07%, Val Acc: 38.24%\n",
            "Epoch 389/1000, Batch Size: 64, Loss: 0.0094, Train Acc: 84.31%, Val Acc: 37.25%\n",
            "Epoch 390/1000, Batch Size: 64, Loss: 0.0090, Train Acc: 84.80%, Val Acc: 35.78%\n",
            "Epoch 391/1000, Batch Size: 64, Loss: 0.0090, Train Acc: 84.80%, Val Acc: 36.76%\n",
            "Epoch 392/1000, Batch Size: 64, Loss: 0.0090, Train Acc: 85.78%, Val Acc: 41.18%\n",
            "Epoch 393/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 84.31%, Val Acc: 41.67%\n",
            "Epoch 394/1000, Batch Size: 64, Loss: 0.0095, Train Acc: 84.07%, Val Acc: 45.10%\n",
            "Epoch 395/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.50%, Val Acc: 45.59%\n",
            "Epoch 396/1000, Batch Size: 64, Loss: 0.0101, Train Acc: 83.21%, Val Acc: 41.18%\n",
            "Epoch 397/1000, Batch Size: 64, Loss: 0.0092, Train Acc: 85.05%, Val Acc: 37.75%\n",
            "Epoch 398/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 84.68%, Val Acc: 38.73%\n",
            "Epoch 399/1000, Batch Size: 64, Loss: 0.0092, Train Acc: 85.66%, Val Acc: 45.10%\n",
            "Epoch 400/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 86.64%, Val Acc: 37.25%\n",
            "Epoch 401/1000, Batch Size: 64, Loss: 0.0103, Train Acc: 85.29%, Val Acc: 41.18%\n",
            "Epoch 402/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 87.01%, Val Acc: 38.24%\n",
            "Epoch 403/1000, Batch Size: 64, Loss: 0.0098, Train Acc: 84.44%, Val Acc: 38.73%\n",
            "Epoch 404/1000, Batch Size: 64, Loss: 0.0099, Train Acc: 82.97%, Val Acc: 37.25%\n",
            "Epoch 405/1000, Batch Size: 64, Loss: 0.0097, Train Acc: 83.46%, Val Acc: 36.76%\n",
            "Epoch 406/1000, Batch Size: 64, Loss: 0.0094, Train Acc: 85.29%, Val Acc: 38.24%\n",
            "Epoch 407/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 86.27%, Val Acc: 39.71%\n",
            "Epoch 408/1000, Batch Size: 64, Loss: 0.0093, Train Acc: 85.42%, Val Acc: 37.25%\n",
            "Epoch 409/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.64%, Val Acc: 40.20%\n",
            "Epoch 410/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 85.91%, Val Acc: 38.24%\n",
            "Epoch 411/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 84.80%, Val Acc: 41.67%\n",
            "Epoch 412/1000, Batch Size: 64, Loss: 0.0090, Train Acc: 85.29%, Val Acc: 37.75%\n",
            "Epoch 413/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 84.56%, Val Acc: 34.31%\n",
            "Epoch 414/1000, Batch Size: 64, Loss: 0.0095, Train Acc: 84.68%, Val Acc: 40.20%\n",
            "Epoch 415/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 87.75%, Val Acc: 44.61%\n",
            "Epoch 416/1000, Batch Size: 64, Loss: 0.0092, Train Acc: 86.64%, Val Acc: 39.71%\n",
            "Epoch 417/1000, Batch Size: 64, Loss: 0.0091, Train Acc: 84.31%, Val Acc: 36.27%\n",
            "Epoch 418/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 85.66%, Val Acc: 38.24%\n",
            "Epoch 419/1000, Batch Size: 64, Loss: 0.0091, Train Acc: 83.95%, Val Acc: 43.14%\n",
            "Epoch 420/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 86.40%, Val Acc: 42.16%\n",
            "Epoch 421/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 85.42%, Val Acc: 40.20%\n",
            "Epoch 422/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 85.66%, Val Acc: 39.22%\n",
            "Epoch 423/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 86.89%, Val Acc: 39.71%\n",
            "Epoch 424/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 87.75%, Val Acc: 41.67%\n",
            "Epoch 425/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 85.17%, Val Acc: 38.24%\n",
            "Epoch 426/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 86.76%, Val Acc: 41.18%\n",
            "Epoch 427/1000, Batch Size: 64, Loss: 0.0096, Train Acc: 85.29%, Val Acc: 39.22%\n",
            "Epoch 428/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.40%, Val Acc: 40.69%\n",
            "Epoch 429/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.89%, Val Acc: 41.67%\n",
            "Epoch 430/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 86.64%, Val Acc: 40.20%\n",
            "Epoch 431/1000, Batch Size: 64, Loss: 0.0091, Train Acc: 85.54%, Val Acc: 40.69%\n",
            "Epoch 432/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 87.13%, Val Acc: 40.69%\n",
            "Epoch 433/1000, Batch Size: 64, Loss: 0.0090, Train Acc: 86.52%, Val Acc: 41.18%\n",
            "Epoch 434/1000, Batch Size: 64, Loss: 0.0094, Train Acc: 84.80%, Val Acc: 41.18%\n",
            "Epoch 435/1000, Batch Size: 64, Loss: 0.0092, Train Acc: 84.19%, Val Acc: 42.16%\n",
            "Epoch 436/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 85.54%, Val Acc: 43.63%\n",
            "Epoch 437/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 86.27%, Val Acc: 44.61%\n",
            "Epoch 438/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.99%, Val Acc: 46.57%\n",
            "Epoch 439/1000, Batch Size: 64, Loss: 0.0090, Train Acc: 84.80%, Val Acc: 48.53%\n",
            "Epoch 440/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.11%, Val Acc: 39.71%\n",
            "Epoch 441/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 86.52%, Val Acc: 39.22%\n",
            "Epoch 442/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 85.29%, Val Acc: 40.69%\n",
            "Epoch 443/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 85.91%, Val Acc: 39.22%\n",
            "Epoch 444/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 86.40%, Val Acc: 41.18%\n",
            "Epoch 445/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 86.03%, Val Acc: 42.65%\n",
            "Epoch 446/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 85.78%, Val Acc: 41.18%\n",
            "Epoch 447/1000, Batch Size: 64, Loss: 0.0090, Train Acc: 85.66%, Val Acc: 41.18%\n",
            "Epoch 448/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 88.24%, Val Acc: 43.63%\n",
            "Epoch 449/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 86.03%, Val Acc: 40.20%\n",
            "Epoch 450/1000, Batch Size: 64, Loss: 0.0099, Train Acc: 84.19%, Val Acc: 40.20%\n",
            "Epoch 451/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 84.44%, Val Acc: 42.65%\n",
            "Epoch 452/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.27%, Val Acc: 41.18%\n",
            "Epoch 453/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 86.40%, Val Acc: 40.69%\n",
            "Epoch 454/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 86.27%, Val Acc: 39.71%\n",
            "Epoch 455/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.75%, Val Acc: 38.73%\n",
            "Epoch 456/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 87.01%, Val Acc: 39.71%\n",
            "Epoch 457/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 85.66%, Val Acc: 39.22%\n",
            "Epoch 458/1000, Batch Size: 64, Loss: 0.0090, Train Acc: 85.66%, Val Acc: 39.71%\n",
            "Epoch 459/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 85.42%, Val Acc: 42.16%\n",
            "Epoch 460/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 87.25%, Val Acc: 41.18%\n",
            "Epoch 461/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.75%, Val Acc: 42.16%\n",
            "Epoch 462/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 88.36%, Val Acc: 43.14%\n",
            "Epoch 463/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 86.27%, Val Acc: 40.69%\n",
            "Epoch 464/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 87.01%, Val Acc: 39.71%\n",
            "Epoch 465/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 86.52%, Val Acc: 38.24%\n",
            "Epoch 466/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 86.89%, Val Acc: 45.10%\n",
            "Epoch 467/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.03%, Val Acc: 42.65%\n",
            "Epoch 468/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 88.36%, Val Acc: 39.22%\n",
            "Epoch 469/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 85.05%, Val Acc: 40.69%\n",
            "Epoch 470/1000, Batch Size: 64, Loss: 0.0071, Train Acc: 89.95%, Val Acc: 43.14%\n",
            "Epoch 471/1000, Batch Size: 64, Loss: 0.0092, Train Acc: 85.42%, Val Acc: 39.71%\n",
            "Epoch 472/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.99%, Val Acc: 37.75%\n",
            "Epoch 473/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 86.40%, Val Acc: 41.18%\n",
            "Epoch 474/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 85.42%, Val Acc: 37.75%\n",
            "Epoch 475/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.13%, Val Acc: 37.75%\n",
            "Epoch 476/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 86.52%, Val Acc: 39.71%\n",
            "Epoch 477/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 86.15%, Val Acc: 42.65%\n",
            "Epoch 478/1000, Batch Size: 64, Loss: 0.0093, Train Acc: 85.29%, Val Acc: 33.82%\n",
            "Epoch 479/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 88.24%, Val Acc: 36.76%\n",
            "Epoch 480/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 85.05%, Val Acc: 43.63%\n",
            "Epoch 481/1000, Batch Size: 64, Loss: 0.0092, Train Acc: 86.15%, Val Acc: 41.18%\n",
            "Epoch 482/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 84.68%, Val Acc: 39.22%\n",
            "Epoch 483/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 85.91%, Val Acc: 43.14%\n",
            "Epoch 484/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 86.27%, Val Acc: 41.67%\n",
            "Epoch 485/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 87.13%, Val Acc: 43.63%\n",
            "Epoch 486/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 84.31%, Val Acc: 37.75%\n",
            "Epoch 487/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 87.99%, Val Acc: 43.14%\n",
            "Epoch 488/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.52%, Val Acc: 37.75%\n",
            "Epoch 489/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 87.13%, Val Acc: 41.67%\n",
            "Epoch 490/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 85.29%, Val Acc: 39.71%\n",
            "Epoch 491/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 87.50%, Val Acc: 40.69%\n",
            "Epoch 492/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 89.22%, Val Acc: 44.12%\n",
            "Epoch 493/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 86.52%, Val Acc: 44.12%\n",
            "Epoch 494/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 88.11%, Val Acc: 45.10%\n",
            "Epoch 495/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 87.62%, Val Acc: 39.22%\n",
            "Epoch 496/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.99%, Val Acc: 43.14%\n",
            "Epoch 497/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 85.29%, Val Acc: 38.24%\n",
            "Epoch 498/1000, Batch Size: 64, Loss: 0.0067, Train Acc: 89.71%, Val Acc: 39.22%\n",
            "Epoch 499/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 87.13%, Val Acc: 40.20%\n",
            "Epoch 500/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 86.40%, Val Acc: 42.16%\n",
            "Epoch 501/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 86.52%, Val Acc: 42.16%\n",
            "Epoch 502/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.38%, Val Acc: 47.06%\n",
            "Epoch 503/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.64%, Val Acc: 44.61%\n",
            "Epoch 504/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.87%, Val Acc: 40.69%\n",
            "Epoch 505/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 88.60%, Val Acc: 41.67%\n",
            "Epoch 506/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.62%, Val Acc: 39.71%\n",
            "Epoch 507/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 87.01%, Val Acc: 39.22%\n",
            "Epoch 508/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 85.91%, Val Acc: 39.71%\n",
            "Epoch 509/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.36%, Val Acc: 39.22%\n",
            "Epoch 510/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.03%, Val Acc: 38.73%\n",
            "Epoch 511/1000, Batch Size: 64, Loss: 0.0068, Train Acc: 89.58%, Val Acc: 42.16%\n",
            "Epoch 512/1000, Batch Size: 64, Loss: 0.0090, Train Acc: 84.80%, Val Acc: 38.24%\n",
            "Epoch 513/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.03%, Val Acc: 41.67%\n",
            "Epoch 514/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.38%, Val Acc: 39.71%\n",
            "Epoch 515/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 87.25%, Val Acc: 44.12%\n",
            "Epoch 516/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.89%, Val Acc: 34.80%\n",
            "Epoch 517/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 85.66%, Val Acc: 44.61%\n",
            "Epoch 518/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.64%, Val Acc: 41.67%\n",
            "Epoch 519/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.50%, Val Acc: 42.16%\n",
            "Epoch 520/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 88.24%, Val Acc: 48.53%\n",
            "Epoch 521/1000, Batch Size: 64, Loss: 0.0070, Train Acc: 88.85%, Val Acc: 38.24%\n",
            "Epoch 522/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.15%, Val Acc: 45.59%\n",
            "Epoch 523/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.87%, Val Acc: 41.18%\n",
            "Epoch 524/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 87.62%, Val Acc: 40.69%\n",
            "Epoch 525/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.50%, Val Acc: 42.16%\n",
            "Epoch 526/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 84.93%, Val Acc: 39.22%\n",
            "Epoch 527/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 85.17%, Val Acc: 44.61%\n",
            "Epoch 528/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 84.44%, Val Acc: 39.22%\n",
            "Epoch 529/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.89%, Val Acc: 38.24%\n",
            "Epoch 530/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 87.01%, Val Acc: 40.20%\n",
            "Epoch 531/1000, Batch Size: 64, Loss: 0.0094, Train Acc: 84.31%, Val Acc: 41.67%\n",
            "Epoch 532/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.15%, Val Acc: 40.69%\n",
            "Epoch 533/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 87.25%, Val Acc: 38.73%\n",
            "Epoch 534/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.76%, Val Acc: 39.71%\n",
            "Epoch 535/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 85.29%, Val Acc: 40.20%\n",
            "Epoch 536/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 87.38%, Val Acc: 38.73%\n",
            "Epoch 537/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 86.40%, Val Acc: 42.65%\n",
            "Epoch 538/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 87.87%, Val Acc: 43.63%\n",
            "Epoch 539/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.13%, Val Acc: 38.24%\n",
            "Epoch 540/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 87.62%, Val Acc: 43.63%\n",
            "Epoch 541/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.64%, Val Acc: 46.08%\n",
            "Epoch 542/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 86.27%, Val Acc: 41.67%\n",
            "Epoch 543/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 87.62%, Val Acc: 37.25%\n",
            "Epoch 544/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 88.60%, Val Acc: 42.65%\n",
            "Epoch 545/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.25%, Val Acc: 39.22%\n",
            "Epoch 546/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 87.01%, Val Acc: 45.59%\n",
            "Epoch 547/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 87.13%, Val Acc: 40.20%\n",
            "Epoch 548/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 86.03%, Val Acc: 39.71%\n",
            "Epoch 549/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.64%, Val Acc: 42.65%\n",
            "Epoch 550/1000, Batch Size: 64, Loss: 0.0090, Train Acc: 85.17%, Val Acc: 43.14%\n",
            "Epoch 551/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.01%, Val Acc: 39.71%\n",
            "Epoch 552/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 85.66%, Val Acc: 41.18%\n",
            "Epoch 553/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 84.44%, Val Acc: 39.71%\n",
            "Epoch 554/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 88.24%, Val Acc: 42.16%\n",
            "Epoch 555/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 86.15%, Val Acc: 39.71%\n",
            "Epoch 556/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 89.34%, Val Acc: 40.20%\n",
            "Epoch 557/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 85.17%, Val Acc: 44.61%\n",
            "Epoch 558/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 86.40%, Val Acc: 42.16%\n",
            "Epoch 559/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.27%, Val Acc: 45.10%\n",
            "Epoch 560/1000, Batch Size: 64, Loss: 0.0093, Train Acc: 84.56%, Val Acc: 38.24%\n",
            "Epoch 561/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 87.99%, Val Acc: 36.76%\n",
            "Epoch 562/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 88.24%, Val Acc: 39.22%\n",
            "Epoch 563/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 87.99%, Val Acc: 44.61%\n",
            "Epoch 564/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 88.48%, Val Acc: 40.20%\n",
            "Epoch 565/1000, Batch Size: 64, Loss: 0.0065, Train Acc: 90.07%, Val Acc: 42.16%\n",
            "Epoch 566/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.50%, Val Acc: 39.22%\n",
            "Epoch 567/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 87.50%, Val Acc: 37.75%\n",
            "Epoch 568/1000, Batch Size: 64, Loss: 0.0090, Train Acc: 85.42%, Val Acc: 42.65%\n",
            "Epoch 569/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 86.64%, Val Acc: 41.67%\n",
            "Epoch 570/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 86.15%, Val Acc: 39.71%\n",
            "Epoch 571/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 85.17%, Val Acc: 37.75%\n",
            "Epoch 572/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 87.25%, Val Acc: 42.65%\n",
            "Epoch 573/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 86.27%, Val Acc: 38.73%\n",
            "Epoch 574/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 85.42%, Val Acc: 41.67%\n",
            "Epoch 575/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.38%, Val Acc: 45.59%\n",
            "Epoch 576/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.25%, Val Acc: 39.22%\n",
            "Epoch 577/1000, Batch Size: 64, Loss: 0.0069, Train Acc: 89.58%, Val Acc: 41.67%\n",
            "Epoch 578/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.99%, Val Acc: 36.27%\n",
            "Epoch 579/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.38%, Val Acc: 37.75%\n",
            "Epoch 580/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 86.27%, Val Acc: 41.18%\n",
            "Epoch 581/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.50%, Val Acc: 37.75%\n",
            "Epoch 582/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 86.03%, Val Acc: 44.61%\n",
            "Epoch 583/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 87.38%, Val Acc: 39.22%\n",
            "Epoch 584/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 87.01%, Val Acc: 41.67%\n",
            "Epoch 585/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 88.60%, Val Acc: 43.14%\n",
            "Epoch 586/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 86.40%, Val Acc: 39.71%\n",
            "Epoch 587/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.25%, Val Acc: 39.22%\n",
            "Epoch 588/1000, Batch Size: 64, Loss: 0.0070, Train Acc: 88.48%, Val Acc: 46.08%\n",
            "Epoch 589/1000, Batch Size: 64, Loss: 0.0093, Train Acc: 83.95%, Val Acc: 39.22%\n",
            "Epoch 590/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 85.17%, Val Acc: 39.22%\n",
            "Epoch 591/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.99%, Val Acc: 47.06%\n",
            "Epoch 592/1000, Batch Size: 64, Loss: 0.0069, Train Acc: 87.99%, Val Acc: 41.67%\n",
            "Epoch 593/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 84.68%, Val Acc: 43.63%\n",
            "Epoch 594/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 87.75%, Val Acc: 40.20%\n",
            "Epoch 595/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 86.40%, Val Acc: 37.25%\n",
            "Epoch 596/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.99%, Val Acc: 37.75%\n",
            "Epoch 597/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 86.89%, Val Acc: 38.24%\n",
            "Epoch 598/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.13%, Val Acc: 38.24%\n",
            "Epoch 599/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 87.13%, Val Acc: 39.22%\n",
            "Epoch 600/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 86.89%, Val Acc: 40.20%\n",
            "Epoch 601/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 87.75%, Val Acc: 41.18%\n",
            "Epoch 602/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 87.01%, Val Acc: 37.75%\n",
            "Epoch 603/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.48%, Val Acc: 41.18%\n",
            "Epoch 604/1000, Batch Size: 64, Loss: 0.0069, Train Acc: 89.71%, Val Acc: 41.18%\n",
            "Epoch 605/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.87%, Val Acc: 39.22%\n",
            "Epoch 606/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 86.52%, Val Acc: 44.12%\n",
            "Epoch 607/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 86.89%, Val Acc: 42.16%\n",
            "Epoch 608/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 88.36%, Val Acc: 39.71%\n",
            "Epoch 609/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.40%, Val Acc: 40.69%\n",
            "Epoch 610/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 88.24%, Val Acc: 40.20%\n",
            "Epoch 611/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 85.91%, Val Acc: 38.73%\n",
            "Epoch 612/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.01%, Val Acc: 36.76%\n",
            "Epoch 613/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.99%, Val Acc: 40.20%\n",
            "Epoch 614/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 88.85%, Val Acc: 41.67%\n",
            "Epoch 615/1000, Batch Size: 64, Loss: 0.0090, Train Acc: 85.66%, Val Acc: 38.24%\n",
            "Epoch 616/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 86.64%, Val Acc: 42.16%\n",
            "Epoch 617/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 89.58%, Val Acc: 40.69%\n",
            "Epoch 618/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.85%, Val Acc: 43.63%\n",
            "Epoch 619/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 85.17%, Val Acc: 40.20%\n",
            "Epoch 620/1000, Batch Size: 64, Loss: 0.0090, Train Acc: 85.17%, Val Acc: 42.65%\n",
            "Epoch 621/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.62%, Val Acc: 39.71%\n",
            "Epoch 622/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 88.85%, Val Acc: 42.16%\n",
            "Epoch 623/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 87.62%, Val Acc: 45.10%\n",
            "Epoch 624/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.62%, Val Acc: 39.22%\n",
            "Epoch 625/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.38%, Val Acc: 44.61%\n",
            "Epoch 626/1000, Batch Size: 64, Loss: 0.0068, Train Acc: 89.09%, Val Acc: 39.22%\n",
            "Epoch 627/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 89.34%, Val Acc: 44.12%\n",
            "Epoch 628/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 87.13%, Val Acc: 41.18%\n",
            "Epoch 629/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 88.48%, Val Acc: 37.25%\n",
            "Epoch 630/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.25%, Val Acc: 42.16%\n",
            "Epoch 631/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 88.85%, Val Acc: 42.16%\n",
            "Epoch 632/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.87%, Val Acc: 42.65%\n",
            "Epoch 633/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 87.62%, Val Acc: 44.12%\n",
            "Epoch 634/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.50%, Val Acc: 46.08%\n",
            "Epoch 635/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 85.54%, Val Acc: 48.53%\n",
            "Epoch 636/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 87.38%, Val Acc: 41.18%\n",
            "Epoch 637/1000, Batch Size: 64, Loss: 0.0069, Train Acc: 88.36%, Val Acc: 42.65%\n",
            "Epoch 638/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 87.13%, Val Acc: 42.65%\n",
            "Epoch 639/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 86.40%, Val Acc: 42.65%\n",
            "Epoch 640/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 87.75%, Val Acc: 44.61%\n",
            "Epoch 641/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 88.85%, Val Acc: 44.61%\n",
            "Epoch 642/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.52%, Val Acc: 45.10%\n",
            "Epoch 643/1000, Batch Size: 64, Loss: 0.0069, Train Acc: 89.83%, Val Acc: 43.63%\n",
            "Epoch 644/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.48%, Val Acc: 39.71%\n",
            "Epoch 645/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.75%, Val Acc: 42.16%\n",
            "Epoch 646/1000, Batch Size: 64, Loss: 0.0093, Train Acc: 85.91%, Val Acc: 41.18%\n",
            "Epoch 647/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 87.01%, Val Acc: 38.24%\n",
            "Epoch 648/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 87.62%, Val Acc: 39.71%\n",
            "Epoch 649/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 85.91%, Val Acc: 36.76%\n",
            "Epoch 650/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.38%, Val Acc: 40.69%\n",
            "Epoch 651/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 88.73%, Val Acc: 41.67%\n",
            "Epoch 652/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 88.24%, Val Acc: 40.69%\n",
            "Epoch 653/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 86.76%, Val Acc: 36.76%\n",
            "Epoch 654/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 88.60%, Val Acc: 47.06%\n",
            "Epoch 655/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.89%, Val Acc: 41.18%\n",
            "Epoch 656/1000, Batch Size: 64, Loss: 0.0068, Train Acc: 90.32%, Val Acc: 44.12%\n",
            "Epoch 657/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.01%, Val Acc: 39.22%\n",
            "Epoch 658/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.50%, Val Acc: 38.24%\n",
            "Epoch 659/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.38%, Val Acc: 44.12%\n",
            "Epoch 660/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 90.07%, Val Acc: 38.24%\n",
            "Epoch 661/1000, Batch Size: 64, Loss: 0.0071, Train Acc: 88.24%, Val Acc: 40.69%\n",
            "Epoch 662/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 86.76%, Val Acc: 39.22%\n",
            "Epoch 663/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 86.89%, Val Acc: 41.18%\n",
            "Epoch 664/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.62%, Val Acc: 39.71%\n",
            "Epoch 665/1000, Batch Size: 64, Loss: 0.0071, Train Acc: 87.38%, Val Acc: 42.65%\n",
            "Epoch 666/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 86.27%, Val Acc: 40.69%\n",
            "Epoch 667/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 85.54%, Val Acc: 38.24%\n",
            "Epoch 668/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.25%, Val Acc: 43.14%\n",
            "Epoch 669/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 84.80%, Val Acc: 38.73%\n",
            "Epoch 670/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 85.66%, Val Acc: 41.67%\n",
            "Epoch 671/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 86.03%, Val Acc: 39.71%\n",
            "Epoch 672/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 89.09%, Val Acc: 38.73%\n",
            "Epoch 673/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.01%, Val Acc: 40.20%\n",
            "Epoch 674/1000, Batch Size: 64, Loss: 0.0069, Train Acc: 89.46%, Val Acc: 37.75%\n",
            "Epoch 675/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 88.11%, Val Acc: 38.24%\n",
            "Epoch 676/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 88.36%, Val Acc: 38.24%\n",
            "Epoch 677/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.01%, Val Acc: 43.14%\n",
            "Epoch 678/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 88.24%, Val Acc: 36.27%\n",
            "Epoch 679/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.99%, Val Acc: 42.16%\n",
            "Epoch 680/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 86.40%, Val Acc: 45.10%\n",
            "Epoch 681/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 86.89%, Val Acc: 42.16%\n",
            "Epoch 682/1000, Batch Size: 64, Loss: 0.0069, Train Acc: 88.11%, Val Acc: 41.18%\n",
            "Epoch 683/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 85.66%, Val Acc: 39.71%\n",
            "Epoch 684/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 86.15%, Val Acc: 38.24%\n",
            "Epoch 685/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.87%, Val Acc: 41.67%\n",
            "Epoch 686/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.85%, Val Acc: 37.25%\n",
            "Epoch 687/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 88.24%, Val Acc: 41.67%\n",
            "Epoch 688/1000, Batch Size: 64, Loss: 0.0070, Train Acc: 87.87%, Val Acc: 43.14%\n",
            "Epoch 689/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 85.29%, Val Acc: 42.65%\n",
            "Epoch 690/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 86.64%, Val Acc: 46.08%\n",
            "Epoch 691/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 85.29%, Val Acc: 38.73%\n",
            "Epoch 692/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 85.91%, Val Acc: 39.22%\n",
            "Epoch 693/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 90.56%, Val Acc: 44.12%\n",
            "Epoch 694/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 88.60%, Val Acc: 39.71%\n",
            "Epoch 695/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 87.38%, Val Acc: 42.65%\n",
            "Epoch 696/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 86.15%, Val Acc: 40.69%\n",
            "Epoch 697/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.62%, Val Acc: 40.69%\n",
            "Epoch 698/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 86.52%, Val Acc: 36.76%\n",
            "Epoch 699/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 86.89%, Val Acc: 38.24%\n",
            "Epoch 700/1000, Batch Size: 64, Loss: 0.0091, Train Acc: 84.93%, Val Acc: 43.14%\n",
            "Epoch 701/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 86.40%, Val Acc: 40.69%\n",
            "Epoch 702/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 88.11%, Val Acc: 39.22%\n",
            "Epoch 703/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.62%, Val Acc: 39.22%\n",
            "Epoch 704/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 86.76%, Val Acc: 47.06%\n",
            "Epoch 705/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 89.22%, Val Acc: 39.71%\n",
            "Epoch 706/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 89.09%, Val Acc: 41.67%\n",
            "Epoch 707/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 85.78%, Val Acc: 43.63%\n",
            "Epoch 708/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 88.85%, Val Acc: 38.73%\n",
            "Epoch 709/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 87.50%, Val Acc: 40.20%\n",
            "Epoch 710/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 87.75%, Val Acc: 41.18%\n",
            "Epoch 711/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.99%, Val Acc: 38.24%\n",
            "Epoch 712/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.76%, Val Acc: 39.22%\n",
            "Epoch 713/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 88.36%, Val Acc: 40.69%\n",
            "Epoch 714/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 86.03%, Val Acc: 36.76%\n",
            "Epoch 715/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 89.22%, Val Acc: 37.75%\n",
            "Epoch 716/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.25%, Val Acc: 41.67%\n",
            "Epoch 717/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 88.60%, Val Acc: 37.25%\n",
            "Epoch 718/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 87.01%, Val Acc: 37.75%\n",
            "Epoch 719/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 85.54%, Val Acc: 34.31%\n",
            "Epoch 720/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.38%, Val Acc: 43.14%\n",
            "Epoch 721/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 88.73%, Val Acc: 39.71%\n",
            "Epoch 722/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.76%, Val Acc: 41.67%\n",
            "Epoch 723/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 86.89%, Val Acc: 43.63%\n",
            "Epoch 724/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 87.01%, Val Acc: 36.76%\n",
            "Epoch 725/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 89.09%, Val Acc: 43.14%\n",
            "Epoch 726/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 87.75%, Val Acc: 40.20%\n",
            "Epoch 727/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 85.29%, Val Acc: 41.67%\n",
            "Epoch 728/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 86.76%, Val Acc: 41.67%\n",
            "Epoch 729/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 89.46%, Val Acc: 42.65%\n",
            "Epoch 730/1000, Batch Size: 64, Loss: 0.0070, Train Acc: 89.46%, Val Acc: 41.18%\n",
            "Epoch 731/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 87.50%, Val Acc: 41.67%\n",
            "Epoch 732/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 88.24%, Val Acc: 44.12%\n",
            "Epoch 733/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 86.76%, Val Acc: 40.69%\n",
            "Epoch 734/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.99%, Val Acc: 37.75%\n",
            "Epoch 735/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.75%, Val Acc: 43.63%\n",
            "Epoch 736/1000, Batch Size: 64, Loss: 0.0059, Train Acc: 90.07%, Val Acc: 39.22%\n",
            "Epoch 737/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 86.64%, Val Acc: 40.20%\n",
            "Epoch 738/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.76%, Val Acc: 43.63%\n",
            "Epoch 739/1000, Batch Size: 64, Loss: 0.0063, Train Acc: 91.05%, Val Acc: 42.16%\n",
            "Epoch 740/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 88.36%, Val Acc: 43.14%\n",
            "Epoch 741/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 86.27%, Val Acc: 42.65%\n",
            "Epoch 742/1000, Batch Size: 64, Loss: 0.0067, Train Acc: 89.58%, Val Acc: 38.73%\n",
            "Epoch 743/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.75%, Val Acc: 38.24%\n",
            "Epoch 744/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.62%, Val Acc: 39.22%\n",
            "Epoch 745/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 89.22%, Val Acc: 42.65%\n",
            "Epoch 746/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 85.17%, Val Acc: 41.67%\n",
            "Epoch 747/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 84.44%, Val Acc: 41.67%\n",
            "Epoch 748/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 89.46%, Val Acc: 39.22%\n",
            "Epoch 749/1000, Batch Size: 64, Loss: 0.0067, Train Acc: 89.95%, Val Acc: 37.25%\n",
            "Epoch 750/1000, Batch Size: 64, Loss: 0.0071, Train Acc: 90.44%, Val Acc: 38.24%\n",
            "Epoch 751/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 88.48%, Val Acc: 41.18%\n",
            "Epoch 752/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.99%, Val Acc: 40.69%\n",
            "Epoch 753/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 88.48%, Val Acc: 41.67%\n",
            "Epoch 754/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.01%, Val Acc: 36.76%\n",
            "Epoch 755/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.75%, Val Acc: 40.69%\n",
            "Epoch 756/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 88.48%, Val Acc: 33.82%\n",
            "Epoch 757/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.25%, Val Acc: 43.63%\n",
            "Epoch 758/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 87.25%, Val Acc: 41.18%\n",
            "Epoch 759/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 88.36%, Val Acc: 38.73%\n",
            "Epoch 760/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 89.34%, Val Acc: 42.16%\n",
            "Epoch 761/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 87.62%, Val Acc: 39.22%\n",
            "Epoch 762/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.75%, Val Acc: 44.12%\n",
            "Epoch 763/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 88.73%, Val Acc: 40.69%\n",
            "Epoch 764/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.13%, Val Acc: 40.69%\n",
            "Epoch 765/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.24%, Val Acc: 42.16%\n",
            "Epoch 766/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.24%, Val Acc: 39.71%\n",
            "Epoch 767/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 88.11%, Val Acc: 42.65%\n",
            "Epoch 768/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 85.91%, Val Acc: 37.75%\n",
            "Epoch 769/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 88.24%, Val Acc: 43.63%\n",
            "Epoch 770/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 87.62%, Val Acc: 42.16%\n",
            "Epoch 771/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 88.11%, Val Acc: 44.61%\n",
            "Epoch 772/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 88.11%, Val Acc: 44.12%\n",
            "Epoch 773/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 86.76%, Val Acc: 38.24%\n",
            "Epoch 774/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 89.46%, Val Acc: 39.22%\n",
            "Epoch 775/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 86.27%, Val Acc: 37.75%\n",
            "Epoch 776/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 89.34%, Val Acc: 40.20%\n",
            "Epoch 777/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 87.87%, Val Acc: 46.08%\n",
            "Epoch 778/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 88.48%, Val Acc: 40.20%\n",
            "Epoch 779/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 87.87%, Val Acc: 34.80%\n",
            "Epoch 780/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.97%, Val Acc: 36.76%\n",
            "Epoch 781/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 87.01%, Val Acc: 41.18%\n",
            "Epoch 782/1000, Batch Size: 64, Loss: 0.0071, Train Acc: 88.85%, Val Acc: 42.65%\n",
            "Epoch 783/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.13%, Val Acc: 38.24%\n",
            "Epoch 784/1000, Batch Size: 64, Loss: 0.0063, Train Acc: 91.79%, Val Acc: 41.18%\n",
            "Epoch 785/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 89.22%, Val Acc: 40.20%\n",
            "Epoch 786/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.40%, Val Acc: 36.76%\n",
            "Epoch 787/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.85%, Val Acc: 40.69%\n",
            "Epoch 788/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 85.91%, Val Acc: 43.63%\n",
            "Epoch 789/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.75%, Val Acc: 41.67%\n",
            "Epoch 790/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 88.11%, Val Acc: 42.16%\n",
            "Epoch 791/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 88.11%, Val Acc: 36.76%\n",
            "Epoch 792/1000, Batch Size: 64, Loss: 0.0067, Train Acc: 90.07%, Val Acc: 40.69%\n",
            "Epoch 793/1000, Batch Size: 64, Loss: 0.0068, Train Acc: 89.95%, Val Acc: 42.65%\n",
            "Epoch 794/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 86.03%, Val Acc: 43.14%\n",
            "Epoch 795/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 85.78%, Val Acc: 38.73%\n",
            "Epoch 796/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.60%, Val Acc: 40.69%\n",
            "Epoch 797/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 87.62%, Val Acc: 40.69%\n",
            "Epoch 798/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.99%, Val Acc: 34.31%\n",
            "Epoch 799/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 86.76%, Val Acc: 41.67%\n",
            "Epoch 800/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 88.85%, Val Acc: 42.16%\n",
            "Epoch 801/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 86.52%, Val Acc: 37.75%\n",
            "Epoch 802/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 85.91%, Val Acc: 45.10%\n",
            "Epoch 803/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 87.13%, Val Acc: 37.75%\n",
            "Epoch 804/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 88.11%, Val Acc: 42.16%\n",
            "Epoch 805/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 88.36%, Val Acc: 38.73%\n",
            "Epoch 806/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 88.36%, Val Acc: 42.65%\n",
            "Epoch 807/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.27%, Val Acc: 37.75%\n",
            "Epoch 808/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.99%, Val Acc: 40.69%\n",
            "Epoch 809/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.62%, Val Acc: 40.20%\n",
            "Epoch 810/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 85.78%, Val Acc: 41.67%\n",
            "Epoch 811/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 86.27%, Val Acc: 36.27%\n",
            "Epoch 812/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 86.76%, Val Acc: 43.14%\n",
            "Epoch 813/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.03%, Val Acc: 42.65%\n",
            "Epoch 814/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.52%, Val Acc: 42.65%\n",
            "Epoch 815/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 87.87%, Val Acc: 40.20%\n",
            "Epoch 816/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 86.89%, Val Acc: 42.65%\n",
            "Epoch 817/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 88.36%, Val Acc: 40.69%\n",
            "Epoch 818/1000, Batch Size: 64, Loss: 0.0064, Train Acc: 89.46%, Val Acc: 40.69%\n",
            "Epoch 819/1000, Batch Size: 64, Loss: 0.0068, Train Acc: 89.34%, Val Acc: 39.71%\n",
            "Epoch 820/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 88.24%, Val Acc: 46.08%\n",
            "Epoch 821/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 87.75%, Val Acc: 41.18%\n",
            "Epoch 822/1000, Batch Size: 64, Loss: 0.0095, Train Acc: 84.31%, Val Acc: 43.14%\n",
            "Epoch 823/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.38%, Val Acc: 39.22%\n",
            "Epoch 824/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 86.89%, Val Acc: 44.61%\n",
            "Epoch 825/1000, Batch Size: 64, Loss: 0.0069, Train Acc: 89.22%, Val Acc: 33.82%\n",
            "Epoch 826/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.38%, Val Acc: 40.69%\n",
            "Epoch 827/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 86.03%, Val Acc: 39.22%\n",
            "Epoch 828/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.62%, Val Acc: 39.22%\n",
            "Epoch 829/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 86.15%, Val Acc: 41.18%\n",
            "Epoch 830/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 87.25%, Val Acc: 39.71%\n",
            "Epoch 831/1000, Batch Size: 64, Loss: 0.0093, Train Acc: 86.03%, Val Acc: 45.59%\n",
            "Epoch 832/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 89.09%, Val Acc: 40.69%\n",
            "Epoch 833/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 86.76%, Val Acc: 46.08%\n",
            "Epoch 834/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.50%, Val Acc: 45.10%\n",
            "Epoch 835/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.62%, Val Acc: 42.65%\n",
            "Epoch 836/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.48%, Val Acc: 43.14%\n",
            "Epoch 837/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.85%, Val Acc: 42.16%\n",
            "Epoch 838/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 87.87%, Val Acc: 39.22%\n",
            "Epoch 839/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.75%, Val Acc: 41.67%\n",
            "Epoch 840/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 86.89%, Val Acc: 39.22%\n",
            "Epoch 841/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 86.27%, Val Acc: 44.61%\n",
            "Epoch 842/1000, Batch Size: 64, Loss: 0.0070, Train Acc: 88.85%, Val Acc: 40.20%\n",
            "Epoch 843/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.13%, Val Acc: 42.65%\n",
            "Epoch 844/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 88.85%, Val Acc: 41.18%\n",
            "Epoch 845/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 86.52%, Val Acc: 43.63%\n",
            "Epoch 846/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.75%, Val Acc: 42.16%\n",
            "Epoch 847/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 89.46%, Val Acc: 35.29%\n",
            "Epoch 848/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 87.75%, Val Acc: 41.67%\n",
            "Epoch 849/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 89.34%, Val Acc: 40.69%\n",
            "Epoch 850/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 88.11%, Val Acc: 43.63%\n",
            "Epoch 851/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.38%, Val Acc: 41.18%\n",
            "Epoch 852/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 89.22%, Val Acc: 40.20%\n",
            "Epoch 853/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.50%, Val Acc: 36.27%\n",
            "Epoch 854/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 86.76%, Val Acc: 42.65%\n",
            "Epoch 855/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 86.40%, Val Acc: 35.78%\n",
            "Epoch 856/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 86.52%, Val Acc: 44.61%\n",
            "Epoch 857/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 87.01%, Val Acc: 44.12%\n",
            "Epoch 858/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 86.40%, Val Acc: 42.16%\n",
            "Epoch 859/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 87.50%, Val Acc: 39.71%\n",
            "Epoch 860/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.62%, Val Acc: 40.20%\n",
            "Epoch 861/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 88.11%, Val Acc: 43.14%\n",
            "Epoch 862/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 86.03%, Val Acc: 41.67%\n",
            "Epoch 863/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.75%, Val Acc: 42.16%\n",
            "Epoch 864/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.75%, Val Acc: 38.24%\n",
            "Epoch 865/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 87.99%, Val Acc: 41.18%\n",
            "Epoch 866/1000, Batch Size: 64, Loss: 0.0089, Train Acc: 87.13%, Val Acc: 40.20%\n",
            "Epoch 867/1000, Batch Size: 64, Loss: 0.0070, Train Acc: 88.48%, Val Acc: 40.20%\n",
            "Epoch 868/1000, Batch Size: 64, Loss: 0.0074, Train Acc: 87.50%, Val Acc: 38.73%\n",
            "Epoch 869/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 86.89%, Val Acc: 42.16%\n",
            "Epoch 870/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.38%, Val Acc: 40.69%\n",
            "Epoch 871/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 87.13%, Val Acc: 43.14%\n",
            "Epoch 872/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.40%, Val Acc: 44.12%\n",
            "Epoch 873/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.13%, Val Acc: 42.16%\n",
            "Epoch 874/1000, Batch Size: 64, Loss: 0.0068, Train Acc: 89.58%, Val Acc: 41.18%\n",
            "Epoch 875/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 89.58%, Val Acc: 41.67%\n",
            "Epoch 876/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 88.73%, Val Acc: 39.22%\n",
            "Epoch 877/1000, Batch Size: 64, Loss: 0.0070, Train Acc: 88.48%, Val Acc: 44.12%\n",
            "Epoch 878/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 87.01%, Val Acc: 42.65%\n",
            "Epoch 879/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 88.24%, Val Acc: 41.67%\n",
            "Epoch 880/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 86.64%, Val Acc: 42.65%\n",
            "Epoch 881/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 88.85%, Val Acc: 43.63%\n",
            "Epoch 882/1000, Batch Size: 64, Loss: 0.0070, Train Acc: 88.24%, Val Acc: 42.16%\n",
            "Epoch 883/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.38%, Val Acc: 37.75%\n",
            "Epoch 884/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 87.13%, Val Acc: 40.69%\n",
            "Epoch 885/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 88.73%, Val Acc: 39.71%\n",
            "Epoch 886/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.25%, Val Acc: 40.69%\n",
            "Epoch 887/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.38%, Val Acc: 41.67%\n",
            "Epoch 888/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 87.38%, Val Acc: 43.63%\n",
            "Epoch 889/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 88.97%, Val Acc: 41.18%\n",
            "Epoch 890/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 87.87%, Val Acc: 44.12%\n",
            "Epoch 891/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.25%, Val Acc: 39.71%\n",
            "Epoch 892/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.75%, Val Acc: 40.20%\n",
            "Epoch 893/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 86.64%, Val Acc: 37.75%\n",
            "Epoch 894/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.87%, Val Acc: 44.61%\n",
            "Epoch 895/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 87.87%, Val Acc: 39.22%\n",
            "Epoch 896/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 87.50%, Val Acc: 41.67%\n",
            "Epoch 897/1000, Batch Size: 64, Loss: 0.0092, Train Acc: 85.05%, Val Acc: 39.71%\n",
            "Epoch 898/1000, Batch Size: 64, Loss: 0.0071, Train Acc: 90.56%, Val Acc: 43.14%\n",
            "Epoch 899/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 86.03%, Val Acc: 38.73%\n",
            "Epoch 900/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 88.36%, Val Acc: 41.18%\n",
            "Epoch 901/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.11%, Val Acc: 39.71%\n",
            "Epoch 902/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 86.40%, Val Acc: 36.76%\n",
            "Epoch 903/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 85.54%, Val Acc: 37.75%\n",
            "Epoch 904/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.13%, Val Acc: 42.16%\n",
            "Epoch 905/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 88.24%, Val Acc: 43.63%\n",
            "Epoch 906/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 86.15%, Val Acc: 42.65%\n",
            "Epoch 907/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.75%, Val Acc: 39.22%\n",
            "Epoch 908/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 87.75%, Val Acc: 41.67%\n",
            "Epoch 909/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 87.75%, Val Acc: 41.18%\n",
            "Epoch 910/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 88.36%, Val Acc: 41.67%\n",
            "Epoch 911/1000, Batch Size: 64, Loss: 0.0071, Train Acc: 89.22%, Val Acc: 43.63%\n",
            "Epoch 912/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.25%, Val Acc: 33.82%\n",
            "Epoch 913/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.48%, Val Acc: 40.69%\n",
            "Epoch 914/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 87.13%, Val Acc: 42.65%\n",
            "Epoch 915/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.13%, Val Acc: 41.18%\n",
            "Epoch 916/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.38%, Val Acc: 40.20%\n",
            "Epoch 917/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.25%, Val Acc: 35.78%\n",
            "Epoch 918/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 88.24%, Val Acc: 40.20%\n",
            "Epoch 919/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.62%, Val Acc: 39.22%\n",
            "Epoch 920/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 87.75%, Val Acc: 42.16%\n",
            "Epoch 921/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.64%, Val Acc: 40.69%\n",
            "Epoch 922/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 86.15%, Val Acc: 44.12%\n",
            "Epoch 923/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.87%, Val Acc: 38.24%\n",
            "Epoch 924/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.38%, Val Acc: 39.22%\n",
            "Epoch 925/1000, Batch Size: 64, Loss: 0.0085, Train Acc: 86.40%, Val Acc: 42.16%\n",
            "Epoch 926/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 86.27%, Val Acc: 45.10%\n",
            "Epoch 927/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 86.89%, Val Acc: 43.63%\n",
            "Epoch 928/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.50%, Val Acc: 38.73%\n",
            "Epoch 929/1000, Batch Size: 64, Loss: 0.0069, Train Acc: 88.85%, Val Acc: 42.16%\n",
            "Epoch 930/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 87.99%, Val Acc: 40.69%\n",
            "Epoch 931/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 87.13%, Val Acc: 38.24%\n",
            "Epoch 932/1000, Batch Size: 64, Loss: 0.0067, Train Acc: 89.95%, Val Acc: 40.69%\n",
            "Epoch 933/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 85.42%, Val Acc: 40.69%\n",
            "Epoch 934/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.64%, Val Acc: 39.22%\n",
            "Epoch 935/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.13%, Val Acc: 39.71%\n",
            "Epoch 936/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.99%, Val Acc: 38.73%\n",
            "Epoch 937/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 87.13%, Val Acc: 43.63%\n",
            "Epoch 938/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.99%, Val Acc: 44.61%\n",
            "Epoch 939/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.97%, Val Acc: 46.57%\n",
            "Epoch 940/1000, Batch Size: 64, Loss: 0.0067, Train Acc: 89.09%, Val Acc: 40.20%\n",
            "Epoch 941/1000, Batch Size: 64, Loss: 0.0086, Train Acc: 85.42%, Val Acc: 43.14%\n",
            "Epoch 942/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.87%, Val Acc: 42.16%\n",
            "Epoch 943/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 87.38%, Val Acc: 44.61%\n",
            "Epoch 944/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 86.15%, Val Acc: 42.16%\n",
            "Epoch 945/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.50%, Val Acc: 42.65%\n",
            "Epoch 946/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 86.15%, Val Acc: 43.14%\n",
            "Epoch 947/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.13%, Val Acc: 37.25%\n",
            "Epoch 948/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 87.87%, Val Acc: 41.18%\n",
            "Epoch 949/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.50%, Val Acc: 41.18%\n",
            "Epoch 950/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 87.87%, Val Acc: 40.69%\n",
            "Epoch 951/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 87.75%, Val Acc: 37.75%\n",
            "Epoch 952/1000, Batch Size: 64, Loss: 0.0071, Train Acc: 87.87%, Val Acc: 41.18%\n",
            "Epoch 953/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 89.22%, Val Acc: 40.69%\n",
            "Epoch 954/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 87.13%, Val Acc: 40.20%\n",
            "Epoch 955/1000, Batch Size: 64, Loss: 0.0071, Train Acc: 88.73%, Val Acc: 43.14%\n",
            "Epoch 956/1000, Batch Size: 64, Loss: 0.0069, Train Acc: 89.09%, Val Acc: 42.65%\n",
            "Epoch 957/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 89.95%, Val Acc: 36.76%\n",
            "Epoch 958/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 86.64%, Val Acc: 43.63%\n",
            "Epoch 959/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 87.01%, Val Acc: 42.65%\n",
            "Epoch 960/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.89%, Val Acc: 38.73%\n",
            "Epoch 961/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 86.89%, Val Acc: 45.10%\n",
            "Epoch 962/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 86.76%, Val Acc: 44.61%\n",
            "Epoch 963/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 86.40%, Val Acc: 40.69%\n",
            "Epoch 964/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 87.50%, Val Acc: 36.27%\n",
            "Epoch 965/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 88.85%, Val Acc: 42.16%\n",
            "Epoch 966/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 86.52%, Val Acc: 36.76%\n",
            "Epoch 967/1000, Batch Size: 64, Loss: 0.0071, Train Acc: 88.97%, Val Acc: 41.18%\n",
            "Epoch 968/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 86.40%, Val Acc: 41.67%\n",
            "Epoch 969/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 87.87%, Val Acc: 41.18%\n",
            "Epoch 970/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.38%, Val Acc: 40.20%\n",
            "Epoch 971/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 86.76%, Val Acc: 42.16%\n",
            "Epoch 972/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.01%, Val Acc: 36.76%\n",
            "Epoch 973/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 87.75%, Val Acc: 44.61%\n",
            "Epoch 974/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 87.01%, Val Acc: 42.65%\n",
            "Epoch 975/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 86.27%, Val Acc: 41.67%\n",
            "Epoch 976/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 85.66%, Val Acc: 41.18%\n",
            "Epoch 977/1000, Batch Size: 64, Loss: 0.0077, Train Acc: 88.36%, Val Acc: 39.22%\n",
            "Epoch 978/1000, Batch Size: 64, Loss: 0.0071, Train Acc: 87.87%, Val Acc: 38.73%\n",
            "Epoch 979/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.15%, Val Acc: 39.22%\n",
            "Epoch 980/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 86.27%, Val Acc: 41.18%\n",
            "Epoch 981/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.75%, Val Acc: 43.63%\n",
            "Epoch 982/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 85.91%, Val Acc: 41.67%\n",
            "Epoch 983/1000, Batch Size: 64, Loss: 0.0067, Train Acc: 89.46%, Val Acc: 44.61%\n",
            "Epoch 984/1000, Batch Size: 64, Loss: 0.0082, Train Acc: 88.11%, Val Acc: 43.14%\n",
            "Epoch 985/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 88.36%, Val Acc: 42.65%\n",
            "Epoch 986/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 86.40%, Val Acc: 40.69%\n",
            "Epoch 987/1000, Batch Size: 64, Loss: 0.0070, Train Acc: 88.60%, Val Acc: 40.69%\n",
            "Epoch 988/1000, Batch Size: 64, Loss: 0.0078, Train Acc: 87.50%, Val Acc: 40.69%\n",
            "Epoch 989/1000, Batch Size: 64, Loss: 0.0076, Train Acc: 88.73%, Val Acc: 38.24%\n",
            "Epoch 990/1000, Batch Size: 64, Loss: 0.0081, Train Acc: 86.15%, Val Acc: 40.20%\n",
            "Epoch 991/1000, Batch Size: 64, Loss: 0.0071, Train Acc: 88.97%, Val Acc: 45.10%\n",
            "Epoch 992/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 88.11%, Val Acc: 42.16%\n",
            "Epoch 993/1000, Batch Size: 64, Loss: 0.0072, Train Acc: 89.34%, Val Acc: 42.65%\n",
            "Epoch 994/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 86.89%, Val Acc: 42.65%\n",
            "Epoch 995/1000, Batch Size: 64, Loss: 0.0075, Train Acc: 88.11%, Val Acc: 43.14%\n",
            "Epoch 996/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 88.73%, Val Acc: 39.71%\n",
            "Epoch 997/1000, Batch Size: 64, Loss: 0.0080, Train Acc: 88.11%, Val Acc: 40.69%\n",
            "Epoch 998/1000, Batch Size: 64, Loss: 0.0073, Train Acc: 88.85%, Val Acc: 45.10%\n",
            "Epoch 999/1000, Batch Size: 64, Loss: 0.0070, Train Acc: 89.09%, Val Acc: 44.61%\n",
            "Epoch 1000/1000, Batch Size: 64, Loss: 0.0084, Train Acc: 85.05%, Val Acc: 38.73%\n",
            "Training complete\n",
            "Test Accuracy: 50.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(512 * 14 * 14, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 512 * 14 * 14)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "learning_rate = 0.0005\n",
        "weight_decay = 0.001\n",
        "dropout_rate = 0.5\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "              for inputs, labels in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct += torch.sum(preds == labels.data)\n",
        "                total += labels.size(0)\n",
        "              return (correct.double() / total) * 100\n",
        "\n",
        "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=1000):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    best_val_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    return model\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=1000)\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycx1AZEyvqc5",
        "outputId": "11bd6156-512e-4b4f-9048-d48dd3d0f2db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000, Batch Size: 64, Loss: 0.4323, Train Acc: 1.10%, Val Acc: 2.45%\n",
            "Epoch 11/1000, Batch Size: 64, Loss: 0.0919, Train Acc: 2.21%, Val Acc: 2.94%\n",
            "Epoch 21/1000, Batch Size: 64, Loss: 0.0689, Train Acc: 10.66%, Val Acc: 7.35%\n",
            "Epoch 31/1000, Batch Size: 64, Loss: 0.0681, Train Acc: 11.76%, Val Acc: 7.35%\n",
            "Epoch 41/1000, Batch Size: 64, Loss: 0.0628, Train Acc: 14.83%, Val Acc: 9.31%\n",
            "Epoch 51/1000, Batch Size: 64, Loss: 0.0589, Train Acc: 17.65%, Val Acc: 11.76%\n",
            "Epoch 61/1000, Batch Size: 64, Loss: 0.0462, Train Acc: 29.17%, Val Acc: 19.12%\n",
            "Epoch 71/1000, Batch Size: 64, Loss: 0.0517, Train Acc: 23.16%, Val Acc: 11.76%\n",
            "Epoch 81/1000, Batch Size: 64, Loss: 0.0501, Train Acc: 24.75%, Val Acc: 17.65%\n",
            "Epoch 91/1000, Batch Size: 64, Loss: 0.0477, Train Acc: 26.72%, Val Acc: 18.14%\n",
            "Epoch 101/1000, Batch Size: 64, Loss: 0.0449, Train Acc: 30.88%, Val Acc: 20.10%\n",
            "Epoch 111/1000, Batch Size: 64, Loss: 0.0410, Train Acc: 33.58%, Val Acc: 17.16%\n",
            "Epoch 121/1000, Batch Size: 64, Loss: 0.0375, Train Acc: 40.56%, Val Acc: 21.57%\n",
            "Epoch 131/1000, Batch Size: 64, Loss: 0.0357, Train Acc: 41.67%, Val Acc: 20.10%\n",
            "Epoch 141/1000, Batch Size: 64, Loss: 0.0331, Train Acc: 46.32%, Val Acc: 25.49%\n",
            "Epoch 151/1000, Batch Size: 64, Loss: 0.0446, Train Acc: 34.07%, Val Acc: 16.18%\n",
            "Epoch 161/1000, Batch Size: 64, Loss: 0.0450, Train Acc: 30.51%, Val Acc: 15.69%\n",
            "Epoch 171/1000, Batch Size: 64, Loss: 0.0379, Train Acc: 39.71%, Val Acc: 24.51%\n",
            "Epoch 181/1000, Batch Size: 64, Loss: 0.0381, Train Acc: 39.83%, Val Acc: 21.57%\n",
            "Epoch 191/1000, Batch Size: 64, Loss: 0.0360, Train Acc: 42.52%, Val Acc: 26.47%\n",
            "Epoch 201/1000, Batch Size: 64, Loss: 0.0348, Train Acc: 43.26%, Val Acc: 22.06%\n",
            "Epoch 211/1000, Batch Size: 64, Loss: 0.0315, Train Acc: 48.65%, Val Acc: 22.55%\n",
            "Epoch 221/1000, Batch Size: 64, Loss: 0.0306, Train Acc: 54.04%, Val Acc: 28.43%\n",
            "Epoch 231/1000, Batch Size: 64, Loss: 0.0287, Train Acc: 55.64%, Val Acc: 21.57%\n",
            "Epoch 241/1000, Batch Size: 64, Loss: 0.0264, Train Acc: 56.74%, Val Acc: 27.45%\n",
            "Epoch 251/1000, Batch Size: 64, Loss: 0.0251, Train Acc: 58.70%, Val Acc: 28.43%\n",
            "Epoch 261/1000, Batch Size: 64, Loss: 0.0253, Train Acc: 59.68%, Val Acc: 30.88%\n",
            "Epoch 271/1000, Batch Size: 64, Loss: 0.0218, Train Acc: 63.85%, Val Acc: 27.45%\n",
            "Epoch 281/1000, Batch Size: 64, Loss: 0.0221, Train Acc: 64.34%, Val Acc: 30.88%\n",
            "Epoch 291/1000, Batch Size: 64, Loss: 0.0209, Train Acc: 66.79%, Val Acc: 30.39%\n",
            "Epoch 301/1000, Batch Size: 64, Loss: 0.0200, Train Acc: 67.03%, Val Acc: 29.90%\n",
            "Epoch 311/1000, Batch Size: 64, Loss: 0.0291, Train Acc: 54.29%, Val Acc: 22.06%\n",
            "Epoch 321/1000, Batch Size: 64, Loss: 0.0292, Train Acc: 52.70%, Val Acc: 31.37%\n",
            "Epoch 331/1000, Batch Size: 64, Loss: 0.0290, Train Acc: 52.08%, Val Acc: 24.51%\n",
            "Epoch 341/1000, Batch Size: 64, Loss: 0.0293, Train Acc: 53.55%, Val Acc: 22.55%\n",
            "Epoch 351/1000, Batch Size: 64, Loss: 0.0292, Train Acc: 54.41%, Val Acc: 22.55%\n",
            "Epoch 361/1000, Batch Size: 64, Loss: 0.0275, Train Acc: 56.86%, Val Acc: 23.53%\n",
            "Epoch 371/1000, Batch Size: 64, Loss: 0.0263, Train Acc: 60.05%, Val Acc: 28.43%\n",
            "Epoch 381/1000, Batch Size: 64, Loss: 0.0232, Train Acc: 62.13%, Val Acc: 28.92%\n",
            "Epoch 391/1000, Batch Size: 64, Loss: 0.0243, Train Acc: 61.52%, Val Acc: 30.88%\n",
            "Epoch 401/1000, Batch Size: 64, Loss: 0.0212, Train Acc: 65.81%, Val Acc: 33.82%\n",
            "Epoch 411/1000, Batch Size: 64, Loss: 0.0216, Train Acc: 64.58%, Val Acc: 27.94%\n",
            "Epoch 421/1000, Batch Size: 64, Loss: 0.0212, Train Acc: 63.85%, Val Acc: 29.41%\n",
            "Epoch 431/1000, Batch Size: 64, Loss: 0.0207, Train Acc: 66.30%, Val Acc: 31.37%\n",
            "Epoch 441/1000, Batch Size: 64, Loss: 0.0205, Train Acc: 66.42%, Val Acc: 32.84%\n",
            "Epoch 451/1000, Batch Size: 64, Loss: 0.0161, Train Acc: 73.77%, Val Acc: 34.80%\n",
            "Epoch 461/1000, Batch Size: 64, Loss: 0.0176, Train Acc: 71.45%, Val Acc: 32.84%\n",
            "Epoch 471/1000, Batch Size: 64, Loss: 0.0168, Train Acc: 72.30%, Val Acc: 28.92%\n",
            "Epoch 481/1000, Batch Size: 64, Loss: 0.0168, Train Acc: 73.04%, Val Acc: 33.33%\n",
            "Epoch 491/1000, Batch Size: 64, Loss: 0.0154, Train Acc: 75.61%, Val Acc: 37.25%\n",
            "Epoch 501/1000, Batch Size: 64, Loss: 0.0150, Train Acc: 75.49%, Val Acc: 37.75%\n",
            "Epoch 511/1000, Batch Size: 64, Loss: 0.0157, Train Acc: 74.63%, Val Acc: 30.88%\n",
            "Epoch 521/1000, Batch Size: 64, Loss: 0.0129, Train Acc: 78.43%, Val Acc: 35.78%\n",
            "Epoch 531/1000, Batch Size: 64, Loss: 0.0118, Train Acc: 81.50%, Val Acc: 38.73%\n",
            "Epoch 541/1000, Batch Size: 64, Loss: 0.0139, Train Acc: 75.98%, Val Acc: 39.71%\n",
            "Epoch 551/1000, Batch Size: 64, Loss: 0.0137, Train Acc: 76.72%, Val Acc: 36.27%\n",
            "Epoch 561/1000, Batch Size: 64, Loss: 0.0111, Train Acc: 81.62%, Val Acc: 37.75%\n",
            "Epoch 571/1000, Batch Size: 64, Loss: 0.0099, Train Acc: 84.31%, Val Acc: 41.67%\n",
            "Epoch 581/1000, Batch Size: 64, Loss: 0.0109, Train Acc: 81.62%, Val Acc: 37.75%\n",
            "Epoch 591/1000, Batch Size: 64, Loss: 0.0104, Train Acc: 83.58%, Val Acc: 39.22%\n",
            "Epoch 601/1000, Batch Size: 64, Loss: 0.0095, Train Acc: 85.17%, Val Acc: 39.71%\n",
            "Epoch 611/1000, Batch Size: 64, Loss: 0.0088, Train Acc: 84.19%, Val Acc: 39.22%\n",
            "Epoch 621/1000, Batch Size: 64, Loss: 0.0103, Train Acc: 83.95%, Val Acc: 38.73%\n",
            "Epoch 631/1000, Batch Size: 64, Loss: 0.0183, Train Acc: 71.08%, Val Acc: 30.39%\n",
            "Epoch 641/1000, Batch Size: 64, Loss: 0.0204, Train Acc: 65.81%, Val Acc: 35.29%\n",
            "Epoch 651/1000, Batch Size: 64, Loss: 0.0207, Train Acc: 65.07%, Val Acc: 33.82%\n",
            "Epoch 661/1000, Batch Size: 64, Loss: 0.0173, Train Acc: 71.45%, Val Acc: 32.84%\n",
            "Epoch 671/1000, Batch Size: 64, Loss: 0.0169, Train Acc: 72.67%, Val Acc: 33.33%\n",
            "Epoch 681/1000, Batch Size: 64, Loss: 0.0181, Train Acc: 70.10%, Val Acc: 28.92%\n",
            "Epoch 691/1000, Batch Size: 64, Loss: 0.0174, Train Acc: 72.06%, Val Acc: 34.80%\n",
            "Epoch 701/1000, Batch Size: 64, Loss: 0.0154, Train Acc: 74.14%, Val Acc: 32.35%\n",
            "Epoch 711/1000, Batch Size: 64, Loss: 0.0164, Train Acc: 72.06%, Val Acc: 32.35%\n",
            "Epoch 721/1000, Batch Size: 64, Loss: 0.0143, Train Acc: 78.43%, Val Acc: 33.82%\n",
            "Epoch 731/1000, Batch Size: 64, Loss: 0.0169, Train Acc: 71.32%, Val Acc: 29.90%\n",
            "Epoch 741/1000, Batch Size: 64, Loss: 0.0155, Train Acc: 73.65%, Val Acc: 32.35%\n",
            "Epoch 751/1000, Batch Size: 64, Loss: 0.0155, Train Acc: 75.12%, Val Acc: 33.82%\n",
            "Epoch 761/1000, Batch Size: 64, Loss: 0.0137, Train Acc: 77.08%, Val Acc: 34.80%\n",
            "Epoch 771/1000, Batch Size: 64, Loss: 0.0141, Train Acc: 76.35%, Val Acc: 37.75%\n",
            "Epoch 781/1000, Batch Size: 64, Loss: 0.0155, Train Acc: 73.90%, Val Acc: 34.80%\n",
            "Epoch 791/1000, Batch Size: 64, Loss: 0.0132, Train Acc: 78.31%, Val Acc: 39.22%\n",
            "Epoch 801/1000, Batch Size: 64, Loss: 0.0137, Train Acc: 75.49%, Val Acc: 33.82%\n",
            "Epoch 811/1000, Batch Size: 64, Loss: 0.0133, Train Acc: 78.92%, Val Acc: 35.29%\n",
            "Epoch 821/1000, Batch Size: 64, Loss: 0.0123, Train Acc: 80.15%, Val Acc: 37.25%\n",
            "Epoch 831/1000, Batch Size: 64, Loss: 0.0128, Train Acc: 79.29%, Val Acc: 40.20%\n",
            "Epoch 841/1000, Batch Size: 64, Loss: 0.0111, Train Acc: 79.78%, Val Acc: 36.27%\n",
            "Epoch 851/1000, Batch Size: 64, Loss: 0.0116, Train Acc: 80.15%, Val Acc: 38.24%\n",
            "Epoch 861/1000, Batch Size: 64, Loss: 0.0115, Train Acc: 80.64%, Val Acc: 36.27%\n",
            "Epoch 871/1000, Batch Size: 64, Loss: 0.0109, Train Acc: 80.64%, Val Acc: 38.73%\n",
            "Epoch 881/1000, Batch Size: 64, Loss: 0.0104, Train Acc: 83.46%, Val Acc: 44.61%\n",
            "Epoch 891/1000, Batch Size: 64, Loss: 0.0116, Train Acc: 80.76%, Val Acc: 40.69%\n",
            "Epoch 901/1000, Batch Size: 64, Loss: 0.0113, Train Acc: 81.25%, Val Acc: 41.18%\n",
            "Epoch 911/1000, Batch Size: 64, Loss: 0.0110, Train Acc: 81.37%, Val Acc: 38.73%\n",
            "Epoch 921/1000, Batch Size: 64, Loss: 0.0096, Train Acc: 84.07%, Val Acc: 37.75%\n",
            "Epoch 931/1000, Batch Size: 64, Loss: 0.0102, Train Acc: 81.74%, Val Acc: 39.71%\n",
            "Epoch 941/1000, Batch Size: 64, Loss: 0.0095, Train Acc: 83.95%, Val Acc: 42.65%\n",
            "Epoch 951/1000, Batch Size: 64, Loss: 0.0096, Train Acc: 83.46%, Val Acc: 37.75%\n",
            "Epoch 961/1000, Batch Size: 64, Loss: 0.0079, Train Acc: 85.91%, Val Acc: 39.71%\n",
            "Epoch 971/1000, Batch Size: 64, Loss: 0.0083, Train Acc: 85.66%, Val Acc: 41.67%\n",
            "Epoch 981/1000, Batch Size: 64, Loss: 0.0091, Train Acc: 83.58%, Val Acc: 41.67%\n",
            "Epoch 991/1000, Batch Size: 64, Loss: 0.0087, Train Acc: 85.78%, Val Acc: 43.14%\n",
            "Training complete\n",
            "Test Accuracy: 54.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(512 * 14 * 14, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 512 * 14 * 14)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "learning_rate = 0.0005\n",
        "weight_decay = 0.001\n",
        "dropout_rate = 0.5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=2000):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    best_val_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    return model\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=2000)\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE0iv2RpiD6q",
        "outputId": "953f1569-f03b-46ee-d503-6e31c6bc6e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000, Batch Size: 64, Loss: 0.3812, Train Acc: 2.21%, Val Acc: 2.94%\n",
            "Epoch 51/2000, Batch Size: 64, Loss: 0.0516, Train Acc: 20.83%, Val Acc: 13.24%\n",
            "Epoch 101/2000, Batch Size: 64, Loss: 0.0427, Train Acc: 33.82%, Val Acc: 16.18%\n",
            "Epoch 151/2000, Batch Size: 64, Loss: 0.0416, Train Acc: 36.03%, Val Acc: 19.12%\n",
            "Epoch 201/2000, Batch Size: 64, Loss: 0.0333, Train Acc: 47.06%, Val Acc: 20.10%\n",
            "Epoch 251/2000, Batch Size: 64, Loss: 0.0226, Train Acc: 62.50%, Val Acc: 23.53%\n",
            "Epoch 301/2000, Batch Size: 64, Loss: 0.0187, Train Acc: 68.14%, Val Acc: 29.41%\n",
            "Epoch 351/2000, Batch Size: 64, Loss: 0.0247, Train Acc: 60.17%, Val Acc: 26.47%\n",
            "Epoch 401/2000, Batch Size: 64, Loss: 0.0225, Train Acc: 64.95%, Val Acc: 27.94%\n",
            "Epoch 451/2000, Batch Size: 64, Loss: 0.0163, Train Acc: 71.57%, Val Acc: 25.49%\n",
            "Epoch 501/2000, Batch Size: 64, Loss: 0.0145, Train Acc: 76.72%, Val Acc: 32.35%\n",
            "Epoch 551/2000, Batch Size: 64, Loss: 0.0104, Train Acc: 82.23%, Val Acc: 37.75%\n",
            "Epoch 601/2000, Batch Size: 64, Loss: 0.0098, Train Acc: 86.03%, Val Acc: 35.29%\n",
            "Epoch 651/2000, Batch Size: 64, Loss: 0.0160, Train Acc: 73.65%, Val Acc: 29.90%\n",
            "Epoch 701/2000, Batch Size: 64, Loss: 0.0160, Train Acc: 74.88%, Val Acc: 33.33%\n",
            "Epoch 751/2000, Batch Size: 64, Loss: 0.0133, Train Acc: 77.21%, Val Acc: 30.88%\n",
            "Epoch 801/2000, Batch Size: 64, Loss: 0.0113, Train Acc: 80.88%, Val Acc: 32.84%\n",
            "Epoch 851/2000, Batch Size: 64, Loss: 0.0124, Train Acc: 80.39%, Val Acc: 34.80%\n",
            "Epoch 901/2000, Batch Size: 64, Loss: 0.0091, Train Acc: 84.19%, Val Acc: 39.71%\n",
            "Epoch 951/2000, Batch Size: 64, Loss: 0.0094, Train Acc: 84.31%, Val Acc: 38.24%\n",
            "Epoch 1001/2000, Batch Size: 64, Loss: 0.0069, Train Acc: 88.60%, Val Acc: 36.27%\n",
            "Epoch 1051/2000, Batch Size: 64, Loss: 0.0049, Train Acc: 92.65%, Val Acc: 44.61%\n",
            "Epoch 1101/2000, Batch Size: 64, Loss: 0.0052, Train Acc: 91.05%, Val Acc: 39.71%\n",
            "Epoch 1151/2000, Batch Size: 64, Loss: 0.0054, Train Acc: 90.69%, Val Acc: 39.71%\n",
            "Epoch 1201/2000, Batch Size: 64, Loss: 0.0037, Train Acc: 93.75%, Val Acc: 48.53%\n",
            "Epoch 1251/2000, Batch Size: 64, Loss: 0.0040, Train Acc: 93.75%, Val Acc: 48.04%\n",
            "Epoch 1301/2000, Batch Size: 64, Loss: 0.0115, Train Acc: 80.88%, Val Acc: 31.37%\n",
            "Epoch 1351/2000, Batch Size: 64, Loss: 0.0091, Train Acc: 85.17%, Val Acc: 39.22%\n",
            "Epoch 1401/2000, Batch Size: 64, Loss: 0.0097, Train Acc: 84.19%, Val Acc: 37.25%\n",
            "Epoch 1451/2000, Batch Size: 64, Loss: 0.0093, Train Acc: 83.82%, Val Acc: 33.33%\n",
            "Epoch 1501/2000, Batch Size: 64, Loss: 0.0090, Train Acc: 83.70%, Val Acc: 36.76%\n",
            "Epoch 1551/2000, Batch Size: 64, Loss: 0.0070, Train Acc: 88.11%, Val Acc: 42.65%\n",
            "Epoch 1601/2000, Batch Size: 64, Loss: 0.0089, Train Acc: 84.19%, Val Acc: 38.73%\n",
            "Epoch 1651/2000, Batch Size: 64, Loss: 0.0078, Train Acc: 86.89%, Val Acc: 42.16%\n",
            "Epoch 1701/2000, Batch Size: 64, Loss: 0.0064, Train Acc: 87.75%, Val Acc: 39.71%\n",
            "Epoch 1751/2000, Batch Size: 64, Loss: 0.0071, Train Acc: 87.62%, Val Acc: 48.53%\n",
            "Epoch 1801/2000, Batch Size: 64, Loss: 0.0055, Train Acc: 89.95%, Val Acc: 38.73%\n",
            "Epoch 1851/2000, Batch Size: 64, Loss: 0.0064, Train Acc: 89.09%, Val Acc: 41.67%\n",
            "Epoch 1901/2000, Batch Size: 64, Loss: 0.0050, Train Acc: 91.42%, Val Acc: 47.06%\n",
            "Epoch 1951/2000, Batch Size: 64, Loss: 0.0043, Train Acc: 92.89%, Val Acc: 48.04%\n",
            "Training complete\n",
            "Test Accuracy: 59.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(512 * 14 * 14, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 512 * 14 * 14)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "learning_rate = 0.0005\n",
        "weight_decay = 0.001\n",
        "dropout_rate = 0.5\n",
        "num_epochs = 1000\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=1000):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    best_val_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "\n",
        "        if epoch % 50 == 0:  # Print every 50 epochs to save time\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqRmYX9uHhY8",
        "outputId": "7b6a3319-b46d-4c10-aa53-00ca2d250b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000, Batch Size: 64, Loss: 0.4560, Train Acc: 0.86%, Val Acc: 0.98%\n",
            "Epoch 51/1000, Batch Size: 64, Loss: 0.0714, Train Acc: 1.84%, Val Acc: 0.00%\n",
            "Epoch 101/1000, Batch Size: 64, Loss: 0.0691, Train Acc: 1.72%, Val Acc: 0.98%\n",
            "Epoch 151/1000, Batch Size: 64, Loss: 0.0690, Train Acc: 1.96%, Val Acc: 1.47%\n",
            "Epoch 201/1000, Batch Size: 64, Loss: 0.0683, Train Acc: 2.08%, Val Acc: 0.49%\n",
            "Epoch 251/1000, Batch Size: 64, Loss: 0.0672, Train Acc: 2.57%, Val Acc: 0.98%\n",
            "Epoch 301/1000, Batch Size: 64, Loss: 0.0651, Train Acc: 3.80%, Val Acc: 2.45%\n",
            "Epoch 351/1000, Batch Size: 64, Loss: 0.0662, Train Acc: 4.29%, Val Acc: 1.47%\n",
            "Epoch 401/1000, Batch Size: 64, Loss: 0.0635, Train Acc: 5.76%, Val Acc: 1.47%\n",
            "Epoch 451/1000, Batch Size: 64, Loss: 0.0595, Train Acc: 9.19%, Val Acc: 4.41%\n",
            "Epoch 501/1000, Batch Size: 64, Loss: 0.0537, Train Acc: 16.42%, Val Acc: 10.78%\n",
            "Epoch 551/1000, Batch Size: 64, Loss: 0.0493, Train Acc: 21.20%, Val Acc: 15.69%\n",
            "Epoch 601/1000, Batch Size: 64, Loss: 0.0475, Train Acc: 23.41%, Val Acc: 17.65%\n",
            "Epoch 651/1000, Batch Size: 64, Loss: 0.0486, Train Acc: 23.16%, Val Acc: 17.16%\n",
            "Epoch 701/1000, Batch Size: 64, Loss: 0.0442, Train Acc: 27.70%, Val Acc: 21.08%\n",
            "Epoch 751/1000, Batch Size: 64, Loss: 0.0413, Train Acc: 31.62%, Val Acc: 23.04%\n",
            "Epoch 801/1000, Batch Size: 64, Loss: 0.0398, Train Acc: 36.76%, Val Acc: 24.02%\n",
            "Epoch 851/1000, Batch Size: 64, Loss: 0.0346, Train Acc: 40.07%, Val Acc: 28.43%\n",
            "Epoch 901/1000, Batch Size: 64, Loss: 0.0340, Train Acc: 42.16%, Val Acc: 33.82%\n",
            "Epoch 951/1000, Batch Size: 64, Loss: 0.0294, Train Acc: 50.00%, Val Acc: 34.31%\n",
            "Training complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(512 * 14 * 14, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 512 * 14 * 14)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "learning_rate = 0.0005\n",
        "weight_decay = 0.001\n",
        "dropout_rate = 0.5\n",
        "num_epochs = 2000\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=2000):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    best_val_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    return model\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs)\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx1QeUBwlZLP",
        "outputId": "b853511e-f969-4dbc-8fc9-cc966a8c61d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to data/flowers-102/102flowers.tgz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 344862509/344862509 [00:25<00:00, 13442959.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/flowers-102/102flowers.tgz to data/flowers-102\n",
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/imagelabels.mat to data/flowers-102/imagelabels.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 502/502 [00:00<00:00, 672911.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/setid.mat to data/flowers-102/setid.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14989/14989 [00:00<00:00, 20572127.83it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000, Batch Size: 64, Loss: 0.4863, Train Acc: 1.72%, Val Acc: 2.45%\n",
            "Epoch 51/2000, Batch Size: 64, Loss: 0.0736, Train Acc: 1.10%, Val Acc: 0.00%\n",
            "Epoch 101/2000, Batch Size: 64, Loss: 0.0735, Train Acc: 1.23%, Val Acc: 0.00%\n",
            "Epoch 151/2000, Batch Size: 64, Loss: 0.0718, Train Acc: 1.47%, Val Acc: 0.49%\n",
            "Epoch 201/2000, Batch Size: 64, Loss: 0.0712, Train Acc: 2.08%, Val Acc: 0.49%\n",
            "Epoch 251/2000, Batch Size: 64, Loss: 0.0693, Train Acc: 2.94%, Val Acc: 0.49%\n",
            "Epoch 301/2000, Batch Size: 64, Loss: 0.0685, Train Acc: 1.47%, Val Acc: 1.47%\n",
            "Epoch 351/2000, Batch Size: 64, Loss: 0.0691, Train Acc: 2.70%, Val Acc: 0.98%\n",
            "Epoch 401/2000, Batch Size: 64, Loss: 0.0677, Train Acc: 3.43%, Val Acc: 2.45%\n",
            "Epoch 451/2000, Batch Size: 64, Loss: 0.0615, Train Acc: 6.62%, Val Acc: 4.41%\n",
            "Epoch 501/2000, Batch Size: 64, Loss: 0.0556, Train Acc: 15.32%, Val Acc: 11.76%\n",
            "Epoch 551/2000, Batch Size: 64, Loss: 0.0510, Train Acc: 19.12%, Val Acc: 17.65%\n",
            "Epoch 601/2000, Batch Size: 64, Loss: 0.0476, Train Acc: 23.41%, Val Acc: 19.61%\n",
            "Epoch 651/2000, Batch Size: 64, Loss: 0.0504, Train Acc: 20.59%, Val Acc: 19.12%\n",
            "Epoch 701/2000, Batch Size: 64, Loss: 0.0447, Train Acc: 27.70%, Val Acc: 17.16%\n",
            "Epoch 751/2000, Batch Size: 64, Loss: 0.0417, Train Acc: 33.95%, Val Acc: 25.98%\n",
            "Epoch 801/2000, Batch Size: 64, Loss: 0.0387, Train Acc: 35.78%, Val Acc: 25.98%\n",
            "Epoch 851/2000, Batch Size: 64, Loss: 0.0364, Train Acc: 40.07%, Val Acc: 32.35%\n",
            "Epoch 901/2000, Batch Size: 64, Loss: 0.0332, Train Acc: 44.00%, Val Acc: 31.86%\n",
            "Epoch 951/2000, Batch Size: 64, Loss: 0.0317, Train Acc: 45.34%, Val Acc: 26.47%\n",
            "Epoch 1001/2000, Batch Size: 64, Loss: 0.0272, Train Acc: 54.04%, Val Acc: 35.78%\n",
            "Epoch 1051/2000, Batch Size: 64, Loss: 0.0227, Train Acc: 59.93%, Val Acc: 33.33%\n",
            "Epoch 1101/2000, Batch Size: 64, Loss: 0.0213, Train Acc: 64.58%, Val Acc: 40.69%\n",
            "Epoch 1151/2000, Batch Size: 64, Loss: 0.0197, Train Acc: 69.00%, Val Acc: 36.76%\n",
            "Epoch 1201/2000, Batch Size: 64, Loss: 0.0209, Train Acc: 64.95%, Val Acc: 35.29%\n",
            "Epoch 1251/2000, Batch Size: 64, Loss: 0.0178, Train Acc: 71.20%, Val Acc: 36.76%\n",
            "Epoch 1301/2000, Batch Size: 64, Loss: 0.0264, Train Acc: 55.02%, Val Acc: 37.75%\n",
            "Epoch 1351/2000, Batch Size: 64, Loss: 0.0242, Train Acc: 60.05%, Val Acc: 37.25%\n",
            "Epoch 1401/2000, Batch Size: 64, Loss: 0.0222, Train Acc: 61.89%, Val Acc: 32.35%\n",
            "Epoch 1451/2000, Batch Size: 64, Loss: 0.0211, Train Acc: 62.99%, Val Acc: 33.82%\n",
            "Epoch 1501/2000, Batch Size: 64, Loss: 0.0194, Train Acc: 68.14%, Val Acc: 38.24%\n",
            "Epoch 1551/2000, Batch Size: 64, Loss: 0.0167, Train Acc: 70.34%, Val Acc: 40.69%\n",
            "Epoch 1601/2000, Batch Size: 64, Loss: 0.0167, Train Acc: 70.83%, Val Acc: 40.20%\n",
            "Epoch 1651/2000, Batch Size: 64, Loss: 0.0150, Train Acc: 73.53%, Val Acc: 44.61%\n",
            "Epoch 1701/2000, Batch Size: 64, Loss: 0.0140, Train Acc: 74.88%, Val Acc: 48.53%\n",
            "Epoch 1751/2000, Batch Size: 64, Loss: 0.0129, Train Acc: 77.82%, Val Acc: 44.61%\n",
            "Epoch 1801/2000, Batch Size: 64, Loss: 0.0114, Train Acc: 80.76%, Val Acc: 45.59%\n",
            "Epoch 1851/2000, Batch Size: 64, Loss: 0.0109, Train Acc: 81.25%, Val Acc: 42.65%\n",
            "Epoch 1901/2000, Batch Size: 64, Loss: 0.0105, Train Acc: 81.62%, Val Acc: 45.10%\n",
            "Epoch 1951/2000, Batch Size: 64, Loss: 0.0105, Train Acc: 81.37%, Val Acc: 49.51%\n",
            "Training complete\n",
            "Test Accuracy: 61.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.6):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(512 * 14 * 14, 2048)\n",
        "        self.fc2 = nn.Linear(2048, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 512 * 14 * 14)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "learning_rate = 0.0001\n",
        "weight_decay = 0.0005\n",
        "dropout_rate = 0.6\n",
        "num_epochs = 2000\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=2000):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    best_val_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    return model\n",
        "\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs)\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8R1_h3FFsuh",
        "outputId": "93db3e21-ae1a-4312-d3a0-89507a1414a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000, Batch Size: 64, Loss: 0.1311, Train Acc: 1.23%, Val Acc: 2.94%\n",
            "Epoch 51/2000, Batch Size: 64, Loss: 0.0654, Train Acc: 6.86%, Val Acc: 5.88%\n",
            "Epoch 101/2000, Batch Size: 64, Loss: 0.0593, Train Acc: 11.15%, Val Acc: 5.88%\n",
            "Epoch 151/2000, Batch Size: 64, Loss: 0.0580, Train Acc: 12.75%, Val Acc: 9.80%\n",
            "Epoch 201/2000, Batch Size: 64, Loss: 0.0543, Train Acc: 16.42%, Val Acc: 13.73%\n",
            "Epoch 251/2000, Batch Size: 64, Loss: 0.0509, Train Acc: 19.98%, Val Acc: 19.61%\n",
            "Epoch 301/2000, Batch Size: 64, Loss: 0.0502, Train Acc: 19.85%, Val Acc: 19.61%\n",
            "Epoch 351/2000, Batch Size: 64, Loss: 0.0507, Train Acc: 18.63%, Val Acc: 18.14%\n",
            "Epoch 401/2000, Batch Size: 64, Loss: 0.0486, Train Acc: 20.83%, Val Acc: 23.04%\n",
            "Epoch 451/2000, Batch Size: 64, Loss: 0.0471, Train Acc: 23.16%, Val Acc: 24.02%\n",
            "Epoch 501/2000, Batch Size: 64, Loss: 0.0434, Train Acc: 29.78%, Val Acc: 23.04%\n",
            "Epoch 551/2000, Batch Size: 64, Loss: 0.0410, Train Acc: 32.60%, Val Acc: 23.04%\n",
            "Epoch 601/2000, Batch Size: 64, Loss: 0.0424, Train Acc: 30.76%, Val Acc: 25.00%\n",
            "Epoch 651/2000, Batch Size: 64, Loss: 0.0447, Train Acc: 28.55%, Val Acc: 26.96%\n",
            "Epoch 701/2000, Batch Size: 64, Loss: 0.0432, Train Acc: 26.84%, Val Acc: 24.51%\n",
            "Epoch 751/2000, Batch Size: 64, Loss: 0.0408, Train Acc: 33.82%, Val Acc: 22.06%\n",
            "Epoch 801/2000, Batch Size: 64, Loss: 0.0393, Train Acc: 37.38%, Val Acc: 25.49%\n",
            "Epoch 851/2000, Batch Size: 64, Loss: 0.0387, Train Acc: 37.38%, Val Acc: 31.86%\n",
            "Epoch 901/2000, Batch Size: 64, Loss: 0.0367, Train Acc: 36.89%, Val Acc: 29.90%\n",
            "Epoch 951/2000, Batch Size: 64, Loss: 0.0336, Train Acc: 43.14%, Val Acc: 31.86%\n",
            "Epoch 1001/2000, Batch Size: 64, Loss: 0.0303, Train Acc: 46.94%, Val Acc: 31.37%\n",
            "Epoch 1051/2000, Batch Size: 64, Loss: 0.0319, Train Acc: 44.12%, Val Acc: 32.84%\n",
            "Epoch 1101/2000, Batch Size: 64, Loss: 0.0305, Train Acc: 49.51%, Val Acc: 39.22%\n",
            "Epoch 1151/2000, Batch Size: 64, Loss: 0.0297, Train Acc: 50.86%, Val Acc: 37.25%\n",
            "Epoch 1201/2000, Batch Size: 64, Loss: 0.0282, Train Acc: 51.59%, Val Acc: 33.33%\n",
            "Epoch 1251/2000, Batch Size: 64, Loss: 0.0297, Train Acc: 49.39%, Val Acc: 37.75%\n",
            "Epoch 1301/2000, Batch Size: 64, Loss: 0.0329, Train Acc: 43.75%, Val Acc: 33.82%\n",
            "Epoch 1351/2000, Batch Size: 64, Loss: 0.0313, Train Acc: 48.41%, Val Acc: 32.84%\n",
            "Epoch 1401/2000, Batch Size: 64, Loss: 0.0300, Train Acc: 49.51%, Val Acc: 35.29%\n",
            "Epoch 1451/2000, Batch Size: 64, Loss: 0.0298, Train Acc: 50.37%, Val Acc: 33.82%\n",
            "Epoch 1501/2000, Batch Size: 64, Loss: 0.0271, Train Acc: 54.53%, Val Acc: 38.24%\n",
            "Epoch 1551/2000, Batch Size: 64, Loss: 0.0283, Train Acc: 51.35%, Val Acc: 34.31%\n",
            "Epoch 1601/2000, Batch Size: 64, Loss: 0.0270, Train Acc: 55.64%, Val Acc: 35.78%\n",
            "Epoch 1651/2000, Batch Size: 64, Loss: 0.0250, Train Acc: 59.19%, Val Acc: 38.73%\n",
            "Epoch 1701/2000, Batch Size: 64, Loss: 0.0252, Train Acc: 57.11%, Val Acc: 41.67%\n",
            "Epoch 1751/2000, Batch Size: 64, Loss: 0.0235, Train Acc: 59.93%, Val Acc: 40.20%\n",
            "Epoch 1801/2000, Batch Size: 64, Loss: 0.0216, Train Acc: 62.62%, Val Acc: 40.20%\n",
            "Epoch 1851/2000, Batch Size: 64, Loss: 0.0210, Train Acc: 65.93%, Val Acc: 41.18%\n",
            "Epoch 1901/2000, Batch Size: 64, Loss: 0.0194, Train Acc: 66.05%, Val Acc: 40.69%\n",
            "Epoch 1951/2000, Batch Size: 64, Loss: 0.0195, Train Acc: 63.97%, Val Acc: 44.12%\n",
            "Training complete\n",
            "Test Accuracy: 56.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(512 * 14 * 14, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 512 * 14 * 14)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "learning_rate = 0.0005\n",
        "weight_decay = 0.01\n",
        "dropout_rate = 0.5\n",
        "num_epochs = 2000\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=2000):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    best_val_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    return model\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs)\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1Sor7mBLRhB",
        "outputId": "d7ae1749-c358-4412-8251-359a5dd5b781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to data/flowers-102/102flowers.tgz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 344862509/344862509 [00:16<00:00, 21024184.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/flowers-102/102flowers.tgz to data/flowers-102\n",
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/imagelabels.mat to data/flowers-102/imagelabels.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 502/502 [00:00<00:00, 797251.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/setid.mat to data/flowers-102/setid.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14989/14989 [00:00<00:00, 17669596.02it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000, Batch Size: 64, Loss: 0.4107, Train Acc: 1.47%, Val Acc: 0.49%\n",
            "Epoch 51/2000, Batch Size: 64, Loss: 0.0694, Train Acc: 1.96%, Val Acc: 0.98%\n",
            "Epoch 101/2000, Batch Size: 64, Loss: 0.0682, Train Acc: 1.23%, Val Acc: 0.98%\n",
            "Epoch 151/2000, Batch Size: 64, Loss: 0.0665, Train Acc: 3.06%, Val Acc: 1.96%\n",
            "Epoch 201/2000, Batch Size: 64, Loss: 0.0605, Train Acc: 7.72%, Val Acc: 5.39%\n",
            "Epoch 251/2000, Batch Size: 64, Loss: 0.0542, Train Acc: 14.09%, Val Acc: 10.78%\n",
            "Epoch 301/2000, Batch Size: 64, Loss: 0.0487, Train Acc: 22.55%, Val Acc: 20.59%\n",
            "Epoch 351/2000, Batch Size: 64, Loss: 0.0514, Train Acc: 18.50%, Val Acc: 12.25%\n",
            "Epoch 401/2000, Batch Size: 64, Loss: 0.0460, Train Acc: 27.21%, Val Acc: 20.59%\n",
            "Epoch 451/2000, Batch Size: 64, Loss: 0.0407, Train Acc: 35.05%, Val Acc: 24.02%\n",
            "Epoch 501/2000, Batch Size: 64, Loss: 0.0342, Train Acc: 44.00%, Val Acc: 26.47%\n",
            "Epoch 551/2000, Batch Size: 64, Loss: 0.0309, Train Acc: 51.23%, Val Acc: 37.25%\n",
            "Epoch 601/2000, Batch Size: 64, Loss: 0.0287, Train Acc: 52.33%, Val Acc: 38.24%\n",
            "Epoch 651/2000, Batch Size: 64, Loss: 0.0362, Train Acc: 38.97%, Val Acc: 28.92%\n",
            "Epoch 701/2000, Batch Size: 64, Loss: 0.0324, Train Acc: 45.71%, Val Acc: 29.90%\n",
            "Epoch 751/2000, Batch Size: 64, Loss: 0.0295, Train Acc: 48.41%, Val Acc: 28.43%\n",
            "Epoch 801/2000, Batch Size: 64, Loss: 0.0268, Train Acc: 54.41%, Val Acc: 36.76%\n",
            "Epoch 851/2000, Batch Size: 64, Loss: 0.0265, Train Acc: 53.06%, Val Acc: 35.29%\n",
            "Epoch 901/2000, Batch Size: 64, Loss: 0.0221, Train Acc: 61.64%, Val Acc: 39.22%\n",
            "Epoch 951/2000, Batch Size: 64, Loss: 0.0205, Train Acc: 66.91%, Val Acc: 43.14%\n",
            "Epoch 1001/2000, Batch Size: 64, Loss: 0.0200, Train Acc: 68.26%, Val Acc: 41.18%\n",
            "Epoch 1051/2000, Batch Size: 64, Loss: 0.0176, Train Acc: 70.34%, Val Acc: 42.65%\n",
            "Epoch 1101/2000, Batch Size: 64, Loss: 0.0174, Train Acc: 70.83%, Val Acc: 45.59%\n",
            "Epoch 1151/2000, Batch Size: 64, Loss: 0.0146, Train Acc: 76.96%, Val Acc: 51.96%\n",
            "Epoch 1201/2000, Batch Size: 64, Loss: 0.0143, Train Acc: 77.45%, Val Acc: 49.51%\n",
            "Epoch 1251/2000, Batch Size: 64, Loss: 0.0141, Train Acc: 75.61%, Val Acc: 50.98%\n",
            "Epoch 1301/2000, Batch Size: 64, Loss: 0.0225, Train Acc: 59.93%, Val Acc: 37.75%\n",
            "Epoch 1351/2000, Batch Size: 64, Loss: 0.0229, Train Acc: 61.64%, Val Acc: 41.67%\n",
            "Epoch 1401/2000, Batch Size: 64, Loss: 0.0226, Train Acc: 61.76%, Val Acc: 39.22%\n",
            "Epoch 1451/2000, Batch Size: 64, Loss: 0.0210, Train Acc: 65.69%, Val Acc: 31.37%\n",
            "Epoch 1501/2000, Batch Size: 64, Loss: 0.0206, Train Acc: 63.85%, Val Acc: 43.63%\n",
            "Epoch 1551/2000, Batch Size: 64, Loss: 0.0204, Train Acc: 65.07%, Val Acc: 48.04%\n",
            "Epoch 1601/2000, Batch Size: 64, Loss: 0.0185, Train Acc: 70.22%, Val Acc: 42.16%\n",
            "Epoch 1651/2000, Batch Size: 64, Loss: 0.0174, Train Acc: 72.06%, Val Acc: 43.63%\n",
            "Epoch 1701/2000, Batch Size: 64, Loss: 0.0186, Train Acc: 69.12%, Val Acc: 47.55%\n",
            "Epoch 1751/2000, Batch Size: 64, Loss: 0.0162, Train Acc: 72.55%, Val Acc: 40.20%\n",
            "Epoch 1801/2000, Batch Size: 64, Loss: 0.0162, Train Acc: 71.20%, Val Acc: 45.10%\n",
            "Epoch 1851/2000, Batch Size: 64, Loss: 0.0149, Train Acc: 76.72%, Val Acc: 45.59%\n",
            "Epoch 1901/2000, Batch Size: 64, Loss: 0.0141, Train Acc: 76.47%, Val Acc: 43.63%\n",
            "Epoch 1951/2000, Batch Size: 64, Loss: 0.0140, Train Acc: 76.84%, Val Acc: 45.59%\n",
            "Training complete\n",
            "Test Accuracy: 62.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.6):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(512 * 14 * 14, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 512 * 14 * 14)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "learning_rate = 0.0001\n",
        "weight_decay = 0.0005\n",
        "dropout_rate = 0.6\n",
        "num_epochs = 1500\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=1)\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=1500):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    best_val_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    return model\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs)\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP2hp3u52oFS",
        "outputId": "906980f4-1d12-4373-b14d-db37ce84d875"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to data/flowers-102/102flowers.tgz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 344862509/344862509 [00:24<00:00, 14066325.24it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/flowers-102/102flowers.tgz to data/flowers-102\n",
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/imagelabels.mat to data/flowers-102/imagelabels.mat\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 502/502 [00:00<00:00, 1477572.36it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/setid.mat to data/flowers-102/setid.mat\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14989/14989 [00:00<00:00, 20741808.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1500, Batch Size: 32, Loss: 0.2139, Train Acc: 1.35%, Val Acc: 1.47%\n",
            "Epoch 51/1500, Batch Size: 32, Loss: 0.1406, Train Acc: 2.08%, Val Acc: 0.49%\n",
            "Epoch 101/1500, Batch Size: 32, Loss: 0.1375, Train Acc: 2.45%, Val Acc: 3.92%\n",
            "Epoch 151/1500, Batch Size: 32, Loss: 0.1373, Train Acc: 2.33%, Val Acc: 2.45%\n",
            "Epoch 201/1500, Batch Size: 32, Loss: 0.1344, Train Acc: 3.55%, Val Acc: 2.45%\n",
            "Epoch 251/1500, Batch Size: 32, Loss: 0.1286, Train Acc: 3.43%, Val Acc: 4.90%\n",
            "Epoch 301/1500, Batch Size: 32, Loss: 0.1246, Train Acc: 6.00%, Val Acc: 5.88%\n",
            "Epoch 351/1500, Batch Size: 32, Loss: 0.1200, Train Acc: 8.82%, Val Acc: 6.37%\n",
            "Epoch 401/1500, Batch Size: 32, Loss: 0.1149, Train Acc: 10.54%, Val Acc: 6.37%\n",
            "Epoch 451/1500, Batch Size: 32, Loss: 0.1102, Train Acc: 13.24%, Val Acc: 12.25%\n",
            "Epoch 501/1500, Batch Size: 32, Loss: 0.1054, Train Acc: 16.05%, Val Acc: 13.24%\n",
            "Epoch 551/1500, Batch Size: 32, Loss: 0.1019, Train Acc: 15.44%, Val Acc: 18.63%\n",
            "Epoch 601/1500, Batch Size: 32, Loss: 0.0971, Train Acc: 20.47%, Val Acc: 15.20%\n",
            "Epoch 651/1500, Batch Size: 32, Loss: 0.0933, Train Acc: 24.51%, Val Acc: 20.10%\n",
            "Epoch 701/1500, Batch Size: 32, Loss: 0.0924, Train Acc: 23.28%, Val Acc: 25.49%\n",
            "Epoch 751/1500, Batch Size: 32, Loss: 0.0876, Train Acc: 27.70%, Val Acc: 30.39%\n",
            "Epoch 801/1500, Batch Size: 32, Loss: 0.0801, Train Acc: 31.25%, Val Acc: 19.61%\n",
            "Epoch 851/1500, Batch Size: 32, Loss: 0.0804, Train Acc: 32.97%, Val Acc: 27.45%\n",
            "Epoch 901/1500, Batch Size: 32, Loss: 0.0737, Train Acc: 36.40%, Val Acc: 26.47%\n",
            "Epoch 951/1500, Batch Size: 32, Loss: 0.0708, Train Acc: 41.05%, Val Acc: 29.41%\n",
            "Epoch 1001/1500, Batch Size: 32, Loss: 0.0717, Train Acc: 39.46%, Val Acc: 29.41%\n",
            "Epoch 1051/1500, Batch Size: 32, Loss: 0.0656, Train Acc: 43.14%, Val Acc: 28.92%\n",
            "Epoch 1101/1500, Batch Size: 32, Loss: 0.0677, Train Acc: 44.73%, Val Acc: 30.39%\n",
            "Epoch 1151/1500, Batch Size: 32, Loss: 0.0606, Train Acc: 46.57%, Val Acc: 27.45%\n",
            "Epoch 1201/1500, Batch Size: 32, Loss: 0.0611, Train Acc: 47.30%, Val Acc: 31.37%\n",
            "Epoch 1251/1500, Batch Size: 32, Loss: 0.0583, Train Acc: 49.51%, Val Acc: 32.84%\n",
            "Epoch 1301/1500, Batch Size: 32, Loss: 0.0621, Train Acc: 47.43%, Val Acc: 29.90%\n",
            "Epoch 1351/1500, Batch Size: 32, Loss: 0.0554, Train Acc: 54.66%, Val Acc: 34.80%\n",
            "Epoch 1401/1500, Batch Size: 32, Loss: 0.0552, Train Acc: 54.29%, Val Acc: 32.35%\n",
            "Epoch 1451/1500, Batch Size: 32, Loss: 0.0548, Train Acc: 52.21%, Val Acc: 31.86%\n",
            "Training complete\n",
            "Test Accuracy: 49.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.6):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(512 * 14 * 14, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 512 * 14 * 14)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "learning_rate = 0.0001\n",
        "weight_decay = 0.0005\n",
        "dropout_rate = 0.6\n",
        "num_epochs = 2000\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=1)\n",
        "\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=2000):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    best_val_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    return model\n",
        "\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs)\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HHumjL3tQl7",
        "outputId": "874d3827-bc6a-408b-a831-19acc2dc4086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to data/flowers-102/102flowers.tgz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 344862509/344862509 [00:23<00:00, 14419904.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/flowers-102/102flowers.tgz to data/flowers-102\n",
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/imagelabels.mat to data/flowers-102/imagelabels.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 502/502 [00:00<00:00, 812009.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/setid.mat to data/flowers-102/setid.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14989/14989 [00:00<00:00, 13667048.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000, Batch Size: 32, Loss: 0.2014, Train Acc: 1.96%, Val Acc: 2.94%\n",
            "Epoch 51/2000, Batch Size: 32, Loss: 0.1389, Train Acc: 2.57%, Val Acc: 0.49%\n",
            "Epoch 101/2000, Batch Size: 32, Loss: 0.1358, Train Acc: 3.19%, Val Acc: 1.47%\n",
            "Epoch 151/2000, Batch Size: 32, Loss: 0.1337, Train Acc: 3.31%, Val Acc: 1.47%\n",
            "Epoch 201/2000, Batch Size: 32, Loss: 0.1318, Train Acc: 5.39%, Val Acc: 4.41%\n",
            "Epoch 251/2000, Batch Size: 32, Loss: 0.1238, Train Acc: 5.15%, Val Acc: 3.92%\n",
            "Epoch 301/2000, Batch Size: 32, Loss: 0.1206, Train Acc: 8.46%, Val Acc: 7.35%\n",
            "Epoch 351/2000, Batch Size: 32, Loss: 0.1153, Train Acc: 10.54%, Val Acc: 8.33%\n",
            "Epoch 401/2000, Batch Size: 32, Loss: 0.1094, Train Acc: 14.34%, Val Acc: 8.82%\n",
            "Epoch 451/2000, Batch Size: 32, Loss: 0.1090, Train Acc: 13.73%, Val Acc: 11.27%\n",
            "Epoch 501/2000, Batch Size: 32, Loss: 0.1042, Train Acc: 17.40%, Val Acc: 13.73%\n",
            "Epoch 551/2000, Batch Size: 32, Loss: 0.1001, Train Acc: 20.59%, Val Acc: 17.16%\n",
            "Epoch 601/2000, Batch Size: 32, Loss: 0.0981, Train Acc: 21.20%, Val Acc: 18.63%\n",
            "Epoch 651/2000, Batch Size: 32, Loss: 0.0897, Train Acc: 25.37%, Val Acc: 18.14%\n",
            "Epoch 701/2000, Batch Size: 32, Loss: 0.0876, Train Acc: 29.78%, Val Acc: 21.08%\n",
            "Epoch 751/2000, Batch Size: 32, Loss: 0.0843, Train Acc: 32.60%, Val Acc: 24.02%\n",
            "Epoch 801/2000, Batch Size: 32, Loss: 0.0817, Train Acc: 32.72%, Val Acc: 25.98%\n",
            "Epoch 851/2000, Batch Size: 32, Loss: 0.0765, Train Acc: 36.52%, Val Acc: 24.02%\n",
            "Epoch 901/2000, Batch Size: 32, Loss: 0.0738, Train Acc: 38.24%, Val Acc: 25.49%\n",
            "Epoch 951/2000, Batch Size: 32, Loss: 0.0686, Train Acc: 39.46%, Val Acc: 25.00%\n",
            "Epoch 1001/2000, Batch Size: 32, Loss: 0.0710, Train Acc: 39.71%, Val Acc: 26.47%\n",
            "Epoch 1051/2000, Batch Size: 32, Loss: 0.0681, Train Acc: 41.67%, Val Acc: 30.88%\n",
            "Epoch 1101/2000, Batch Size: 32, Loss: 0.0666, Train Acc: 44.12%, Val Acc: 33.82%\n",
            "Epoch 1151/2000, Batch Size: 32, Loss: 0.0635, Train Acc: 48.04%, Val Acc: 32.84%\n",
            "Epoch 1201/2000, Batch Size: 32, Loss: 0.0599, Train Acc: 48.90%, Val Acc: 30.39%\n",
            "Epoch 1251/2000, Batch Size: 32, Loss: 0.0595, Train Acc: 47.30%, Val Acc: 32.35%\n",
            "Epoch 1301/2000, Batch Size: 32, Loss: 0.0589, Train Acc: 49.51%, Val Acc: 29.90%\n",
            "Epoch 1351/2000, Batch Size: 32, Loss: 0.0557, Train Acc: 50.74%, Val Acc: 32.84%\n",
            "Epoch 1401/2000, Batch Size: 32, Loss: 0.0537, Train Acc: 52.82%, Val Acc: 30.88%\n",
            "Epoch 1451/2000, Batch Size: 32, Loss: 0.0491, Train Acc: 54.53%, Val Acc: 32.84%\n",
            "Epoch 1501/2000, Batch Size: 32, Loss: 0.0529, Train Acc: 54.78%, Val Acc: 35.29%\n",
            "Epoch 1551/2000, Batch Size: 32, Loss: 0.0502, Train Acc: 56.00%, Val Acc: 36.27%\n",
            "Epoch 1601/2000, Batch Size: 32, Loss: 0.0496, Train Acc: 58.46%, Val Acc: 32.84%\n",
            "Epoch 1651/2000, Batch Size: 32, Loss: 0.0512, Train Acc: 55.51%, Val Acc: 36.76%\n",
            "Epoch 1701/2000, Batch Size: 32, Loss: 0.0468, Train Acc: 60.66%, Val Acc: 34.80%\n",
            "Epoch 1751/2000, Batch Size: 32, Loss: 0.0423, Train Acc: 63.48%, Val Acc: 41.67%\n",
            "Epoch 1801/2000, Batch Size: 32, Loss: 0.0444, Train Acc: 62.62%, Val Acc: 34.31%\n",
            "Epoch 1851/2000, Batch Size: 32, Loss: 0.0420, Train Acc: 63.24%, Val Acc: 41.18%\n",
            "Epoch 1901/2000, Batch Size: 32, Loss: 0.0407, Train Acc: 65.20%, Val Acc: 37.25%\n",
            "Epoch 1951/2000, Batch Size: 32, Loss: 0.0435, Train Acc: 63.97%, Val Acc: 43.63%\n",
            "Training complete\n",
            "Test Accuracy: 54.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(512 * 14 * 14, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 512 * 14 * 14)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "learning_rate = 0.0005\n",
        "weight_decay = 0.01\n",
        "dropout_rate = 0.5\n",
        "num_epochs = 2000\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=2000):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    best_val_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    return model\n",
        "\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs)\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n"
      ],
      "metadata": {
        "id": "ZS9IwN516QjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90295aa7-30b2-4cd6-9f07-d1e3e2e387d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000, Batch Size: 64, Loss: 0.3428, Train Acc: 0.86%, Val Acc: 1.96%\n",
            "Epoch 51/2000, Batch Size: 64, Loss: 0.0697, Train Acc: 1.23%, Val Acc: 0.49%\n",
            "Epoch 101/2000, Batch Size: 64, Loss: 0.0694, Train Acc: 1.59%, Val Acc: 1.96%\n",
            "Epoch 151/2000, Batch Size: 64, Loss: 0.0686, Train Acc: 2.33%, Val Acc: 2.94%\n",
            "Epoch 201/2000, Batch Size: 64, Loss: 0.0600, Train Acc: 9.19%, Val Acc: 6.86%\n",
            "Epoch 251/2000, Batch Size: 64, Loss: 0.0530, Train Acc: 17.65%, Val Acc: 9.80%\n",
            "Epoch 301/2000, Batch Size: 64, Loss: 0.0477, Train Acc: 24.14%, Val Acc: 19.12%\n",
            "Epoch 351/2000, Batch Size: 64, Loss: 0.0519, Train Acc: 20.47%, Val Acc: 12.75%\n",
            "Epoch 401/2000, Batch Size: 64, Loss: 0.0469, Train Acc: 24.75%, Val Acc: 19.61%\n",
            "Epoch 451/2000, Batch Size: 64, Loss: 0.0389, Train Acc: 35.42%, Val Acc: 29.90%\n",
            "Epoch 501/2000, Batch Size: 64, Loss: 0.0347, Train Acc: 44.36%, Val Acc: 33.82%\n",
            "Epoch 551/2000, Batch Size: 64, Loss: 0.0304, Train Acc: 50.25%, Val Acc: 40.69%\n",
            "Epoch 601/2000, Batch Size: 64, Loss: 0.0275, Train Acc: 56.37%, Val Acc: 37.25%\n",
            "Epoch 651/2000, Batch Size: 64, Loss: 0.0368, Train Acc: 38.97%, Val Acc: 24.51%\n",
            "Epoch 701/2000, Batch Size: 64, Loss: 0.0311, Train Acc: 47.79%, Val Acc: 30.39%\n",
            "Epoch 751/2000, Batch Size: 64, Loss: 0.0296, Train Acc: 50.49%, Val Acc: 31.86%\n",
            "Epoch 801/2000, Batch Size: 64, Loss: 0.0269, Train Acc: 55.02%, Val Acc: 34.80%\n",
            "Epoch 851/2000, Batch Size: 64, Loss: 0.0248, Train Acc: 59.68%, Val Acc: 42.16%\n",
            "Epoch 901/2000, Batch Size: 64, Loss: 0.0235, Train Acc: 60.91%, Val Acc: 36.27%\n",
            "Epoch 951/2000, Batch Size: 64, Loss: 0.0201, Train Acc: 66.30%, Val Acc: 43.14%\n",
            "Epoch 1001/2000, Batch Size: 64, Loss: 0.0188, Train Acc: 67.03%, Val Acc: 42.16%\n",
            "Epoch 1051/2000, Batch Size: 64, Loss: 0.0181, Train Acc: 70.59%, Val Acc: 47.55%\n",
            "Epoch 1101/2000, Batch Size: 64, Loss: 0.0160, Train Acc: 72.43%, Val Acc: 47.55%\n",
            "Epoch 1151/2000, Batch Size: 64, Loss: 0.0150, Train Acc: 76.47%, Val Acc: 51.96%\n",
            "Epoch 1201/2000, Batch Size: 64, Loss: 0.0140, Train Acc: 76.84%, Val Acc: 51.96%\n",
            "Epoch 1251/2000, Batch Size: 64, Loss: 0.0128, Train Acc: 80.02%, Val Acc: 52.45%\n",
            "Epoch 1301/2000, Batch Size: 64, Loss: 0.0228, Train Acc: 62.25%, Val Acc: 38.73%\n",
            "Epoch 1351/2000, Batch Size: 64, Loss: 0.0232, Train Acc: 59.68%, Val Acc: 33.33%\n",
            "Epoch 1401/2000, Batch Size: 64, Loss: 0.0221, Train Acc: 60.54%, Val Acc: 36.27%\n",
            "Epoch 1451/2000, Batch Size: 64, Loss: 0.0219, Train Acc: 62.99%, Val Acc: 41.18%\n",
            "Epoch 1501/2000, Batch Size: 64, Loss: 0.0210, Train Acc: 63.36%, Val Acc: 43.14%\n",
            "Epoch 1551/2000, Batch Size: 64, Loss: 0.0196, Train Acc: 66.54%, Val Acc: 37.75%\n",
            "Epoch 1601/2000, Batch Size: 64, Loss: 0.0190, Train Acc: 68.50%, Val Acc: 44.12%\n",
            "Epoch 1651/2000, Batch Size: 64, Loss: 0.0174, Train Acc: 70.71%, Val Acc: 47.06%\n",
            "Epoch 1701/2000, Batch Size: 64, Loss: 0.0179, Train Acc: 69.36%, Val Acc: 48.04%\n",
            "Epoch 1751/2000, Batch Size: 64, Loss: 0.0179, Train Acc: 68.50%, Val Acc: 50.49%\n",
            "Epoch 1801/2000, Batch Size: 64, Loss: 0.0164, Train Acc: 70.47%, Val Acc: 49.51%\n",
            "Epoch 1851/2000, Batch Size: 64, Loss: 0.0151, Train Acc: 74.39%, Val Acc: 46.57%\n",
            "Epoch 1901/2000, Batch Size: 64, Loss: 0.0138, Train Acc: 75.49%, Val Acc: 49.51%\n",
            "Epoch 1951/2000, Batch Size: 64, Loss: 0.0156, Train Acc: 74.02%, Val Acc: 49.02%\n",
            "Training complete\n",
            "Test Accuracy: 60.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "flowers102_dataset = datasets.Flowers102(root='data', split='train', download=True, transform=train_transform)\n",
        "test_dataset = datasets.Flowers102(root='data', split='test', download=True, transform=test_transform)\n",
        "\n",
        "\n",
        "dataset_size = len(flowers102_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(flowers102_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "class FlowerClassifier(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super(FlowerClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(512 * 14 * 14, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 512 * 14 * 14)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "learning_rate = 0.0005\n",
        "weight_decay = 0.01\n",
        "dropout_rate = 0.5\n",
        "num_epochs = 2000\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FlowerClassifier(dropout_rate=dropout_rate).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "    return (correct.double() / total) * 100\n",
        "\n",
        "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs=2000):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    best_val_accuracy = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = (correct.double() / total) * 100\n",
        "        val_accuracy = calculate_accuracy(val_loader, model)\n",
        "\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Batch Size: {batch_size}, Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training complete')\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    return model\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs)\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    test_accuracy = calculate_accuracy(test_loader, model)\n",
        "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "evaluate_model(trained_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdmjsuDJQ1A9",
        "outputId": "a136ef64-3f9d-48c7-fbd8-9e7179649d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to data/flowers-102/102flowers.tgz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 344862509/344862509 [00:23<00:00, 14530798.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/flowers-102/102flowers.tgz to data/flowers-102\n",
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/imagelabels.mat to data/flowers-102/imagelabels.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 502/502 [00:00<00:00, 783602.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/setid.mat to data/flowers-102/setid.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14989/14989 [00:00<00:00, 20240960.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000, Batch Size: 64, Loss: 0.3224, Train Acc: 0.86%, Val Acc: 0.49%\n",
            "Epoch 51/2000, Batch Size: 64, Loss: 0.0681, Train Acc: 2.82%, Val Acc: 1.96%\n",
            "Epoch 101/2000, Batch Size: 64, Loss: 0.0665, Train Acc: 4.53%, Val Acc: 3.92%\n",
            "Epoch 151/2000, Batch Size: 64, Loss: 0.0632, Train Acc: 5.88%, Val Acc: 2.45%\n",
            "Epoch 201/2000, Batch Size: 64, Loss: 0.0596, Train Acc: 11.89%, Val Acc: 10.29%\n",
            "Epoch 251/2000, Batch Size: 64, Loss: 0.0524, Train Acc: 19.49%, Val Acc: 13.24%\n",
            "Epoch 301/2000, Batch Size: 64, Loss: 0.0469, Train Acc: 26.72%, Val Acc: 20.59%\n",
            "Epoch 351/2000, Batch Size: 64, Loss: 0.0495, Train Acc: 19.12%, Val Acc: 15.69%\n",
            "Epoch 401/2000, Batch Size: 64, Loss: 0.0458, Train Acc: 25.37%, Val Acc: 21.08%\n",
            "Epoch 451/2000, Batch Size: 64, Loss: 0.0404, Train Acc: 34.68%, Val Acc: 23.04%\n",
            "Epoch 501/2000, Batch Size: 64, Loss: 0.0340, Train Acc: 45.22%, Val Acc: 30.88%\n",
            "Epoch 551/2000, Batch Size: 64, Loss: 0.0293, Train Acc: 51.84%, Val Acc: 37.25%\n",
            "Epoch 601/2000, Batch Size: 64, Loss: 0.0265, Train Acc: 57.23%, Val Acc: 40.20%\n",
            "Epoch 651/2000, Batch Size: 64, Loss: 0.0350, Train Acc: 39.71%, Val Acc: 36.27%\n",
            "Epoch 701/2000, Batch Size: 64, Loss: 0.0319, Train Acc: 47.18%, Val Acc: 26.47%\n",
            "Epoch 751/2000, Batch Size: 64, Loss: 0.0297, Train Acc: 50.98%, Val Acc: 38.24%\n",
            "Epoch 801/2000, Batch Size: 64, Loss: 0.0265, Train Acc: 54.53%, Val Acc: 38.24%\n",
            "Epoch 851/2000, Batch Size: 64, Loss: 0.0259, Train Acc: 56.86%, Val Acc: 40.20%\n",
            "Epoch 901/2000, Batch Size: 64, Loss: 0.0242, Train Acc: 60.66%, Val Acc: 37.25%\n",
            "Epoch 951/2000, Batch Size: 64, Loss: 0.0219, Train Acc: 62.75%, Val Acc: 46.08%\n",
            "Epoch 1001/2000, Batch Size: 64, Loss: 0.0182, Train Acc: 67.89%, Val Acc: 46.57%\n",
            "Epoch 1051/2000, Batch Size: 64, Loss: 0.0177, Train Acc: 70.83%, Val Acc: 44.12%\n",
            "Epoch 1101/2000, Batch Size: 64, Loss: 0.0164, Train Acc: 72.18%, Val Acc: 50.00%\n",
            "Epoch 1151/2000, Batch Size: 64, Loss: 0.0143, Train Acc: 76.96%, Val Acc: 48.53%\n",
            "Epoch 1201/2000, Batch Size: 64, Loss: 0.0135, Train Acc: 79.53%, Val Acc: 53.92%\n",
            "Epoch 1251/2000, Batch Size: 64, Loss: 0.0140, Train Acc: 79.53%, Val Acc: 50.00%\n",
            "Epoch 1301/2000, Batch Size: 64, Loss: 0.0228, Train Acc: 60.78%, Val Acc: 35.29%\n",
            "Epoch 1351/2000, Batch Size: 64, Loss: 0.0228, Train Acc: 61.27%, Val Acc: 38.73%\n",
            "Epoch 1401/2000, Batch Size: 64, Loss: 0.0231, Train Acc: 62.62%, Val Acc: 43.14%\n",
            "Epoch 1451/2000, Batch Size: 64, Loss: 0.0210, Train Acc: 62.62%, Val Acc: 41.18%\n",
            "Epoch 1501/2000, Batch Size: 64, Loss: 0.0205, Train Acc: 64.34%, Val Acc: 41.67%\n",
            "Epoch 1551/2000, Batch Size: 64, Loss: 0.0202, Train Acc: 65.20%, Val Acc: 38.73%\n",
            "Epoch 1601/2000, Batch Size: 64, Loss: 0.0192, Train Acc: 65.93%, Val Acc: 40.69%\n",
            "Epoch 1651/2000, Batch Size: 64, Loss: 0.0188, Train Acc: 66.91%, Val Acc: 40.69%\n",
            "Epoch 1701/2000, Batch Size: 64, Loss: 0.0181, Train Acc: 70.59%, Val Acc: 49.51%\n",
            "Epoch 1751/2000, Batch Size: 64, Loss: 0.0159, Train Acc: 72.18%, Val Acc: 45.59%\n",
            "Epoch 1801/2000, Batch Size: 64, Loss: 0.0155, Train Acc: 74.26%, Val Acc: 46.08%\n",
            "Epoch 1851/2000, Batch Size: 64, Loss: 0.0144, Train Acc: 77.33%, Val Acc: 48.53%\n",
            "Epoch 1901/2000, Batch Size: 64, Loss: 0.0139, Train Acc: 77.33%, Val Acc: 45.10%\n",
            "Epoch 1951/2000, Batch Size: 64, Loss: 0.0137, Train Acc: 77.33%, Val Acc: 50.49%\n",
            "Training complete\n",
            "Test Accuracy: 61.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as  plt\n",
        "\n",
        "\n",
        "epochs = list(range(1, 2001, 50))\n",
        "\n",
        "\n",
        "training_accuracy = [0.86, 2.82, 4.53, 5.88, 11.89, 19.49, 26.72, 19.12, 25.37, 34.68,\n",
        "                     45.22, 51.84, 57.23, 39.71, 47.18, 50.98, 54.53, 56.86, 60.66, 62.75,\n",
        "                     67.89, 70.83, 72.18, 76.96, 79.53, 79.53, 60.78, 61.27, 62.62, 62.62,\n",
        "                     64.34, 65.20, 65.93, 66.91, 70.59, 72.18, 74.26, 77.33, 77.33, 77.33]\n",
        "validation_accuracy = [0.49, 1.96, 3.92, 2.45, 10.29, 13.24, 20.59, 15.69, 21.08, 23.04,\n",
        "                       30.88, 37.25, 40.20, 36.27, 26.47, 38.24, 38.24, 40.20, 37.25, 46.08,\n",
        "                       46.57, 44.12, 50.00, 48.53, 53.92, 50.00, 35.29, 38.73, 43.14, 41.18,\n",
        "                       41.67, 38.73, 40.69, 40.69, 49.51, 45.59, 46.08, 48.53, 45.10, 50.49]\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.plot(epochs, training_accuracy, label='Training Accuracy', marker='o')\n",
        "plt.plot(epochs, validation_accuracy, label='Validation Accuracy', marker='o')\n",
        "\n",
        "plt.title('Training and Validation Accuracy vs. Number of Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "OyIVNoYEGelA",
        "outputId": "52e0aed8-d29d-494b-c5c4-25cd46b0d37b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}